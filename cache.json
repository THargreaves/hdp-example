{"Jos\u00e9 Miguel Hern\u00e1ndez-Lobato": {"Automatic chemical design using a data-driven continuous representation of molecules": "We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This model allows us to generate new molecules for efficient exploration and optimization through open-ended spaces of chemical compounds. A deep neural network was trained on hundreds of thousands of existing chemical structures to construct three coupled functions: an encoder, a decoder, and a predictor. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to discrete molecular representations. The predictor estimates chemical properties from the latent continuous vector representation of the molecule. Continuous representations of molecules allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding \u2026", "Probabilistic backpropagation for scalable learning of bayesian neural networks": "Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, eg, having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.", "Grammar variational autoencoder": "Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as natural images, artwork, and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which directly encodes from and decodes to these parse trees, ensuring the generated outputs are always syntactically valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecule generation.", "Minerva: Enabling low-power, highly-accurate deep neural network accelerators": "The continued success of Deep Neural Networks (DNNs) in classification tasks has sparked a trend of accelerating their execution with specialized hardware. While published designs easily give an order of magnitude improvement over general-purpose hardware, few look beyond an initial implementation. This paper presents Minerva, a highly automated co-design approach across the algorithm, architecture, and circuit levels to optimize DNN hardware accelerators. Compared to an established fixed-point accelerator baseline, we show that fine-grained, heterogeneous datatype optimization reduces power by 1.5\u00d7; aggressive, inline predication and pruning of small activity values further reduces power by 2.0\u00d7; and active hardware fault detection coupled with domain-aware error mitigation eliminates an additional 2.7\u00d7 through lowering SRAM voltages. Across five datasets, these optimizations provide a collective \u2026", "Predictive entropy search for efficient global optimization of black-box functions": "We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search (PES). At each iteration, PES selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. PES codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows PES to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search (ES). Furthermore, PES can easily perform a fully Bayesian treatment of the model hyperparameters while ES cannot. We evaluate PES in both synthetic and real-world applications, including optimization problems in machine learning, finance, biotechnology, and robotics. We show that the increased accuracy of PES leads to significant gains in optimization performance.", "Gans for sequences of discrete elements with the gumbel-softmax distribution": "Generative Adversarial Networks (GAN) have limitations when the goal is to generate sequences of discrete elements. The reason for this is that samples from a distribution on discrete objects such as the multinomial are not differentiable with respect to the distribution parameters. This problem can be avoided by using the Gumbel-softmax distribution, which is a continuous approximation to a multinomial distribution parameterized in terms of the softmax function. In this work, we evaluate the performance of GANs based on recurrent neural networks with Gumbel-softmax output distributions in the task of generating sequences of discrete elements.", "Deep Gaussian processes for regression using approximate expectation propagation": "Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic models and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. This paper develops a new approximate Bayesian learning scheme that enables DGPs to be applied to a range of medium to large scale regression problems for the first time. The new method uses an approximate Expectation Propagation procedure and a novel and efficient extension of the probabilistic backpropagation algorithm for learning. We evaluate the new method for non-linear regression on eleven real-world datasets, showing that it always outperforms GP regression and is almost always better than state-of-the-art deterministic and sampling-based approximate inference methods for Bayesian neural networks. As a by-product, this work provides a comprehensive analysis of six approximate Bayesian methods for training neural networks.", "Black-box alpha divergence minimization": "Black-box alpha (BB-\u03b1) is a new approximate inference method based on the minimization of \u03b1-divergences. BB-\u03b1scales to large datasets because it can be implemented using stochastic gradient descent. BB-\u03b1can be applied to complex probabilistic models with little effort since it only requires as input the likelihood function and its gradients. These gradients can be easily obtained using automatic differentiation. By changing the divergence parameter \u03b1, the method is able to interpolate between variational Bayes (VB)(\u03b1\u2192 0) and an algorithm similar to expectation propagation (EP)(\u03b1= 1). Experiments on probit regression and neural network regression and classification problems show that BB-\u03b1with non-standard settings of \u03b1, such as \u03b1= 0.5, usually produces better predictions than with \u03b1\u2192 0 (VB) or \u03b1= 1 (EP).", "Decomposition of uncertainty in Bayesian deep learning for efficient and risk-sensitive learning": "Bayesian neural networks with latent variables are scalable and flexible probabilistic models: they account for uncertainty in the estimation of the network weights and, by making use of latent variables, can capture complex noise patterns in the data. Using these models we show how to perform and utilize a decomposition of uncertainty in aleatoric and epistemic components for decision making purposes. This allows us to successfully identify informative points for active learning of functions with heteroscedastic and bimodal noise. Using the decomposition we further define a novel risk-sensitive criterion for reinforcement learningto identify policies that balance expected cost, model-bias and noise aversion.", "Predictive entropy search for multi-objective bayesian optimization": "We present\\small PESMO, a Bayesian method for identifying the Pareto set of multi-objective optimization problems, when the functions are expensive to evaluate.\\small PESMO chooses the evaluation points to maximally reduce the entropy of the posterior distribution over the Pareto set. The\\small PESMO acquisition function is decomposed as a sum of objective-specific acquisition functions, which makes it possible to use the algorithm in\\emphdecoupled scenarios in which the objectives can be evaluated separately and perhaps with different costs. This decoupling capability is useful to identify difficult objectives that require more evaluations.\\small PESMO also offers gains in efficiency, as its cost scales linearly with the number of objectives, in comparison to the exponential cost of other methods. We compare\\small PESMO with other methods on synthetic and real-world problems. The results show that\\small PESMO produces better recommendations with a smaller number of evaluations, and that a decoupled evaluation can lead to improvements in performance, particularly when the number of objectives is large.", "Learning and policy search in stochastic dynamical systems with bayesian neural networks": "We present an algorithm for model-based reinforcement learning that combines Bayesian neural networks (BNNs) with random roll-outs and stochastic optimization for policy learning. The BNNs are trained by minimizing -divergences, allowing us to capture complicated statistical patterns in the transition dynamics, e.g. multi-modality and heteroskedasticity, which are usually missed by other common modeling approaches. We illustrate the performance of our method by solving a challenging benchmark where model-based approaches usually fail and by obtaining promising results in a real-world scenario for controlling a gas turbine.", "Collaborative gaussian processes for preference learning": "We present a new model based on Gaussian processes (GPs) for learning pairwise preferences expressed by multiple users. Inference is simplified by using a preference kernel for GPs which allows us to combine supervised GP learning of user preferences with unsupervised dimensionality reduction for multi-user systems. The model not only exploits collaborative information from the shared structure in user behavior, but may also incorporate user features if they are available. Approximate inference is implemented using a combination of expectation propagation and variational Bayes. Finally, we present an efficient active learning strategy for querying preferences. The proposed technique performs favorably on real-world data against state-of-the-art multi-user preference learning algorithms.", "Stochastic expectation propagation": "Expectation propagation (EP) is a deterministic approximation algorithm that is often used to perform approximate Bayesian parameter learning. EP approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint. EP can offer analytic and computational advantages over other approximations, such as Variational Inference (VI), and is the method of choice for a number of models. The local nature of EP appears to make it an ideal candidate for performing Bayesian learning on large models in large-scale dataset settings. However, EP has a crucial limitation in this context: the number of approximating factors needs to increase with the number of data-points, N, which often entails a prohibitively large memory overhead. This paper presents an extension to EP, called stochastic expectation propagation (SEP), that maintains a global posterior approximation (like VI) but updates it in a local way (like EP). Experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that SEP performs almost as well as full EP, but reduces the memory consumption by a factor of . SEP is therefore ideally suited to performing approximate Bayesian learning in the large model, large dataset setting.", "Probabilistic matrix factorization with non-random missing data": "We propose a probabilistic matrix factorization model for collaborative filtering that learns from data that is missing not at random (MNAR). Matrix factorization models exhibit state-of-the-art predictive performance in collaborative filtering. However, these models usually assume that the data is missing at random (MAR), and this is rarely the case. For example, the data is not MAR if users rate items they like more than ones they dislike. When the MAR assumption is incorrect, inferences are biased and predictive performance can suffer. Therefore, we model both the generative process for the data and the missing data mechanism. By learning these two models jointly we obtain improved performance over state-of-the-art methods when predicting the ratings and when modeling the data observation process. We present the first viable MF model for MNAR data. Our results are promising and we expect that further research on NMAR models will yield large gains in collaborative filtering.", "Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control": "This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.", "Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space": "Chemical space is so large that brute force searches for new interesting molecules are infeasible. High-throughput virtual screening via computer cluster simulations can speed up the discovery process by collecting very large amounts of data in parallel, eg, up to hundreds or thousands of parallel measurements. Bayesian optimization (BO) can produce additional acceleration by sequentially identifying the most useful simulations or experiments to be performed next. However, current BO methods cannot scale to the large numbers of parallel measurements and the massive libraries of molecules currently used in high-throughput screening. Here, we propose a scalable solution based on a parallel and distributed implementation of Thompson sampling (PDTS). We show that, in small scale problems, PDTS performs similarly as parallel expected improvement (EI), a batch version of the most widely used BO heuristic. Additionally, in settings where parallel EI does not scale, PDTS outperforms other scalable baselines such as a greedy search, -greedy approaches and a random search method. These results show that PDTS is a successful solution for large-scale parallel BO.", "Predictive entropy search for bayesian optimization with unknown constraints": "Unknown constraints arise in many types of expensive black-box optimization problems. Several methods have been proposed recently for performing Bayesian optimization with constraints, based on the expected improvement (EI) heuristic. However, EI can lead to pathologies when used with constraints. For example, in the case of decoupled constraints\u2014ie, when one can independently evaluate the objective or the constraints\u2014EI can encounter a pathology that prevents exploration. Additionally, computing EI requires a current best solution, which may not exist if none of the data collected so far satisfy the constraints. By contrast, information-based approaches do not suffer from these failure modes. In this paper, we present a new information-based method called Predictive Entropy Search with Constraints (PESC). We analyze the performance of PESC and show that it compares favorably to EI-based approaches on synthetic and benchmark problems, as well as several real-world examples. We demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization.", "A general framework for constrained bayesian optimization using information-based search": "We present an information-theoretic framework for solving global black-box optimization problems that also have black-box constraints. Of particular interest to us is to efficiently solve problems with decoupled constraints, in which subsets of the objective and constraint functions may be evaluated independently. For example, when the objective is evaluated on a CPU and the constraints are evaluated independently on a GPU. These problems require an acquisition function that can be separated into the contributions of the individual function evaluations. We develop one such acquisition function and call it Predictive Entropy Search with Constraints (PESC). PESC is an approximation to the expected information gain criterion and it compares favorably to alternative approaches based on improvement in several synthetic and real-world problems. In addition to this, we consider problems with a mix of functions that are fast and slow to evaluate. These problems require balancing the amount of time spent in the meta-computation of PESC and in the actual evaluation of the target objective. We take a bounded rationality approach and develop a partial update for PESC which trades off accuracy against speed. We then propose a method for adaptively switching between the partial and full updates for PESC. This allows us to interpolate between versions of PESC that are efficient in terms of function evaluations and those that are efficient in terms of wall-clock time. Overall, we demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization.", "Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation.": "We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efficiently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about specific groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy.", "Deterministic variational inference for robust bayesian neural networks": "Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data. Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient. With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications? We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates. We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances. Combining these two innovations, the resulting method is highly efficient and robust. On the application of heteroscedastic regression we demonstrate good predictive performance over alternative approaches.", "Constrained Bayesian optimization for automatic chemical design using variational autoencoders": "Automatic Chemical Design is a framework for generating novel molecules with optimized properties. The original scheme, featuring Bayesian optimization over the latent space of a variational autoencoder, suffers from the pathology that it tends to produce invalid molecular structures. First, we demonstrate empirically that this pathology arises when the Bayesian optimization scheme queries latent space points far away from the data on which the variational autoencoder has been trained. Secondly, by reformulating the search procedure as a constrained Bayesian optimization problem, we show that the effects of this pathology can be mitigated, yielding marked improvements in the validity of the generated molecules. We posit that constrained Bayesian optimization is a good approach for solving this kind of training set mismatch in many generative tasks involving Bayesian optimization over the latent space of a \u2026", "Robust multi-class Gaussian process classification": "Multi-class Gaussian Process Classifiers (MGPCs) are often affected by overfitting problems when labeling errors occur far from the decision boundaries. To prevent this, we investigate a robust MGPC (RMGPC) which considers labeling errors independently of their distance to the decision boundaries. Expectation propagation is used for approximate inference. Experiments with several datasets in which noise is injected in the labels illustrate the benefits of RMGPC. This method performs better than other Gaussian process alternatives based on considering latent Gaussian noise or heavy-tailed processes. When no noise is injected in the labels, RMGPC still performs equal or better than the other methods. Finally, we show how RMGPC can be used for successfully identifying data instances which are difficult to classify correctly in practice.", "Eddi: Efficient dynamic discovery of high-value information with partial vae": "Many real-life decision-making situations allow further relevant information to be acquired at a specific cost, for example, in assessing the health status of a patient we may decide to take additional measurements such as diagnostic tests or imaging scans before making a final assessment. Acquiring more relevant information enables better decision making, but may be costly. How can we trade off the desire to make good decisions by acquiring further information with the cost of performing that acquisition? To this end, we propose a principled framework, named EDDI (Efficient Dynamic Discovery of high-value Information), based on the theory of Bayesian experimental design. In EDDI, we propose a novel partial variational autoencoder (Partial VAE) to predict missing data entries problematically given any subset of the observed ones, and combine it with an acquisition function that maximizes expected information gain on a set of target variables. We show cost reduction at the same decision quality and improved decision quality at the same cost in multiple machine learning benchmarks and two real-world health-care applications.", "Inference in deep gaussian processes using stochastic gradient hamiltonian monte carlo": "Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian Processes that combine well calibrated uncertainty estimates with the high flexibility of multilayer models. One of the biggest challenges with these models is that exact inference is intractable. The current state-of-the-art inference method, Variational Inference (VI), employs a Gaussian approximation to the posterior distribution. This can be a potentially poor unimodal approximation of the generally multimodal posterior. In this work, we provide evidence for the non-Gaussian nature of the posterior and we apply the Stochastic Gradient Hamiltonian Monte Carlo method to generate samples. To efficiently optimize the hyperparameters, we introduce the Moving Window MCEM algorithm. This results in significantly better predictions at a lower computational cost than its VI counterpart. Thus our method establishes a new state-of-the-art for inference in DGPs.", "Constrained Bayesian Optimization for Automatic Chemical Design": "Automatic Chemical Design is a framework for generating novel molecules with optimized properties. The original scheme, featuring Bayesian optimization over the latent space of a variational autoencoder, suffers from the pathology that it tends to produce invalid molecular structures. First, we demonstrate empirically that this pathology arises when the Bayesian optimization scheme queries latent points far away from the data on which the variational autoencoder has been trained. Secondly, by reformulating the search procedure as a constrained Bayesian optimization problem, we show that the effects of this pathology can be mitigated, yielding marked improvements in the validity of the generated molecules. We posit that constrained Bayesian optimization is a good approach for solving this class of training set mismatch in many generative tasks involving Bayesian optimization over the latent space of a variational autoencoder.", "Expectation propagation in linear regression models with spike-and-slab priors": "An expectation propagation (EP) algorithm is proposed for approximate inference in linear regression models with spike-and-slab priors. This EP method is applied to regression tasks in which the number of training instances is small and the number of dimensions of the feature space is large. The problems analyzed include the reconstruction of genetic networks, the recovery of sparse signals, the prediction of user sentiment from customer-written reviews and the analysis of biscuit dough constituents from NIR spectra. The proposed EP method outperforms in most of these tasks another EP method that ignores correlations in the posterior and a variational Bayes technique for approximate inference. Additionally, the solutions generated by EP are very close to those given by Gibbs sampling, which can be taken as the gold standard but can be much more computationally expensive. In the tasks analyzed, spike-and \u2026", "Scalable Gaussian process classification via expectation propagation": "Variational methods have been recently considered for scaling the training process of Gaussian process classifiers to large datasets. As an alternative, we describe here how to train these classifiers efficiently using expectation propagation (EP). The proposed EP method allows to train Gaussian process classifiers on very large datasets, with millions of instances, that were out of the reach of previous implementations of EP. More precisely, it can be used for (i) training in a distributed fashion where the data instances are sent to different nodes in which the required computations are carried out, and for (ii) maximizing an estimate of the marginal likelihood using a stochastic approximation of the gradient. Several experiments involving large datasets show that the method described is competitive with the variational approach.", "Bayesian batch active learning as sparse subset approximation": "Leveraging the wealth of unlabeled data produced in recent years provides great potential for improving supervised models. When the cost of acquiring labels is high, probabilistic active learning methods can be used to greedily select the most informative data points to be labeled. However, for many large-scale problems standard greedy procedures become computationally infeasible and suffer from negligible model change. In this paper, we introduce a novel Bayesian batch active learning approach that mitigates these issues. Our approach is motivated by approximating the complete data posterior of the model parameters. While naive batch construction methods result in correlated queries, our algorithm produces diverse batches that enable efficient active learning at scale. We derive interpretable closed-form solutions akin to existing active learning procedures for linear models, and generalize to arbitrary models using random projections. We demonstrate the benefits of our approach on several large-scale regression and classification tasks.", "Cold-start active learning with robust ordinal matrix factorization": "We present a new matrix factorization model for rating data and a corresponding active learning strategy to address the cold-start problem. Cold-start is one of the most challenging tasks for recommender systems: what to recommend with new users or items for which one has little or no data. An approach is to use active learning to collect the most useful initial ratings. However, the performance of active learning depends strongly upon having accurate estimates of i) the uncertainty in model parameters and ii) the intrinsic noisiness of the data. To achieve these estimates we propose a heteroskedastic Bayesian model for ordinal matrix factorization. We also present a computationally efficient framework for Bayesian active learning with this type of complex probabilistic model. This algorithm successfully distinguishes between informative and noisy data points. Our model yields state-of-the-art predictive performance and, coupled with our active learning strategy, enables us to gain useful information in the cold-start setting from the very first active sample.", "A model to search for synthesizable molecules": "Deep generative models are able to suggest new organic molecules by generating strings, trees, and graphs representing their structure. While such models allow one to generate molecules with desirable properties, they give no guarantees that the molecules can actually be synthesized in practice. We propose a new molecule generation model, mirroring a more realistic real-world process, where (a) reactants are selected, and (b) combined to form more complex molecules. More specifically, our generative model proposes a bag of initial reactants (selected from a pool of commercially-available molecules) and uses a reaction model to predict how they react together to generate new molecules. We first show that the model can generate diverse, valid and unique molecules due to the useful inductive biases of modeling reactions. Furthermore, our model allows chemists to interrogate not only the properties of the generated molecules but also the feasibility of the synthesis routes. We conclude by using our model to solve retrosynthesis problems, predicting a set of reactants that can produce a target product.", "A case for efficient accelerator design space exploration via bayesian optimization": "In this paper we propose using machine learning to improve the design of deep neural network hardware accelerators. We show how to adapt multi-objective Bayesian optimization to overcome a challenging design problem: optimizing deep neural network hardware accelerators for both accuracy and energy efficiency. DNN accelerators exhibit all aspects of a challenging optimization space: the landscape is rough, evaluating designs is expensive, the objectives compete with each other, and both design spaces (algorithmic and microarchitectural) are unwieldy. With multi-objective Bayesian optimization, the design space exploration is made tractable and the design points found vastly outperform traditional methods across all metrics of interest.", "'In-Between'Uncertainty in Bayesian Neural Networks": "We describe a limitation in the expressiveness of the predictive uncertainty estimate given by mean-field variational inference (MFVI), a popular approximate inference method for Bayesian neural networks. In particular, MFVI fails to give calibrated uncertainty estimates in between separated regions of observations. This can lead to catastrophically overconfident predictions when testing on out-of-distribution data. Avoiding such overconfidence is critical for active learning, Bayesian optimisation and out-of-distribution robustness. We instead find that a classical technique, the linearised Laplace approximation, can handle 'in-between' uncertainty much better for small network architectures.", "Gaussian process vine copulas for multivariate dependence": "Copulas allow to learn marginal distributions separately from the multivariate dependence structure (copula) that links them together into a density function. Vine factorizations ease the learning of high-dimensional copulas by constructing a hierarchy of conditional bivariate copulas. However, to simplify inference, it is common to assume that each of these conditional bivariate copulas is independent from its conditioning variables. In this paper, we relax this assumption by discovering the latent functions that specify the shape of a conditional copula given its conditioning variables. We learn these functions by following a Bayesian approach based on sparse Gaussian processes with expectation propagation for scalable, approximate inference. Experiments on real-world datasets show that, when modeling all conditional dependencies, we obtain better estimates of the underlying copula of the data.", "Variational implicit processes": "We introduce the implicit processes (IPs), a stochastic process that places implicitly defined multivariate distributions over any finite collections of random variables. IPs are therefore highly flexible implicit priors over functions, with examples including data simulators, Bayesian neural networks and non-linear transformations of stochastic processes. A novel and efficient approximate inference algorithm for IPs, namely the variational implicit processes (VIPs), is derived using generalised wake-sleep updates. This method returns simple update equations and allows scalable hyper-parameter learning with stochastic optimization. Experiments show that VIPs return better uncertainty estimates and lower errors over existing inference methods for challenging models such as Bayesian neural networks, and Gaussian processes.", "Expectation Propagation for microarray data classification": "Microarray experiments are a very promising tool for early diagnosis and disease treatment. The datasets obtained in these experiments typically consist of a small number of instances and a large number of covariates, most of which are irrelevant for discrimination. These characteristics pose severe difficulties for standard learning algorithms. A Bayesian approach can be useful to overcome these problems and produce more accurate and robust predictions. However, exact Bayesian inference is computationally costly and in many cases infeasible. In practice, some form of approximation has to be made. In this paper we consider a Bayesian linear model for microarray data classification based on a prior distribution that favors sparsity in the model coefficients. Expectation Propagation (EP) is then used to perform approximate inference as an alternative to computationally more expensive methods, such as Markov \u2026", "Dynamic covariance models for multivariate financial time series": "The accurate prediction of time-changing covariances is an important problem in the modeling of multivariate financial data. However, some of the most popular models suffer from a) overfitting problems and multiple local optima, b) failure to capture shifts in market conditions and c) large computational costs. To address these problems we introduce a novel dynamic model for time-changing covariances. Over-fitting and local optima are avoided by following a Bayesian approach instead of computing point estimates. Changes in market conditions are captured by assuming a diffusion process in parameter values, and finally computationally efficient and scalable inference is performed using particle filters. Experiments with financial data show excellent performance of the proposed method with respect to current standard models.", "Gaussian process volatility model": "The prediction of time-changing variances is an important task in the modeling of financial data. Standard econometric models are often limited as they assume rigid functional relationships for the evolution of the variance. Moreover, functional parameters are usually learned by maximum likelihood, which can lead to overfitting. To address these problems we introduce GP-Vol, a novel non-parametric model for time-changing variances based on Gaussian Processes. This new model can capture highly flexible functional relationships for the variances. Furthermore, we introduce a new online algorithm for fast inference in GP-Vol. This method is much faster than current offline inference procedures and it avoids overfitting problems by following a fully Bayesian approach. Experiments with financial data show that GP-Vol performs significantly better than current standard alternatives.", "Semi-supervised domain adaptation with non-parametric copulas": "A new framework based on the theory of copulas is proposed to address semi- supervised domain adaptation problems. The presented method factorizes any multivariate density into a product of marginal distributions and bivariate cop- ula functions. Therefore, changes in each of these factors can be detected and corrected to adapt a density model accross different learning domains. Impor- tantly, we introduce a novel vine copula model, which allows for this factorization in a non-parametric manner. Experimental results on regression problems with real-world data illustrate the efficacy of the proposed approach when compared to state-of-the-art techniques.", "Reinforcement learning for molecular design guided by quantum mechanics": "Automating molecular design using deep reinforcement learning (RL) holds the promise of accelerating the discovery of new chemical compounds. Existing approaches work with molecular graphs and thus ignore the location of atoms in space, which restricts them to 1) generating single organic molecules and 2) heuristic reward functions. To address this, we present a novel RL formulation for molecular design in Cartesian coordinates, thereby extending the class of molecules that can be built. Our reward function is directly based on fundamental physical properties such as the energy, which we approximate via fast quantum-chemical methods. To enable progress towards de-novo molecular design, we introduce MolGym, an RL environment comprising several challenging molecular design tasks along with baselines. In our experiments, we show that our agent can efficiently learn to solve these tasks from scratch by working in a translation and rotation invariant state-action space.", "Deconfounding reinforcement learning in observational settings": "We propose a general formulation for addressing reinforcement learning (RL) problems in settings with observational data. That is, we consider the problem of learning good policies solely from historical data in which unobserved factors (confounders) affect both observed actions and rewards. Our formulation allows us to extend a representative RL algorithm, the Actor-Critic method, to its deconfounding variant, with the methodology for this extension being easily applied to other RL algorithms. In addition to this, we develop a new benchmark for evaluating deconfounding RL algorithms by modifying the OpenAI Gym environments and the MNIST dataset. Using this benchmark, we demonstrate that the proposed algorithms are superior to traditional RL methods in confounded environments with observational data. To the best of our knowledge, this is the first time that confounders are taken into consideration for addressing full RL problems with observational data. Code is available at https://github.com/CausalRL/DRL.", "Taking gradients through experiments: LSTMs and memory proximal policy optimization for black-box quantum control": "In this work we introduce a general method to solve quantum control tasks as an interesting reinforcement learning problem not yet discussed in the machine learning community. We analyze the structure of the reinforcement learning problems typically arising in quantum physics and argue that agents parameterized by long short-term memory (LSTM) networks trained via stochastic policy gradients yield a versatile method to solving them. In this context we introduce a variant of the proximal policy optimization (PPO) algorithm called the memory proximal policy optimization (MPPO) which is based on the previous analysis. We argue that our method can by design be easily combined with numerical simulations as well as real experiments providing the reward signal. We demonstrate how the method can incorporate physical domain knowledge and present results of numerical experiments showing that it \u2026", "A generative model for electron paths": "Chemical reactions can be described as the stepwise redistribution of electrons in molecules. As such, reactions are often depicted using `arrow-pushing' diagrams which show this movement as a sequence of arrows. We propose an electron path prediction model (ELECTRO) to learn these sequences directly from raw reaction data. Instead of predicting product molecules directly from reactant molecules in one shot, learning a model of electron movement has the benefits of (a) being easy for chemists to interpret, (b) incorporating constraints of chemistry, such as balanced atom counts before and after the reaction, and (c) naturally encoding the sparsity of chemical reactions, which usually involve changes in only a small number of atoms in the reactants.We design a method to extract approximate reaction paths from any dataset of atom-mapped reaction SMILES strings. Our model achieves excellent performance on an important subset of the USPTO reaction dataset, comparing favorably to the strongest baselines. Furthermore, we show that our model recovers a basic knowledge of chemistry without being explicitly trained to do so.", "Expectation propagation for Bayesian multi-task feature selection": "In this paper we propose a Bayesian model for multi-task feature selection. This model is based on a generalized spike and slab sparse prior distribution that enforces the selection of a common subset of features across several tasks. Since exact Bayesian inference in this model is intractable, approximate inference is performed through expectation propagation (EP). EP approximates the posterior distribution of the model using a parametric probability distribution. This posterior approximation is particularly useful to identify relevant features for prediction. We focus on problems for which the number of features d is significantly larger than the number of instances for each task. We propose an efficient parametrization of the EP algorithm that offers a computational complexity linear in d. Experiments on several multi-task datasets show that the proposed model outperforms baseline approaches for single-task \u2026", "Sample-efficient optimization in the latent space of deep generative models via weighted retraining": "Many important problems in science and engineering, such as drug design, involve optimizing an expensive black-box objective function over a complex, high-dimensional, and structured input space. Although machine learning techniques have shown promise in solving such problems, existing approaches substantially lack sample efficiency. We introduce an improved method for efficient black-box optimization, which performs the optimization in the low-dimensional, continuous latent manifold learned by a deep generative model. In contrast to previous approaches, we actively steer the generative model to maintain a latent manifold that is highly useful for efficiently optimizing the objective. We achieve this by periodically retraining the generative model on the data points queried along the optimization trajectory, as well as weighting those data points according to their objective function value. This weighted retraining can be easily implemented on top of existing methods, and is empirically shown to significantly improve their efficiency and performance on synthetic and real-world optimization problems.", "Meta-learning for stochastic gradient MCMC": "Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has become increasingly popular for simulating posterior samples in large-scale Bayesian modeling. However, existing SG-MCMC schemes are not tailored to any specific probabilistic model, even a simple modification of the underlying dynamical system requires significant physical intuition. This paper presents the first meta-learning algorithm that allows automated design for the underlying continuous dynamics of an SG-MCMC sampler. The learned sampler generalizes Hamiltonian dynamics with state-dependent drift and diffusion, enabling fast traversal and efficient exploration of neural network energy landscapes. Experiments validate the proposed approach on both Bayesian fully connected neural network and Bayesian recurrent neural network tasks, showing that the learned sampler out-performs generic, hand-designed SG-MCMC algorithms, and generalizes to different datasets and larger architectures.", "Stochastic inference for scalable probabilistic modeling of binary matrices": "Fully observed large binary matrices appear in a wide variety of contexts. To model them, probabilistic matrix factorization (PMF) methods are an attractive solution. However, current batch algorithms for PMF can be inefficient because they need to analyze the entire data matrix before producing any parameter updates. We derive an efficient stochastic inference algorithm for PMF models of fully observed binary matrices. Our method exhibits faster convergence rates than more expensive batch approaches and has better predictive performance than scalable alternatives. The proposed method includes new data subsampling strategies which produce large gains over standard uniform subsampling. We also address the task of automatically selecting the size of the minibatches of data used by our method. For this, we derive an algorithm that adjusts this hyper-parameter online.", "A generative model for molecular distance geometry": "Great computational effort is invested in generating equilibrium states for molecular systems using, for example, Markov chain Monte Carlo. We present a probabilistic model that generates statistically independent samples for molecules from their graph representations. Our model learns a low-dimensional manifold that preserves the geometry of local atomic neighborhoods through a principled learning representation that is based on Euclidean distance geometry. In a new benchmark for molecular conformation generation, we show experimentally that our generative model achieves state-of-the-art accuracy. Finally, we show how to use our model as a proposal distribution in an importance sampling scheme to compute molecular properties.", "Depth uncertainty in neural networks": "Existing methods for estimating uncertainty in deep learning tend to require multiple forward passes, making them unsuitable for applications where computational resources are limited. To solve this, we perform probabilistic reasoning over the depth of neural networks. Different depths correspond to subnetworks which share weights and whose predictions are combined via marginalisation, yielding model uncertainty. By exploiting the sequential structure of feed-forward networks, we are able to both evaluate our training objective and make predictions with a single forward pass. We validate our approach on real-world regression and image classification tasks. Our approach provides uncertainty calibration, robustness to dataset shift, and accuracies competitive with more computationally expensive baselines.", "Ambiguity helps: Classification with disagreements in crowdsourced annotations": "Imagine we show an image to a person and ask her/him to decide whether the scene in the image is warm or not warm, and whether it is easy or not to spot a squirrel in the image. For exactly the same image, the answers to those questions are likely to differ from person to person. This is because the task is inherently ambiguous. Such an ambiguous, therefore challenging, task is pushing the boundary of computer vision in showing what can and can not be learned from visual data. Crowdsourcing has been invaluable for collecting annotations. This is particularly so for a task that goes beyond a clear-cut dichotomy as multiple human judgments per image are needed to reach a consensus. This paper makes conceptual and technical contributions. On the conceptual side, we define disagreements among annotators as privileged information about the data instance. On the technical side, we propose a framework to incorporate annotation disagreements into the classifiers. The proposed framework is simple, relatively fast, and outperforms classifiers that do not take into account the disagreements, especially if tested on high confidence annotations.", "Learning feature selection dependencies in multi-task learning": "A probabilistic model based on the horseshoe prior is proposed for learning dependencies in the process of identifying relevant features for prediction. Exact inference is intractable in this model. However, expectation propagation offers an approximate alternative. Because the process of estimating feature selection dependencies may suffer from over-fitting in the model proposed, additional data from a multi-task learning scenario are considered for induction. The same model can be used in this setting with few modifications. Furthermore, the assumptions made are less restrictive than in other multi-task methods: The different tasks must share feature selection dependencies, but can have different relevant features and model coefficients. Experiments with real and synthetic data show that this model performs better than other multi-task alternatives from the literature. The experiments also show that the model is able to induce suitable feature selection dependencies for the problems considered, only from the training data.", "Minimal random code learning: Getting bits back from compressed model parameters": "While deep neural networks are a highly successful model class, their large memory footprint puts considerable strain on energy consumption, communication bandwidth, and storage requirements. Consequently, model size reduction has become an utmost goal in deep learning. A typical approach is to train a set of deterministic weights, while applying certain techniques such as pruning and quantization, in order that the empirical weight distribution becomes amenable to Shannon-style coding schemes. However, as shown in this paper, relaxing weight determinism and using a full variational distribution over weights allows for more efficient coding schemes and consequently higher compression rates. In particular, following the classical bits-back argument, we encode the network weights using a random sample, requiring only a number of bits corresponding to the Kullback-Leibler divergence between the sampled variational distribution and the encoding distribution. By imposing a constraint on the Kullback-Leibler divergence, we are able to explicitly control the compression rate, while optimizing the expected loss on the training set. The employed encoding scheme can be shown to be close to the optimal information-theoretical lower bound, with respect to the employed variational family. Our method sets new state-of-the-art in neural network compression, as it strictly dominates previous approaches in a Pareto sense: On the benchmarks LeNet-5/MNIST and VGG-16/CIFAR-10, our approach yields the best test performance for a fixed memory budget, and vice versa, it achieves the highest compression rates for a fixed test performance.", "Uncertainty decomposition in bayesian neural networks with latent variables": "Bayesian neural networks (BNNs) with latent variables are probabilistic models which can automatically identify complex stochastic patterns in the data. We describe and study in these models a decomposition of predictive uncertainty into its epistemic and aleatoric components. First, we show how such a decomposition arises naturally in a Bayesian active learning scenario by following an information theoretic approach. Second, we use a similar decomposition to develop a novel risk sensitive objective for safe reinforcement learning (RL). This objective minimizes the effect of model bias in environments whose stochastic dynamics are described by BNNs with latent variables. Our experiments illustrate the usefulness of the resulting decomposition in active learning and safe RL settings.", "A probabilistic model for dirty multi-task feature selection": "Multi-task feature selection methods often make the hypothesis that learning tasks share relevant and irrelevant features. However, this hypothesis may be too restrictive in practice. For example, there may be a few tasks with specific relevant and irrelevant features (outlier tasks). Similarly, a few of the features may be relevant for only some of the tasks (outlier features). To account for this, we propose a model for multi-task feature selection based on a robust prior distribution that introduces a set of binary latent variables to identify outlier tasks and outlier features. Expectation propagation can be used for efficient approximate inference under the proposed prior. Several experiments show that a model based on the new robust prior provides better predictive performance than other benchmark methods.", "Dropout as a structured shrinkage prior": "Dropout regularization of deep neural networks has been a mysterious yet effective tool to prevent overfitting. Explanations for its success range from the prevention of\" co-adapted\" weights to it being a form of cheap Bayesian inference. We propose a novel framework for understanding multiplicative noise in neural networks, considering continuous distributions as well as Bernoulli noise (ie dropout). We show that multiplicative noise induces structured shrinkage priors on a network\u2019s weights. We derive the equivalence through reparametrization properties of scale mixtures and without invoking any approximations. Given the equivalence, we then show that dropout\u2019s Monte Carlo training objective approximates marginal MAP estimation. We leverage these insights to propose a novel shrinkage framework for resnets, terming the prior\u2019automatic depth determination\u2019as it is the natural analog of automatic relevance determination for network depth. Lastly, we investigate two inference strategies that improve upon the aforementioned MAP approximation in regression benchmarks.", "Network-based sparse Bayesian classification": "In some classification problems there is prior information about the joint relevance of groups of features. This knowledge can be encoded in a network whose nodes correspond to features and whose edges connect features that should be either both excluded or both included in the predictive model. In this paper, we introduce a novel network-based sparse Bayesian classifier (NBSBC) that makes use of the information about feature dependencies encoded in such a network to improve its prediction accuracy, especially in problems with a high-dimensional feature space and a limited amount of available training data. Approximate Bayesian inference is efficiently implemented in this model using expectation propagation. The NBSBC method is validated on four real-world classification problems from different domains of application: phonemes, handwritten digits, precipitation records and gene expression \u2026", "Successor uncertainties: exploration and uncertainty in temporal difference learning": "Posterior sampling for reinforcement learning (PSRL) is an effective method for balancing exploration and exploitation in reinforcement learning. Randomised value functions (RVF) can be viewed as a promising approach to scaling PSRL. However, we show that most contemporary algorithms combining RVF with neural network function approximation do not possess the properties which make PSRL effective, and provably fail in sparse reward problems. Moreover, we find that propagation of uncertainty, a property of PSRL previously thought important for exploration, does not preclude this failure. We use these insights to design Successor Uncertainties (SU), a cheap and easy to implement RVF algorithm that retains key properties of PSRL. SU is highly effective on hard tabular exploration benchmarks. Furthermore, on the Atari 2600 domain, it surpasses human performance on 38 of 49 games tested (achieving a median human normalised score of 2.09), and outperforms its closest RVF competitor, Bootstrapped DQN, on 36 of those.", "Bayesian semisupervised learning with deep generative models": "Neural network based generative models with discriminative components are a powerful approach for semi-supervised learning. However, these techniques a) cannot account for model uncertainty in the estimation of the model's discriminative component and b) lack flexibility to capture complex stochastic patterns in the label generation process. To avoid these problems, we first propose to use a discriminative component with stochastic inputs for increased noise flexibility. We show how an efficient Gibbs sampling procedure can marginalize the stochastic inputs when inferring missing labels in this model. Following this, we extend the discriminative component to be fully Bayesian and produce estimates of uncertainty in its parameter values. This opens the door for semi-supervised Bayesian active learning.", "Pruning adaptive boosting ensembles by means of a genetic algorithm": "This work analyzes the problem of whether, given a classification ensemble built by Adaboost, it is possible to find a subensemble with lower generalization error. In order to solve this task a genetic algorithm is proposed and compared with other heuristics like Kappa pruning and Reduce-error pruning with backfitting. Experiments carried out over a wide variety of classification problems show that the genetic algorithm behaves better than, or at least, as well as the best of those heuristics and that subensembles with similar and sometimes better prediction accuracy can be obtained.", "Bayesian variational autoencoders for unsupervised out-of-distribution detection": "Despite their successes, deep neural networks may make unreliable predictions when faced with test data drawn from a distribution different to that of the training data, constituting a major problem for AI safety. While this has recently motivated the development of methods to detect such out-of-distribution (OoD) inputs, a robust solution is still lacking. We propose a new probabilistic, unsupervised approach to this problem based on a Bayesian variational autoencoder model, which estimates a full posterior distribution over the decoder parameters using stochastic gradient Markov chain Monte Carlo, instead of fitting a point estimate. We describe how information-theoretic measures based on this posterior can then be used to detect OoD inputs both in input space and in the model's latent space. We empirically demonstrate the effectiveness of our proposed approach.", "Partial VAE for hybrid recommender system": "We propose a novel hybrid recommender system method that treats missing data in a principled manner and that uses amortized inference for fast predictions. We name this method, the Partial Variational Autoencoder (p-VAE). P-VAE uses a novel probabilistic generative model to handle varying numbers of user ratings in a principled way. Using the proposed amortized partial inference technique in p-VAEs, learning and inference can be efficiently performed by minimizing the so-called partial variational upper bound, without making ad-hoc assumptions on the values of missing ratings. Empirical experiments on the MovieLens dataset demonstrate the state-of-the-art performance of our method for movie recommendations.", "Designing neural network hardware accelerators with decoupled objective evaluations": "Software-based implementations of deep neural network predictions consume large amounts of energy, limiting their deployment in power-constrained environments. Hardware acceleration is a promising alternative. However, it is challenging to efficiently design accelerators that have both low prediction error and low energy consumption. Bayesian optimization can be used to accelerate the design problem. However, most of the existing techniques collect data in a coupled way by always evaluating the two objectives (energy and error) jointly at the same input, which is inefficient. Instead, in this work we consider a decoupled approach in which, at each iteration, we choose which objective to evaluate next and at which input. We show that considering decoupled evaluations produces better solutions when computational resources are limited. Our results also indicate that evaluating the prediction error is more important than evaluating the energy consumption.", "Learning a generative model for validity in complex discrete structures": "Deep generative models have been successfully used to learn representations for high-dimensional discrete spaces by representing discrete objects as sequences and employing powerful sequence-based deep models. Unfortunately, these sequence-based models often produce invalid sequences: sequences which do not represent any underlying discrete structure; invalid sequences hinder the utility of such models. As a step towards solving this problem, we propose to learn a deep recurrent validator model, which can estimate whether a partial sequence can function as the beginning of a full, valid sequence. This validator provides insight as to how individual sequence elements influence the validity of the overall sequence, and can be used to constrain sequence based models to generate valid sequences -- and thus faithfully model discrete objects. Our approach is inspired by reinforcement learning, where an oracle which can evaluate validity of complete sequences provides a sparse reward signal. We demonstrate its effectiveness as a generative model of Python 3 source code for mathematical expressions, and in improving the ability of a variational autoencoder trained on SMILES strings to decode valid molecular structures.", "Training deep Gaussian processes using stochastic expectation propagation and probabilistic backpropagation": "Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are probabilistic and non-parametric and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. The focus of this paper is scalable approximate Bayesian learning of these networks. The paper develops a novel and efficient extension of probabilistic backpropagation, a state-of-the-art method for training Bayesian neural networks, that can be used to train DGPs. The new method leverages a recently proposed method for scaling Expectation Propagation, called stochastic Expectation Propagation. The method is able to automatically discover useful input warping, expansion or compression, and it is therefore is a flexible form of Bayesian kernel design. We demonstrate the success of the new method for supervised learning on several real-world datasets, showing that it typically outperforms GP regression and is never much worse.", "Icebreaker: Element-wise active information acquisition with bayesian deep latent gaussian model": "In this paper we introduce the ice-start problem, i.e., the challenge of deploying machine learning models when only little or no training data is initially available, and acquiring each feature element of data is associated with costs. This setting is representative for the real-world machine learning applications. For instance, in the health-care domain, when training an AI system for predicting patient metrics from lab tests, obtaining every single measurement comes with a high cost. Active learning, where only the label is associated with a cost does not apply to such problem, because performing all possible lab tests to acquire a new training datum would be costly, as well as unnecessary due to redundancy. We propose Icebreaker, a principled framework to approach the ice-start problem. Icebreaker uses a full Bayesian Deep Latent Gaussian Model (BELGAM) with a novel inference method. Our proposed method combines recent advances in amortized inference and stochastic gradient MCMC to enable fast and accurate posterior inference. By utilizing BELGAM's ability to fully quantify model uncertainty, we also propose two information acquisition functions for imputation and active prediction problems. We demonstrate that BELGAM performs significantly better than the previous VAE (Variational autoencoder) based models, when the data set size is small, using both machine learning benchmarks and real-world recommender systems and health-care applications. Moreover, based on BELGAM, Icebreaker further improves the performance and demonstrate the ability to use minimum amount of the training data to obtain the highest test time \u2026", "Symmetry-aware actor-critic for 3d molecular design": "Automating molecular design using deep reinforcement learning (RL) has the potential to greatly accelerate the search for novel materials. Despite recent progress on leveraging graph representations to design molecules, such methods are fundamentally limited by the lack of three-dimensional (3D) information. In light of this, we propose a novel actor-critic architecture for 3D molecular design that can generate molecular structures unattainable with previous approaches. This is achieved by exploiting the symmetries of the design process through a rotationally covariant state-action representation based on a spherical harmonics series expansion. We demonstrate the benefits of our approach on several 3D molecular design tasks, where we find that building in such symmetries significantly improves generalization and the quality of generated molecules.", "Instructions and Guide for Diagnostic Questions: The NeurIPS 2020 Education Challenge": "Digital technologies are becoming increasingly prevalent in education, enabling personalized, high quality education resources to be accessible by students across the world. Importantly, among these resources are diagnostic questions: the answers that the students give to these questions reveal key information about the specific nature of misconceptions that the students may hold. Analyzing the massive quantities of data stemming from students' interactions with these diagnostic questions can help us more accurately understand the students' learning status and thus allow us to automate learning curriculum recommendations. In this competition, participants will focus on the students' answer records to these multiple-choice diagnostic questions, with the aim of 1) accurately predicting which answers the students provide; 2) accurately predicting which questions have high quality; and 3) determining a personalized sequence of questions for each student that best predicts the student's answers. These tasks closely mimic the goals of a real-world educational platform and are highly representative of the educational challenges faced today. We provide over 20 million examples of students' answers to mathematics questions from Eedi, a leading educational platform which thousands of students interact with daily around the globe. Participants to this competition have a chance to make a lasting, real-world impact on the quality of personalized education for millions of students across the world.", "Combining deep generative and discriminative models for Bayesian semi-supervised learning": "Generative models can be used for a wide range of tasks, and have the appealing ability to learn from both labelled and unlabelled data. In contrast, discriminative models cannot learn from unlabelled data, but tend to outperform their generative counterparts in supervised tasks. We develop a framework to jointly train deep generative and discriminative models, enjoying the benefits of both. The framework allows models to learn from labelled and unlabelled data, as well as naturally account for uncertainty in predictive distributions, providing the first Bayesian approach to semi-supervised learning with deep generative models. We demonstrate that our blended discriminative and generative models outperform purely generative models in both predictive performance and uncertainty calibration in a number of semi-supervised learning tasks.", "Actively learning what makes a discrete sequence valid": "Deep learning techniques have been hugely successful for traditional supervised and unsupervised machine learning problems. In large part, these techniques solve continuous optimization problems. Recently however, discrete generative deep learning models have been successfully used to efficiently search high-dimensional discrete spaces. These methods work by representing discrete objects as sequences, for which powerful sequence-based deep models can be employed. Unfortunately, these techniques are significantly hindered by the fact that these generative models often produce invalid sequences. As a step towards solving this problem, we propose to learn a deep recurrent validator model. Given a partial sequence, our model learns the probability of that sequence occurring as the beginning of a full valid sequence. Thus this identifies valid versus invalid sequences and crucially it also provides insight about how individual sequence elements influence the validity of discrete objects. To learn this model we propose an approach inspired by seminal work in Bayesian active learning. On a synthetic dataset, we demonstrate the ability of our model to distinguish valid and invalid sequences. We believe this is a key step toward learning generative models that faithfully produce valid discrete objects.", "Gaussian process conditional copulas with applications to financial time series": "The estimation of dependencies between multiple variables is a central problem in the analysis of financial time series. A common approach is to express these dependencies in terms of a copula function. Typically the copula function is assumed to be constant but this may be inaccurate when there are covariates that could have a large influence on the dependence structure of the data. To account for this, a Bayesian framework for the estimation of conditional copulas is proposed. In this framework the parameters of a copula are non-linearly related to some arbitrary conditioning variables. We evaluate the ability of our method to predict time-varying dependencies on several equities and currencies and observe consistent performance gains compared to static copula models and other time-varying copula methods.", "Bayes machines for binary classification": "In this work, we propose an approach to binary classification based on an extension of Bayes Point Machines. Particularly, we take into account the whole set of hypotheses that are consistent with the data (the so-called version space) and the intrinsic noise in class labeling. We follow a Bayesian approach and compute an approximate posterior distribution for the model parameters, which leads to a predictive distribution over unseen data. The most compelling feature of the proposed model is that it is able to learn the noise present in the data with no additional cost. All the computations are carried out by means of the approximate Bayesian inference algorithm Expectation Propagation. Experimental results indicate that the proposed approach outperforms Support Vector Machines over several of the classification problems studied and is competitive with other Bayesian classification algorithms based on Gaussian \u2026", "Sliced kernelized Stein discrepancy": "Kernelized Stein discrepancy (KSD), though being extensively used in goodness-of-fit tests and model learning, suffers from the curse-of-dimensionality. We address this issue by proposing the sliced Stein discrepancy and its scalable and kernelized variants, which employ kernel-based test functions defined on the optimal one-dimensional projections. When applied to goodness-of-fit tests, extensive experiments show the proposed discrepancy significantly outperforms KSD and various baselines in high dimensions. For model learning, we show its advantages over existing Stein discrepancy baselines by training independent component analysis models with different discrepancies. We further propose a novel particle inference method called sliced Stein variational gradient descent (S-SVGD) which alleviates the mode-collapse issue of SVGD in training variational autoencoders.", "Semiparametric bivariate Archimedean copulas": "While parametric copulas often lack expressive capacity to capture the complex dependencies that are usually found in empirical data, non-parametric copulas can have poor generalization performance because of overfitting. A semiparametric copula method based on the family of bivariate Archimedean copulas is introduced as an intermediate approach that aims to provide both accurate and robust fits. The Archimedean copula is expressed in terms of a latent function that can be readily represented using a basis of natural cubic splines. The model parameters are determined by maximizing the sum of the log-likelihood and a term that penalizes non-smooth solutions. The performance of the semiparametric estimator is analyzed in experiments with simulated and real-world data, and compared to other methods for copula estimation: three parametric copula models, two semiparametric estimators of Archimedean \u2026", "Balancing flexibility and robustness in machine learning semi-parametric methods and sparse linear models": "Machine learning problems can be addressed by a variety of methods that span a wide range of degrees of flexibility and robustness. In the process of building a model for data, flexibility and robustness are desirable but often conflicting goals. On one side of the spectrum, parametric methods are very robust, in the sense that they are resilient to noise and are not generally misled by spurious regularities, which may be present in the data only by accident. However, their expressive capacity is limited. On the other side, non-parametric methods are very flexible and can in principle learn arbitrarily complex patterns when sufficient amounts of data are available for induction. However, as a result of this high flexibility, they are also more prone to overfitting. In practice, selecting the optimal method to address a specific learning task involves attaining the appropriate balance between flexibility and robustness. There are some learning problems for which this balance cannot be attained using standard parametric or purely non-parametric approaches in isolation. Semi-parametric methods include both parametric and non-parametric components in the models assumed. The parametric part provides a robust description of some of the patterns in the data. The non-parametric component endows the model with the flexibility necessary to capture additional complex patterns. In this thesis, we analyze several problems in which semi-parametric methods provide accurate models for the data. The first one is the modeling of financial time series. The trends in these series are described by parametric models. The density of the innovations is directly learned from \u2026", "Getting a CLUE: A Method for Explaining Uncertainty Estimates": "Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the input's prediction. We validate CLUE through 1) a novel framework for evaluating counterfactual explanations of uncertainty, 2) a series of ablation experiments, and 3) a user study. Our experiments show that CLUE outperforms baselines and enables practitioners to better understand which input patterns are responsible for predictive uncertainty.", "VAEM: a deep generative model for heterogeneous mixed type data": "Deep generative models often perform poorly in real-world applications due to the heterogeneity of natural data sets. Heterogeneity arises from data containing different types of features (categorical, ordinal, continuous, etc.) and features of the same type having different marginal distributions. We propose an extension of variational autoencoders (VAEs) called VAEM to handle such heterogeneous data. VAEM is a deep generative model that is trained in a two stage manner such that the first stage provides a more uniform representation of the data to the second stage, thereby sidestepping the problems caused by heterogeneous data. We provide extensions of VAEM to handle partially observed data, and demonstrate its performance in data generation, missing data prediction and sequential feature selection tasks. Our results show that VAEM broadens the range of real-world applications where deep generative models can be successfully deployed.", "Decomposition of uncertainty for active learning and reliable reinforcement learning in stochastic systems": "Bayesian neural networks (BNNs) with latent variables are probabilistic models which can automatically identify complex stochastic patterns in the data. We study in these models a decomposition of predictive uncertainty into its epistemic and aleatoric components. We show how such a decomposition arises naturally in a Bayesian active learning scenario and develop a new objective for reliable reinforcement learning (RL) with an epistemic and aleatoric risk element. Our experiments illustrate the usefulness of the resulting decomposition in active learning and reliable RL.", "Quantifying mismatch in Bayesian optimization": "How does misspecifying prior smoothness assumptions about the target function affect the performance of Bayesian optimization routines? We show that misspecifying smoothness leads to an increase in regret, that this effect gets worse in higher dimensions, and that it remains substantial even if hyper-parameters are optimized.", "Interpretable outcome prediction with sparse Bayesian neural networks in intensive care": "Clinical decision making is challenging because of pathological complexity, as well as large amounts of heterogeneous data generated as part of routine clinical care. In recent years, machine learning tools have been developed to aid this process. Intensive care unit (ICU) admissions represent the most data dense and time-critical patient care episodes. In this context, prediction models may help clinicians determine which patients are most at risk and prioritize care. However, flexible tools such as artificial neural networks (ANNs) suffer from a lack of interpretability limiting their acceptability to clinicians. In this work, we propose a novel interpretable Bayesian neural network architecture which offers both the flexibility of ANNs and interpretability in terms of feature selection. In particular, we employ a sparsity inducing prior distribution in a tied manner to learn which features are important for outcome prediction. We evaluate our approach on the task of mortality prediction using two real-world ICU cohorts. In collaboration with clinicians we found that, in addition to the predicted outcome results, our approach can provide novel insights into the importance of different clinical measurements. This suggests that our model can support medical experts in their decision making process.", "Regulator discovery from gene expression time series of malaria parasites: a hierarchical approach": "We introduce a hierarchical Bayesian model for the discovery of putative regula tors from gene expression data only. The hierarchy incorporates the knowledge that there are just a few regulators that by themselves only regulate a handful of genes. This is implemented through a so-called spike-and-slab prior, a mix ture of Gaussians with different widths, with mixing weights from a hierarchical Bernoulli model. For efficient inference we implemented expectation propaga tion. Running the model on a malaria parasite data set, we found four genes with significant homology to transcription factors in an amoebe, one RNA regulator and three genes of unknown function (out of the top ten genes considered).", "Getting a clue: A method for explaining uncertainty estimates": "Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the input's prediction. We validate CLUE through 1) a novel framework for evaluating counterfactual explanations of uncertainty, 2) a series of ablation experiments, and 3) a user study. Our experiments show that CLUE outperforms baselines and enables practitioners to better understand which input patterns are responsible for predictive uncertainty.", "Nonlinear invariant risk minimization: A causal approach": "Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either explicitly or implicitly, attempted to find a data representation that has an invariant causal relationship with the target. This is done by leveraging a diverse set of training environments to reduce the effect of spurious features and build an invariant predictor. However, these methods have generalization guarantees only when both data representation and classifiers come from a linear model class. We propose Invariant Causal Representation Learning (ICRL), a learning paradigm that enables out-of-distribution (OOD) generalization in the nonlinear setting (i.e., nonlinear representations and nonlinear classifiers). It builds upon a practical and general assumption: the prior over the data representation factorizes when conditioning on the target and the environment. Based on this, we show identifiability of the data representation up to very simple transformations. We also prove that all direct causes of the target can be fully discovered, which further enables us to obtain generalization guarantees in the nonlinear setting. Extensive experiments on both synthetic and real-world datasets show that our approach significantly outperforms a variety of baseline methods. Finally, in the concluding discussion, we further explore the aforementioned assumption and propose a general view, called the Agnostic Hypothesis: there exist a set of hidden causal factors affecting both inputs and outcomes. The Agnostic Hypothesis can provide a unifying view of machine learning in \u2026", "Barking up the right tree: an approach to search over molecule synthesis dags": "When designing new molecules with particular properties, it is not only important what to make but crucially how to make it. These instructions form a synthesis directed acyclic graph (DAG), describing how a large vocabulary of simple building blocks can be recursively combined through chemical reactions to create more complicated molecules of interest. In contrast, many current deep generative models for molecules ignore synthesizability. We therefore propose a deep generative model that better represents the real world process, by directly outputting molecule synthesis DAGs. We argue that this provides sensible inductive biases, ensuring that our model searches over the same chemical space that chemists would also have access to, as well as interpretability. We show that our approach is able to model chemical space well, producing a wide range of diverse molecules, and allows for unconstrained optimization of an inherently constrained problem: maximize certain chemical properties such that discovered molecules are synthesizable.", "Expressive yet tractable bayesian deep learning via subnetwork inference": "The Bayesian paradigm has the potential to solve some of the core issues in modern deep learning, such as poor calibration, data inefficiency, and catastrophic forgetting. However, scaling Bayesian inference to the high-dimensional parameter spaces of deep neural networks requires restrictive approximations. In this paper, we propose performing inference over only a small subset of the model parameters while keeping all others as point estimates. This enables us to use expressive posterior approximations that would otherwise be intractable for the full model. In particular, we develop a practical and scalable Bayesian deep learning method that first trains a point estimate, and then infers a full covariance Gaussian posterior approximation over a subnetwork. We propose a subnetwork selection procedure which aims to maximally preserve posterior uncertainty. We empirically demonstrate the effectiveness of our approach compared to point-estimated networks and methods that use less expressive posterior approximations over the full network.", "Icebreaker: Element-wise efficient information acquisition with a bayesian deep latent gaussian model": "In this paper we introduce the ice-start problem, ie, the challenge of training machine learning models when only little or no training data is initially available, and acquiring each feature element of data is associated with costs. This setting is representative for real-world machine learning applications. For instance, in the health-care domain, when training an AI system for predicting patient metrics from lab tests, obtaining every single measurement comes with high cost. Active learning, where only the label is associated with a cost does not apply to such problem, because performing all possible lab tests to acquire a new training datum would be costly, as well as unnecessary due to redundancy. We propose Icebreaker, a principled framework to approach the ice-start problem. Icebreaker uses a full Bayesian Deep Latent Gaussian Model (BELGAM) with a novel inference method. Our proposed method combines recent advances in amortized inference and stochastic gradient MCMC to enable fast and accurate posterior inference. Utilizing BELGAM's ability to fully quantify model uncertainty, we also propose two information acquisition functions for imputation and active prediction problems. We demonstrate the excellent performance of Icebreaker in tasks relevant for recommender systems and health-care applications.", "A cold approach to generating optimal samples": "Optimising discrete data for a desired characteristic using gradient-based methods involves projecting the data into a continuous latent space and carrying out optimisation in this space. Carrying out global optimisation is difficult as optimisers are likely to follow gradients into regions of the latent space that the model has not been exposed to during training; samples generated from these regions are likely to be too dissimilar to the training data to be useful. We propose Constrained Optimisation with Latent Distributions (COLD), a constrained global optimisation procedure to find samples with high values of a desired property that are similar to yet distinct from the training data. We find that on MNIST, our procedure yields optima for each of three different objectives, and that enforcing tighter constraints improves the quality and increases the diversity of the generated images. On the ChEMBL molecular dataset, our method generates a diverse set of new molecules with drug-likeness scores similar to those of the highest-scoring molecules in the training data. We also demonstrate a computationally efficient way to approximate the constraint when evaluating it exactly is computationally expensive.", "Bringing representativeness into social media monitoring and analysis": "The opinions, expectations and behavior of citizens are increasingly reflected online - therefore, mining the Internet for such data can enhance decision-making in public policy, communications, marketing, finance and other fields. However, to come closer to the representativeness of classic opinion surveys there is a lack of knowledge about the socio-demographic characteristics of those voicing opinions on the internet. This paper proposes to calibrate online opinions aggregated from multiple and heterogeneous data sources with traditional surveys enhanced with rich socio-demographic information to enable insights into which opinions are expressed on the Internet by specific segments of society. The goal of this research is to provide professionals in citizen- and consumer-centered domains with more concise near real-time intelligence on online opinions. To become effective, the methodologies presented in \u2026", "GARCH processes with non-parametric innovations for market risk estimation": "A procedure to estimate the parameters of GARCH processes with non-parametric innovations is proposed. We also design an improved technique to estimate the density of heavy-tailed distributions with real support from empirical data. The performance of GARCH processes with non-parametric innovations is evaluated in a series of experiments on the daily log-returns of IBM stocks. These experiments demonstrate the capacity of the improved estimator to yield a precise quantification of market risk.", "Predictive Complexity Priors": "Specifying a Bayesian prior is notoriously difficult for complex models such as neural networks. Reasoning about parameters is made challenging by the high-dimensionality and over-parameterization of the space. Priors that seem benign and uninformative can have unintuitive and detrimental effects on a model\u2019s predictions. For this reason, we propose predictive complexity priors: a functional prior that is defined by comparing the model\u2019s predictions to those of a reference model. Although originally defined on the model outputs, we transfer the prior to the model parameters via a change of variables. The traditional Bayesian workflow can then proceed as usual. We apply our predictive complexity prior to high-dimensional regression, reasoning over neural network depth, and sharing of statistical strength for few-shot learning.", "Compressing images by encoding their latent representations with relative entropy coding": "Variational Autoencoders (VAEs) have seen widespread use in learned image compression. They are used to learn expressive latent representations on which downstream compression methods can operate with high efficiency. Recently proposed 'bits-back' methods can indirectly encode the latent representation of images with codelength close to the relative entropy between the latent posterior and the prior. However, due to the underlying algorithm, these methods can only be used for lossless compression, and they only achieve their nominal efficiency when compressing multiple images simultaneously; they are inefficient for compressing single images. As an alternative, we propose a novel method, Relative Entropy Coding (REC), that can directly encode the latent representation with codelength close to the relative entropy for single images, supported by our empirical results obtained on the Cifar10, ImageNet32 and Kodak datasets. Moreover, unlike previous bits-back methods, REC is immediately applicable to lossy compression, where it is competitive with the state-of-the-art on the Kodak dataset.", "Determining optimal coherency interface for many-accelerator socs using bayesian optimization": "The modern system-on-chip (SoC) of the current exascale computing era is complex. These SoCs not only consist of several general-purpose processing cores but also integrate many specialized hardware accelerators. Three common coherency interfaces are used to integrate the accelerators with the memory hierarchy: non-coherent,coherent with the last-level cache (LLC), and fully-coherent.However, using a single coherence interface for all the accelerators in an SoC can lead to significant overheads: in the non-coherent model, accelerators directly access the main memory, which can have considerable performance penalty; whereas in the LLC-coherent model, the accelerators access the LLC but may suffer from performance bottleneck due to contention between several accelerators; and the fully-coherent model, that relies on private caches, can incur non-trivial power/area overheads. Given the limitations \u2026", "Sensitivity analysis for predictive uncertainty in Bayesian neural networks": "We derive a novel sensitivity analysis of input variables for predictive epistemic and aleatoric uncertainty. We use Bayesian neural networks with latent variables as a model class and illustrate the usefulness of our sensitivity analysis on real-world datasets. Our method increases the interpretability of complex black-box probabilistic models.", "Sample-efficient reinforcement learning via counterfactual-based data augmentation": "Reinforcement learning (RL) algorithms usually require a substantial amount of interaction data and perform well only for specific tasks in a fixed environment. In some scenarios such as healthcare, however, usually only few records are available for each patient, and patients may show different responses to the same treatment, impeding the application of current RL algorithms to learn optimal policies. To address the issues of mechanism heterogeneity and related data scarcity, we propose a data-efficient RL algorithm that exploits structural causal models (SCMs) to model the state dynamics, which are estimated by leveraging both commonalities and differences across subjects. The learned SCM enables us to counterfactually reason what would have happened had another treatment been taken. It helps avoid real (possibly risky) exploration and mitigates the issue that limited experiences lead to biased policies. We propose counterfactual RL algorithms to learn both population-level and individual-level policies. We show that counterfactual outcomes are identifiable under mild conditions and that Q- learning on the counterfactual-based augmented data set converges to the optimal value function. Experimental results on synthetic and real-world data demonstrate the efficacy of the proposed approach.", "Excursion search for constrained bayesian optimization under a limited budget of failures": "When learning to ride a bike, a child falls down a number of times before achieving the first success. As falling down usually has only mild consequences, it can be seen as a tolerable failure in exchange for a faster learning process, as it provides rich information about an undesired behavior. In the context of Bayesian optimization under unknown constraints (BOC), typical strategies for safe learning explore conservatively and avoid failures by all means. On the other side of the spectrum, non conservative BOC algorithms that allow failing may fail an unbounded number of times before reaching the optimum. In this work, we propose a novel decision maker grounded in control theory that controls the amount of risk we allow in the search as a function of a given budget of failures. Empirical validation shows that our algorithm uses the failures budget more efficiently in a variety of optimization experiments, and generally achieves lower regret, than state-of-the-art methods. In addition, we propose an original algorithm for unconstrained Bayesian optimization inspired by the notion of excursion sets in stochastic processes, upon which the failures-aware algorithm is built.", "Odin: Optimal discovery of high-value information using model-based deep reinforcement learning": "We consider the problem of active feature selection where we dynamically choose the set of features that acquires the highest predictive performance relative to a task. We propose a modelbased deep reinforcement learning framework for Optimal Discovery of high-value INformation (ODIN) in which the agent either chooses to ask for a new feature or to stop and predict. Utilizing the ability of the partial variational autoencoder (Ma et al., 2018) the framework models the conditional distribution of the features allowing for data efficiency. We introduce a novel cost function that is sensitive to both cost and order of feature acquisition. ODIN handles missing data naturally and ensures the globally optimal solution for most efficient feature acquisition while preserving data efficiency. We show improved performance on both synthetic and real-life datasets.", "Deep Gaussian processes with decoupled inducing inputs": "Deep Gaussian Processes (DGP) are hierarchical generalizations of Gaussian Processes (GP) that have proven to work effectively on a multiple supervised regression tasks. They combine the well calibrated uncertainty estimates of GPs with the great flexibility of multilayer models. In DGPs, given the inputs, the outputs of the layers are Gaussian distributions parameterized by their means and covariances. These layers are realized as Sparse GPs where the training data is approximated using a small set of pseudo points. In this work, we show that the computational cost of DGPs can be reduced with no loss in performance by using a separate, smaller set of pseudo points when calculating the layerwise variance while using a larger set of pseudo points when calculating the layerwise mean. This enabled us to train larger models that have lower cost and better predictive performance.", "Hub gene selection methods for the reconstruction of transcription networks": "Transcription control networks have a scale-free topological structure: While most genes are involved in a reduced number of links, a few hubs or key regulators are connected to a significantly large number of nodes. Several methods have been developed for the reconstruction of these networks from gene expression data, e.g. ARACNE. However, few of them take into account the scale-free structure of transcription networks. In this paper, we focus on the hubs that commonly appear in scale-free networks. First, three feature selection methods are proposed for the identification of those genes that are likely to be hubs and second, we introduce an improvement in ARACNE so that this technique can take into account the list of hub genes generated by the feature selection methods. Experiments with synthetic gene expression data validate the accuracy of the feature selection methods in the task of identifying \u2026", "Bayesian deep learning via subnetwork inference": "The Bayesian paradigm has the potential to solve core issues of deep neural networks such as poor calibration and data inefficiency. Alas, scaling Bayesian inference to large weight spaces often requires restrictive approximations. In this work, we show that it suffices to perform inference over a small subset of model weights in order to obtain accurate predictive posteriors. The other weights are kept as point estimates. This subnetwork inference framework enables us to use expressive, otherwise intractable, posterior approximations over such subsets. In particular, we implement subnetwork linearized Laplace as a simple, scalable Bayesian deep learning method: We first obtain a MAP estimate of all weights and then infer a full-covariance Gaussian posterior over a subnetwork using the linearized Laplace approximation. We propose a subnetwork selection strategy that aims to maximally preserve the model\u2019s predictive uncertainty. Empirically, our approach compares favorably to ensembles and less expressive posterior approximations over full networks.", "Activation-level uncertainty in deep neural networks": "Current approaches for uncertainty estimation in deep learning often produce too confident results. Bayesian Neural Networks (BNNs) model uncertainty in the space of weights, which is usually high-dimensional and limits the quality of variational approximations. The more recent functional BNNs (fBNNs) address this only partially because, although the prior is specified in the space of functions, the posterior approximation is still defined in terms of stochastic weights. In this work we propose to move uncertainty from the weights (which are deterministic) to the activation function. Specifically, the activations are modelled with simple 1D Gaussian Processes (GP), for which a triangular kernel inspired by the ReLu non-linearity is explored. Our experiments show that activation-level stochasticity provides more reliable uncertainty estimates than BNN and fBNN, whereas it performs competitively in standard prediction tasks. We also study the connection with deep GPs, both theoretically and empirically. More precisely, we show that activation-level uncertainty requires fewer inducing points and is better suited for deep architectures.", "DRIFT: Deep Reinforcement Learning for Functional Software Testing": "Efficient software testing is essential for productive software development and reliable user experiences. As human testing is inefficient and expensive, automated software testing is needed. In this work, we propose a Reinforcement Learning (RL) framework for functional software testing named DRIFT. DRIFT operates on the symbolic representation of the user interface. It uses Q-learning through Batch-RL and models the state-action value function with a Graph Neural Network. We apply DRIFT to testing the Windows 10 operating system and show that DRIFT can robustly trigger the desired software functionality in a fully automated manner. Our experiments test the ability to perform single and combined tasks across different applications, demonstrating that our framework can efficiently test software with a large range of testing objectives.", "Variational depth search in ResNets": "One-shot neural architecture search allows joint learning of weights and network architecture, reducing computational cost. We limit our search space to the depth of residual networks and formulate an analytically tractable variational objective that allows for obtaining an unbiased approximate posterior over depths in one-shot. We propose a heuristic to prune our networks based on this distribution. We compare our proposed method against manual search over network depths on the MNIST, Fashion-MNIST, SVHN datasets. We find that pruned networks do not incur a loss in predictive performance, obtaining accuracies competitive with unpruned networks. Marginalising over depth allows us to obtain better-calibrated test-time uncertainty estimates than regular networks, in a single forward pass.", "Convergent expectation propagation in linear models with spike-and-slab priors": "Exact inference in the linear regression model with spike and slab priors is often intractable. Expectation propagation (EP) can be used for approximate inference. However, the regular sequential form of EP (R-EP) may fail to converge in this model when the size of the training set is very small. As an alternative, we propose a provably convergent EP algorithm (PC-EP). PC-EP is proved to minimize an energy function which, under some constraints, is bounded from below and whose stationary points coincide with the solution of R-EP. Experiments with synthetic data indicate that when R-EP does not converge, the approximation generated by PC-EP is often better. By contrast, when R-EP converges, both methods perform similarly.", "Gaussianity Measures for Detecting the Direction of Causal Time Series": "We conjecture that the distribution of the time-reversed residuals of a causal linear process is closer to a Gaussian than the distribution of the noise used to generate the process in the forward direction. This property is demonstrated for causal AR (1) processes assuming that all the cumulants of the distribution of the noise are defined. Based on this observation, it is possible to design a decision rule for detecting the direction of time series that can be described as linear processes: The correct direction (forward in time) is the one in which the residuals from a linear fit to the time series are less Gaussian. A series of experiments with simulated and real-world data illustrate the superior results of the proposed rule when compared with other state-of-the-art methods based on independence tests.", "Improving black-box optimization in VAE latent space using decoder uncertainty": "Optimization in the latent space of variational autoencoders is a promising approach to generate high-dimensional discrete objects that maximize an expensive black-box property (eg, drug-likeness in molecular generation, function approximation with arithmetic expressions). However, existing methods lack robustness as they may decide to explore areas of the latent space for which no data was available during training and where the decoder can be unreliable, leading to the generation of unrealistic or invalid objects. We propose to leverage the epistemic uncertainty of the decoder to guide the optimization process. This is not trivial though, as a naive estimation of uncertainty in the high-dimensional and structured settings we consider would result in high estimator variance. To solve this problem, we introduce an importance sampling-based estimator that provides more robust estimates of epistemic uncertainty. Our uncertainty-guided optimization approach does not require modifications of the model architecture nor the training process. It produces samples with a better trade-off between black-box objective and validity of the generated samples, sometimes improving both simultaneously. We illustrate these advantages across several experimental settings in digit generation, arithmetic expression approximation and molecule generation for drug design.", "Large-scale educational question analysis with partial variational auto-encoders": "Online education platforms enable teachers to share a large number of educational resources such as questions to form exercises and quizzes for students. With large volumes of such crowd-sourced questions, quantifying the properties of these questions in crowd-sourced online education platforms is of great importance to enable both teachers and students to find high-quality and suitable resources. In this work, we propose a framework for large-scale question analysis. We utilize the state-of-the-art Bayesian deep learning method, in particular partial variational auto-encoders, to analyze real-world educational data. We also develop novel objectives to quantify question quality and difficulty. We apply our proposed framework to a real-world cohort with millions of question-answer pairs from an online education platform. Our framework not only demonstrates promising results in terms of statistical metrics but also obtains highly consistent results with domain expert evaluation.", "HM-VAEs: a deep generative model for real-valued data with heterogeneous marginals": "In this paper, we propose a very simple but e ective VAE model (HM-VAE) that can handle real-valued data with heterogeneous marginals, meaning that they have drastically distinct marginal distributions, statistical properties as well as semantics. Preliminary results show that the HM-VAE can learn distributions with heterogeneous marginal distributions, whereas the vanilla VAEs fails.", "Compression without Quantization": "Standard compression algorithms work by mapping an image to discrete code using an encoder from which the original image can be reconstructed through a decoder. This process, due to the quantization step, is inherently non-differentiable so these algorithms must rely on approximate methods to train the encoder and decoder end-to-end. In this paper, we present an innovative framework for lossy image compression which is able to circumvent the quantization step by relying on a non-deterministic compression codec. The decoder maps the input image to a distribution in continuous space from which a sample can be encoded with expected code length being the relative entropy to the encoding distribution, ie it is bits-back efficient. The result is a principled, end-to-end differentiable compression framework that can be straight-forwardly trained using standard gradient-based optimizers. To showcase the efficiency of our method, we apply it to lossy image compression by training Probabilistic Ladder Networks (PLNs) on the CLIC 2018 dataset and show that their rate-distortion curves on the Kodak dataset are competitive with the state-of-the-art on low bitrates.", "Generating molecules via chemical reactions": "Over the last few years exciting work in deep generative models has produced models able to suggest new organic molecules by generating strings, trees, and graphs representing their structure. While such models are able to generate molecules with desirable properties, their utility in practice is limited due to the difficulty in knowing how to synthesize these molecules. We therefore propose a new molecule generation model, mirroring a more realistic real-world process, where reactants are selected and combined to form more complex molecules. More specifically, our generative model proposes a bag of initial reactants (selected from a pool of commercially-available molecules) and uses a reaction model to predict how they react together to generate new molecules. Modeling the entire process of constructing a molecule during generation offers a number of advantages. First, we show that such a model has the ability to generate a wide, diverse set of valid and unique molecules due to the useful inductive biases of modeling reactions. Second, modeling synthesis routes rather than final molecules offers practical advantages to chemists who are not only interested in new molecules but also suggestions on stable and safe synthetic routes. Third, we demonstrate the capabilities of our model to also solve one-step retrosynthesis problems, predicting a set of reactants that can produce a target product.", "Distributed thompson sampling for large-scale accelerated exploration of chemical space": "Chemical space is so large that brute force searches for new interesting molecules are infeasible. High-throughput virtual screening can speed up the discovery process by collecting very large amounts of data in parallel, eg, up to hundreds or thousands of parallel measurements. Bayesian optimization (BO) can produce additional acceleration by sequentially identifying the most useful simulations or experiments to be performed. However, current BO methods cannot scale to the large numbers of parallel measurements and the massive libraries of molecules currently used in high-throughput screening. Here, we propose a scalable solution based on a distributed implementation of the Thompson sampling heuristic. We show that this method outperforms other scalable approaches such as a greedy search strategy, \u03f5-greedy approaches and a random search method.", "Learning the semantics of discrete random variables: Ordinal or categorical": "When specifying a probabilistic model of data, the form of the model will typically depend on the spaces in which random variables take their values. In particular, different probability distributions are appropriate for continuous, discrete and binary data. As we respond to ever increasing quantities of data, with increasingly more automatic data analysis techniques, it is necessary to identify these different types of data automatically. While it is trivial to create concise logical rules to distinguish between many different types of data, this cannot be said for choosing between categorical and ordinal data, let alone inferring the ordering. We present some first attempts at this problem and evaluate their performance empirically.", "Modeling dependence in financial data with semiparametric Archimedean copulas": "Modeling Dependence in Financial Data with Semiparametric Archimedean Copulas Page 1 Introduction to copulas Semiparametric Archimedean copulas Experiments with financial data Modeling Dependence in Financial Data with Semiparametric Archimedean Copulas Jos\u00e9 Miguel Hern\u00e1ndez-Lobato and Alberto Su\u00e1rez Universidad Aut\u00f3noma de Madrid July 17, 2009 1 / 25 Page 2 Introduction to copulas Semiparametric Archimedean copulas Experiments with financial data Outline 1 Introduction to copulas 2 Semiparametric Archimedean copulas 3 Experiments with financial data 2 / 25 Page 3 Introduction to copulas Semiparametric Archimedean copulas Experiments with financial data Outline 1 Introduction to copulas 2 Semiparametric Archimedean copulas 3 Experiments with financial data 3 / 25 Page 4 Introduction to copulas Semiparametric Archimedean copulas Experiments with financial data Copulas \u2026", "A Gradient Based Strategy for Hamiltonian Monte Carlo Hyperparameter Optimization": "Hamiltonian Monte Carlo (HMC) is one of the most successful sampling methods in machine learning. However, its performance is significantly affected by the choice of hyperparameter values. Existing approaches for optimizing the HMC hyperparameters either optimize a proxy for mixing speed or consider the HMC chain as an implicit variational distribution and optimize a tractable lower bound that can be very loose in practice. Instead, we propose to optimize an objective that quantifies directly the speed of convergence to the target distribution. Our objective can be easily optimized using stochastic gradient descent. We evaluate our proposed method and compare to baselines on a variety of problems including sampling from synthetic 2D distributions, reconstructing sparse signals, learning deep latent variable models and sampling molecular configurations from the Boltzmann distribution of a 22 atom molecule. We find that our method is competitive with or improves upon alternative baselines in all these experiments.", "Results and Insights from Diagnostic Questions: The NeurIPS 2020 Education Challenge": "This competition concerns educational diagnostic questions, which are pedagogically effective, multiple-choice questions (MCQs) whose distractors embody misconceptions. With a large and ever-increasing number of such questions, it becomes overwhelming for teachers to know which questions are the best ones to use for their students. We thus seek to answer the following question: how can we use data on hundreds of millions of answers to MCQs to drive automatic personalized learning in large-scale learning scenarios where manual personalization is infeasible? Success in using MCQ data at scale helps build more intelligent, personalized learning platforms that ultimately improve the quality of education en masse. To this end, we introduce a new, large-scale, real-world dataset and formulate 4 data mining tasks on MCQs that mimic real learning scenarios and target various aspects of the above question in a competition setting at NeurIPS 2020. We report on our NeurIPS competition in which nearly 400 teams submitted approximately 4000 submissions, with encouragingly diverse and effective approaches to each of our tasks.", "A comprehensive methodology to determine optimal coherence interfaces for many-accelerator SoCs": "Modern systems-on-chip (SoCs) include not only general-purpose CPUs but also specialized hardware accelerators. Typically, there are three coherence model choices to integrate an accelerator with the memory hierarchy: no coherence, coherent with the last-level cache (LLC), and private cache based full coherence. However, there has been very limited research on finding which coherence models are optimal for the accelerators of a complex many-accelerator SoC. This paper focuses on determining a cost-aware coherence interface for an SoC and its target application: find the best coherence models for the accelerators that optimize their power and performance, considering both workload characteristics and system-level contention. A novel comprehensive methodology is proposed that uses Bayesian optimization to efficiently find the cost-aware coherence interfaces for SoCs that are modeled using the \u2026", "Refining the variational posterior through iterative optimization": "Variational inference (VI) is a popular approach for approximate Bayesian inference that is particularly promising for highly parameterized models such as deep neural networks. A key challenge of variational inference is to approximate the posterior over model parameters with a distribution that is simpler and tractable yet sufficiently expressive. In this work, we propose a method for training highly flexible variational distributions by starting with a coarse approximation and iteratively refining it. Each refinement step makes cheap, local adjustments and only requires optimization of simple variational families. We demonstrate theoretically that our method always improves a bound on the approximation (the Evidence Lower BOund) and observe this empirically across a variety of benchmark tasks. In experiments, our method consistently outperforms recent variational inference methods for deep learning in terms of log-likelihood and the ELBO. We see that the gains are further amplified on larger scale models, significantly outperforming standard VI and deep ensembles on residual networks on CIFAR10.", "Method and system for mining frequent and in-frequent items from a large transaction database": "The technique relates to a system and method for mining frequent and in-frequent items from a large transaction database to provide the dynamic recommendation of items. The method involves determining user interest for an item by monitoring short item behavior of at least one user then selecting a local category, a neighborhood category and a disjoint category with respect to the item clicked by the at least one user based on long term preferences data of a plurality of users of the ecommerce environment thereafter selecting one or more frequent and infrequent items from each of the selected local, neighborhood and disjoint category items and finally generating one or more dynamic recommendations based on the one or more items selected from the local category, the neighborhood category and the disjoint category and the one or more selected frequent and infrequent items.", "Ergodic inference: Accelerate convergence by optimisation": "Statistical inference methods are fundamentally important in machine learning. Most state-of-the-art inference algorithms are variants of Markov chain Monte Carlo (MCMC) or variational inference (VI). However, both methods struggle with limitations in practice: MCMC methods can be computationally demanding; VI methods may have large bias. In this work, we aim to improve upon MCMC and VI by a novel hybrid method based on the idea of reducing simulation bias of finite-length MCMC chains using gradient-based optimisation. The proposed method can generate low-biased samples by increasing the length of MCMC simulation and optimising the MCMC hyper-parameters, which offers attractive balance between approximation bias and computational efficiency. We show that our method produces promising results on popular benchmarks when compared to recent hybrid methods of MCMC and VI.", "Automatic depth determination for Bayesian ResNets": "The size of neural networks (NNs) is increasing at a steady pace, and as these models gain ever more capacity, proper regularization and model selection become increasingly important. Currently, a deep learning practitioner often needs a GPU cluster to thoroughly search over architectures, regularization strength, and other hyper-parameters. Automatic methods for hyper-parameter tuning based on genetic algorithms [19], reinforcement learning [32], and Bayesian optimization [25] have achieved some success but still can be slow simply due to the challenges of combinatorial optimization. Bayesian inference presents an alternative approach that integrates over parameter uncertainty to find the most suitable model. This is known as the evidence framework [16] and has been called the occam\u2019s razor effect of Bayesian inference [17, 20].\nAutomatic relevance determination (ARD)[18, 22, 28] is the best known Bayesian method that can automatically tune the size of a neural architecture. Specifically, ARD selects a NN\u2019s number of hidden units by placing structured priors on the NN\u2019s weights. Denoting a weight in row i and column j as wi, j, the ARD prior is defined as", "Competitive and collaborative mixtures of experts for financial risk analysis": "We compare the performance of competitive and collaborative strategies for mixtures of autoregressive experts with normal innovations for conditional risk analysis in financial time series. The prediction of the mixture of collaborating experts is an average of the outputs of the experts. If a competitive strategy is used the prediction is generated by a single expert. The expert that becomes activated is selected either deterministically (hard competition) or at random, with a certain probability (soft competition). The different strategies are compared in a sliding window experiment for the time series of log-returns of the Spanish stock index IBEX 35, which is preprocessed to account for the heteroskedasticity of the series. Experiments indicate that the best performance for risk analysis is obtained by mixtures with soft competition, where the experts have a probability of activation given by the output of a gating \u2026", "Resampling Base Distributions of Normalizing Flows": "Normalizing flows are a popular class of models for approximating probability distributions. However, their invertible nature limits their ability to model target distributions with a complex topological structure, such as Boltzmann distributions. Several procedures have been proposed to solve this problem but many of them sacrifice invertibility and, thereby, tractability of the log-likelihood as well as other desirable properties. To address these limitations, we introduce a base distribution for normalizing flows based on learned rejection sampling, allowing the resulting normalizing flow to model complex topologies without giving up bijectivity. Furthermore, we develop suitable learning algorithms using both maximizing the log-likelihood and the optimization of the reverse Kullback-Leibler divergence, and apply them to various sample problems, i.e.\\ approximating 2D densities, density estimation of tabular data, image generation, and modeling Boltzmann distributions. In these experiments our method is competitive with or outperforms the baselines.", "Action-Sufficient State Representation Learning for Control with Structural Constraints": "Perceived signals in real-world scenarios are usually high-dimensional and noisy, and finding and using their representation that contains essential and sufficient information required by downstream decision-making tasks will help improve computational efficiency and generalization ability in the tasks. In this paper, we focus on partially observable environments and propose to learn a minimal set of state representations that capture sufficient information for decision-making, termed \\textit{Action-Sufficient state Representations} (ASRs). We build a generative environment model for the structural relationships among variables in the system and present a principled way to characterize ASRs based on structural constraints and the goal of maximizing cumulative reward in policy learning. We then develop a structured sequential Variational Auto-Encoder to estimate the environment model and extract ASRs. Our empirical results on CarRacing and VizDoom demonstrate a clear advantage of learning and using ASRs for policy learning. Moreover, the estimated environment model and ASRs allow learning behaviors from imagined outcomes in the compact latent space to improve sample efficiency.", "FIT: a Fast and Accurate Framework for Solving Medical Inquiring and Diagnosing Tasks": "Automatic self-diagnosis provides low-cost and accessible healthcare via an agent that queries the patient and makes predictions about possible diseases. From a machine learning perspective, symptom-based self-diagnosis can be viewed as a sequential feature selection and classification problem. Reinforcement learning methods have shown good performance in this task but often suffer from large search spaces and costly training. To address these problems, we propose a competitive bipartite framework, called FIT, which uses an information-theoretic reward to determine what data to collect next. FIT improves over previous information-based approaches by using a multimodal variational autoencoder (MVAE) model and a two-step sampling strategy for disease prediction. Furthermore, we propose novel methods to substantially reduce the computational cost of FIT to a level that is acceptable for practical online self-diagnosis. Our results in two simulated datasets show that FIT can effectively deal with large search space problems, outperforming existing RL baselines. Moreover, using several public medical datasets, we show that FIT is a competitive alternative in various real-world settings.", "Invariant Causal Representation Learning": "Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either explicitly or implicitly, attempted to find a data representation that has an invariant causal relationship with the outcome. This is done by leveraging a diverse set of training environments to reduce the effect of spurious features, on top of which an invariant classifier is then built. However, these methods have generalization guarantees only when both data representation and classifiers come from a linear model class. As an alternative, we propose Invariant Causal Representation Learning (ICRL), a learning paradigm that enables out-of-distribution generalization in the nonlinear setting (ie, nonlinear representations and nonlinear classifiers). It builds upon a practical and general assumption: data representations factorize when conditioning on the outcome and the environment. Based on this, we show identifiability up to a permutation and pointwise transformation. We also prove that all direct causes of the outcome can be fully discovered, which further enables us to obtain generalization guarantees in the nonlinear setting. Extensive experiments on both synthetic and real-world datasets show that our approach significantly outperforms a variety of baseline methods.", "Principled uncertainty estimation for high dimensional data": "The ability to quantify the uncertainty in the prediction of a Bayesian deep learning model has significant practical implications\u2014from more robust machine-learning based systems to more effective expert-in-the loop processes. While several general measures of model uncertainty exist, they are often intractable in practice when dealing with high dimensional data such as long sequences. Instead, researchers often resort to ad hoc approaches or to introducing independence assumptions to make computation tractable. We introduce a principled approach to estimate uncertainty in high dimensions that circumvents these challenges, and demonstrate its benefits in de novo molecular design.", "Importance weighted autoencoders with random neural network parameters": "Deep generative models for unsupervised learning have recently received considerable attention [1, 2, 3, 4, 5, 6]. These models can find a set of low-dimensional representative latent features that can accurately describe observed data. Furthermore, they can also infer the underlying mechanism that generates, from these features, new data instances similar to the observed ones. In general, these models need to perform posterior inference during learning, a task that is carried out by training, in addition to the top-down generative network, a bottom-up recognition network. This recognition network is used to predict the posterior distribution of the latent variables given the observed ones.\nVariational autoencoders (VAEs) are a family of generative models in which the parameters of the generative network and the recognition network are optimized during training to maximize a lower bound on the log-likelihood [3, 5, 6]. The VAE in [3] defines a generative process p (xi| zi, \u03b8) for the observed variables of the i-th data instance, xi, given the corresponding latent variables zi. p (xi| zi, \u03b8) is set to be a factorized Gaussian distribution (or a product of Bernoulli distributions in the case of binary data) whose mean and variance is computed by a deterministic feed-forward neural network with parameters \u03b8. The recognition network computes q (zi| xi, \u03c6), an approximation to p (zi| xi). q (zi| xi, \u03c6) is also a factorized Gaussian whose mean and variance is also computed by a feed-forward network with parameters \u03c6. The prior for each zi, p (zi), is set to be a product of standard Gaussians. During training, \u03b8 and \u03c6 are found by maximizing the lower bound", "Stochastic Expectation Propagation for Large Scale Gaussian Process Classification": "A method for large scale Gaussian process classification has been recently proposed based on expectation propagation (EP). Such a method allows Gaussian process classifiers to be trained on very large datasets that were out of the reach of previous deployments of EP and has been shown to be competitive with related techniques based on stochastic variational inference. Nevertheless, the memory resources required scale linearly with the dataset size, unlike in variational methods. This is a severe limitation when the number of instances is very large. Here we show that this problem is avoided when stochastic EP is used to train the model.", "The new generation of neural networks": "The deep architecture is initially empty. 1 Learn an RBM and put it on the top. 2 Filter the data through the current deep architecture. 3 Learn an RBM using the filtered data and put it on the top. 4 Filter the data through the current deep architecture. 5 Repeat 3 and 4 until n RBMs have been stacked. To filter the data through the deep architecture we just propagate expectations up in the RBMs, conditioning to the original data.", "Depth Uncertainty Networks for Active Learning": "In active learning, the size and complexity of the training dataset changes over time. Simple models that are well specified by the amount of data available at the start of active learning might suffer from bias as more points are actively sampled. Flexible models that might be well suited to the full dataset can suffer from overfitting towards the start of active learning. We tackle this problem using Depth Uncertainty Networks (DUNs), a BNN variant in which the depth of the network, and thus its complexity, is inferred. We find that DUNs outperform other BNN variants on several active learning tasks. Importantly, we show that on the tasks in which DUNs perform best they present notably less overfitting than baselines.", "Addressing Bias in Active Learning with Depth Uncertainty Networks... or Not": "Farquhar et al. [2021] show that correcting for active learning bias with underparameterised models leads to improved downstream performance. For overparameterised models such as NNs, however, correction leads either to decreased or unchanged performance. They suggest that this is due to an \"overfitting bias\" which offsets the active learning bias. We show that depth uncertainty networks operate in a low overfitting regime, much like underparameterised models. They should therefore see an increase in performance with bias correction. Surprisingly, they do not. We propose that this negative result, as well as the results Farquhar et al. [2021], can be explained via the lens of the bias-variance decomposition of generalisation error.", "Functional Variational Inference based on Stochastic Process Generators": "Bayesian inference in the space of functions has been an important topic for Bayesian modeling in the past. In this paper, we propose a new solution to this problem called Functional Variational Inference (FVI). In FVI, we minimize a divergence in function space between the variational distribution and the posterior process. This is done by using as functional variational family a new class of flexible distributions called Stochastic Process Generators (SPGs), which are cleverly designed so that the functional ELBO can be estimated efficiently using analytic solutions and mini-batch sampling. FVI can be applied to stochastic process priors when random function samples from those priors are available. Our experiments show that FVI consistently outperforms weight-space and function space VI methods on several tasks, which validates the effectiveness of our approach.", "Bootstrap Your Flow": "Normalising flows are flexible, parameterized distributions that can be used to approximate expectations from intractable distributions via importance sampling. However, current flow-based approaches are limited on challenging targets where they either suffer from mode seeking behaviour or high variance in the training loss, or rely on samples from the target distribution, which may not be available. To address these challenges, we combine flows with annealed importance sampling (AIS), while using the -divergence as our objective, in a novel training procedure, FAB (Flow AIS Bootstrap). Thereby, the flow and AIS to improve each other in a bootstrapping manner. We demonstrate that FAB can be used to produce accurate approximations to complex target distributions, including Boltzmann distributions, in problems where previous flow-based methods fail.", "Variational auto encoder for mixed data types": "In a first stage, training each of a plurality of first variational auto encoders, VAEs, each comprising: a respective first encoder arranged to encode a respective subset of one or more features of a feature space into a respective first latent representation, and a respective first decoder arranged to decode from the respective latent representation back to a decoded version of the respective subset of the feature space, wherein different subsets comprise features of different types of data. In a second stage following the first stage, training a second VAE comprising: a second encoder arranged to encode a plurality of inputs into a second latent representation, and a second decoder arranged to decode the second latent representation into decoded versions of the first latent representations, wherein each of the plurality of inputs comprises a combination of a different respective one of feature subsets with the respective first \u2026", "Sampling the Variational Posterior with Local Refinement": "Variational inference is an optimization-based method for approximating the posterior distribution of the parameters in Bayesian probabilistic models. A key challenge of variational inference is to approximate the posterior with a distribution that is computationally tractable yet sufficiently expressive. We propose a novel method for generating samples from a highly flexible variational approximation. The method starts with a coarse initial approximation and generates samples by refining it in selected, local regions. This allows the samples to capture dependencies and multi-modality in the posterior, even when these are absent from the initial approximation. We demonstrate theoretically that our method always improves the quality of the approximation (as measured by the evidence lower bound). In experiments, our method consistently outperforms recent variational inference methods in terms of log-likelihood and ELBO across three example tasks: the Eight-Schools example (an inference task in a hierarchical model), training a ResNet-20 (Bayesian inference in a large neural network), and the Mushroom task (posterior sampling in a contextual bandit problem).", "DOCKSTRING: easy molecular docking yields better benchmarks for ligand design": "The field of machine learning for drug discovery is witnessing an explosion of novel methods. These methods are often benchmarked on simple physicochemical properties such as solubility or general druglikeness, which can be readily computed. However, these properties are poor representatives of objective functions in drug design, mainly because they do not depend on the candidate's interaction with the target. By contrast, molecular docking is a widely successful method in drug discovery to estimate binding affinities. However, docking simulations require a significant amount of domain knowledge to set up correctly which hampers adoption. To this end, we present DOCKSTRING, a bundle for meaningful and robust comparison of ML models consisting of three components: (1) an open-source Python package for straightforward computation of docking scores; (2) an extensive dataset of docking scores and poses of more than 260K ligands for 58 medically-relevant targets; and (3) a set of pharmaceutically-relevant benchmark tasks including regression, virtual screening, and de novo design. The Python package implements a robust ligand and target preparation protocol that allows non-experts to obtain meaningful docking scores. Our dataset is the first to include docking poses, as well as the first of its size that is a full matrix, thus facilitating experiments in multiobjective optimization and transfer learning. Overall, our results indicate that docking scores are a more appropriate evaluation objective than simple physicochemical properties, yielding more realistic benchmark tasks and molecular candidates.", "Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation": "Machine learning training methods depend plentifully and intricately on hyperparameters, motivating automated strategies for their optimisation. Many existing algorithms restart training for each new hyperparameter choice, at considerable computational cost. Some hypergradient-based one-pass methods exist, but these either cannot be applied to arbitrary optimiser hyperparameters (such as learning rates and momenta) or take several times longer to train than their base models. We extend these existing methods to develop an approximate hypergradient-based hyperparameter optimiser which is applicable to any continuous hyperparameter appearing in a differentiable model weight update, yet requires only one training episode, with no restarts. We also provide a motivating argument for convergence to the true hypergradient, and perform tractable gradient-based optimisation of independent learning rates for each model parameter. Our method performs competitively from varied random hyperparameter initialisations on several UCI datasets and Fashion-MNIST (using a one-layer MLP), Penn Treebank (using an LSTM) and CIFAR-10 (using a ResNet-18), in time only 2-3x greater than vanilla training.", "A Fresh Look at De Novo Molecular Design Benchmarks": "De novo molecular design is a thriving research area in machine learning (ML) that lacks ubiquitous, high-quality, standardized benchmark tasks. Many existing benchmark tasks do not precisely specify a training dataset or an evaluation budget, which is problematic as they can significantly affect the performance of ML algorithms. This work elucidates the effect of dataset sizes and experimental budgets on established molecular optimization methods through a comprehensive evaluation with 11 selected benchmark tasks. We observe that the dataset size and budget significantly impact all methods' performance and relative ranking, suggesting that a meaningful comparison requires more than a single benchmark setup. Our results also highlight the relative difficulty of benchmarks, implying in particular that logP and QED are poor objectives. We end by offering guidance to researchers on their choice of experiments.", "Contextual HyperNetworks for Novel Feature Adaptation": "While deep learning has obtained state-of-the-art results in many applications, the adaptation of neural network architectures to incorporate new output features remains a challenge, as neural networks are commonly trained to produce a fixed output dimension. This issue is particularly severe in online learning settings, where new output features, such as items in a recommender system, are added continually with few or no associated observations. As such, methods for adapting neural networks to novel features which are both time and data-efficient are desired. To address this, we propose the Contextual HyperNetwork (CHN), an auxiliary model which generates parameters for extending the base model to a new feature, by utilizing both existing data as well as any observations and/or metadata associated with the new feature. At prediction time, the CHN requires only a single forward pass through a neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. To assess the performance of CHNs, we use a CHN to augment a partial variational autoencoder (P-VAE), a deep generative model which can impute the values of missing features in sparsely-observed data. We show that this system obtains improved few-shot learning performance for novel features over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks.", "Active Slices for Sliced Stein Discrepancy": "Sliced Stein discrepancy (SSD) and its kernelized variants have demonstrated promising successes in goodness-of-fit tests and model learning in high dimensions. Despite their theoretical elegance, their empirical performance depends crucially on the search of optimal slicing directions to discriminate between two distributions. Unfortunately, previous gradient-based optimisation approaches for this task return sub-optimal results: they are computationally expensive, sensitive to initialization, and they lack theoretical guarantees for convergence. We address these issues in two steps. First, we provide theoretical results stating that the requirement of using optimal slicing directions in the kernelized version of SSD can be relaxed, validating the resulting discrepancy with finite random slicing directions. Second, given that good slicing directions are crucial for practical performance, we propose a fast algorithm for finding such slicing directions based on ideas of active sub-space construction and spectral decomposition. Experiments on goodness-of-fit tests and model learning show that our approach achieves both improved performance and faster convergence. Especially, we demonstrate a 14-80x speed-up in goodness-of-fit tests when comparing with gradient-based alternatives.", "Collecting observations for machine learning": "A method of training a model comprising a generative network mapping a latent vector to a feature vector, wherein weights in the generative network are modelled as probabilistic distributions. The method comprises: a) obtaining one or more observed data points, each comprising an incomplete observation of the features in the feature vector; b) training the model based on the observed data points to learn values of the weights of the generative network which map the latent vector to the feature vector; c) from amongst a plurality of potential next features to observe, searching for a target feature of the feature vector which maximizes a measure of expected reduction in uncertainty in a distribution of said weights of the generative network given the observed data points so far; and d) outputting a request to collect a target data point comprising at least the target feature.", "Interpretable neural network": "A method of operating a neural network, comprising: at each input node of an input layer, weighting a respective input element received by that node by applying a first class of probability distribution, thereby generating a respective set of output parameters describing an output probability distribution; and from each input node, outputting the respective set of output parameters to one or more nodes in a next, hidden layer of the network, thereby propagating the respective set of output parameters through the hidden layers to an output layer; the propagating comprising, at one or more nodes of at least one hidden layer, combining the sets of input parameters and weighting the combination by applying a second class of probability distribution, thereby generating a respective set of output parameters describing an output probability distribution, wherein the first class of probability distribution is more sparsity inducing \u2026", "Gradient-based tuning of Hamiltonian Monte Carlo hyperparameters": "Hamiltonian Monte Carlo (HMC) is one of the most successful sampling methods in machine learning. However, its performance is significantly affected by the choice of hyperparameter values, which require careful tuning. Existing approaches for automating this task either optimise a proxy for mixing speed or consider the HMC chain as an implicit variational distribution and optimize a tractable lower bound that is too loose to be useful in practice. Instead, we propose to optimize an objective that quantifies directly the speed of convergence to the target distribution. Our objective can be easily optimized using stochastic gradient descent. We evaluate our proposed method and compare to baselines on a variety of problems including synthetic 2D distributions, the posteriors of variational autoencoders and the Boltzmann distribution for molecular configurations of a 22 atom molecule. We find our method is competitive with or improves upon alternative baselines on all problems we consider.", "Sample-Efficient Optimization in the Latent Space of Deep Generative Models via Weighted Retraining": "Sample-Efficient Optimization in the Latent Space of Deep Generative Models via Weighted Retraining Page 1 Sample-Efficient Optimization in the Latent Space of Deep Generative Models via Weighted Retraining Erik Daxberger*, Austin Tripp* & Jos\u00e9 Miguel Hern\u00e1ndez-Lobato RealML @ ICML2020 2020-07-18 Page 2 Problem Optimization of expensive, black box functions on structured input spaces. Examples: Drug Design Materials Discovery Neural Architecture Search Erik Daxberger*, Austin Tripp* & Jos\u00e9 Miguel Hern\u00e1ndez-Lobato Keyword: Weighted Retraining 2/4 Page 3 Latent Space Optimization Optimize in the latent space Z of a deep generative model (instead of data space X) \u2662 \u2662 Z Z Normal \u2662 \u2662 Z Z With weighted retraining Erik Daxberger*, Austin Tripp* & Jos\u00e9 Miguel Hern\u00e1ndez-Lobato Keyword: Weighted Retraining 3/4 Page 4 Results Chemical Design Task 0 100 200 300 400 500 Number of \u2026", "Data retrieval": "In various examples there is a data retrieval apparatus. The apparatus has a processor configured to receive a data retrieval request associated with a user. The apparatus also has a machine learning system configured to compute an affinity matrix of users for data items. The affinity matrix has a plurality of observed ratings of data items, and a plurality of predicted ratings of data items. The processor is configured to output a ranked list of data items for the user according to contents of the affinity matrix.", "Gathering data in a communication system": "A computer-implemented method comprising: outputting questions to a user via one or more user devices, and receiving back responses to some of the questions from the user via one or more user devices; over time, controlling the outputting of the questions so as to output the questions under circumstances of different values for each of one or more items of metadata, wherein the one or more items of metadata comprise at least a time and/or a location at which a question was output to the user via the one or more user devices; monitoring whether or not the user responds when the question is output with the different metadata values; training the machine learning algorithm to learn a value of each of the items of metadata which optimizes a reward function, and based thereon selecting a time and/or location at which to output subsequent questions.", "Bayesian EDDI: Sequential Variable Selection with Bayesian Partial VAE": "Obtaining more relevant information enables better decision making, but may be costly. Optimal sequential decision making allows us to trade off the desire to make good decisions by acquiring further information with the cost of performing that acquisition. To this end, we propose a principled framework, named EDDI (Efficient Dynamic Discovery of high-value Information), based on the theory of Bayesian experimental design. In EDDI we propose a novel partial variational autoencoder (Partial VAE), to efficiently handle missing data with different missing patterns. Additionally, we extend the VAE based framework to Bayesian treatment of the weights, which obtains better performance in small data regime. EDDI then combines it with an acquisition function that maximizes expected information gain on a set of target variables at each step.", "DAWN": "The success of machine learning has been demonstrated time and time again in classification, generative modelling, and reinforcement learning. In particular, we have recently seen interesting developments where ML has been applied to the natural sciences (chemistry, physics, materials science, neuroscience and biology). Here, often the data is not abundant and very costly. This workshop will focus on the unique challenges of applying machine learning to molecules and materials.\nAccurate prediction of chemical and physical properties is a crucial ingredient toward rational compound design in chemical and pharmaceutical industries. Many discoveries in chemistry can be guided by screening large databases of computational molecular structures and properties, but high level quantum-chemical calculations can take up to several days per molecule or material at the required accuracy, placing the ultimate achievement of in silico design out of reach for the foreseeable future. In large part the current state of the art for such problems is the expertise of individual researchers or at best highly-specific rule-based heuristic systems. Efficient methods in machine learning, applied to property and structure prediction, can therefore have pivotal impact in enabling chemical discovery and foster fundamental insights.", "Semi-Supervised Bayesian Optimisation for Automatic Chemical Design": "Project Objectives\nThe scope of the project involves the investigation of two sources of improvement to the model of [1]:", "Predictive Entropy Search for Bayesian Optimization with Unknown Constraints Supplementary Material": "PESC computes a Gaussian approximation to the NFCPD (main text, Eq.(11)) using Expectation Propagation (EP)(Minka, 2001). EP is a method for approximating a product of factors (often a single prior factor and multiple likelihood factors) with a tractable distribution, for example a Gaussian. EP generates a Gaussian approximation by approximating each individual factor with a Gaussian. The product all these Gaussians results in a single Gaussian distribution that approximates the product of all the exact factors. This is in contrast to the Laplace approximation which fits a single Gaussian distribution to the whole posterior. EP can be intuitively understood as fitting the individual Gaussian approximations by minimizing the Kullback-Leibler (KL) divergences between each exact factor and its corresponding Gaussian approximation. This would correspond to matching first and second moments between exact and approximate factors. However, EP does this moment matching in the context of all the other approximate factors, since we are ultimately interested in having a good approximation in regions where the overall posterior probability is high. Concretely, assume we wish to approximate the distribution q (x)=", "Gaussian Process Vine Copulas for Multivariate Dependence": "Gaussian Process Vine Copulas for Multivariate Dependence Page 1 Gaussian Process Vine Copulas for Multivariate Dependence Jos\u00e9 Miguel Hern\u00e1ndez-Lobato1,2 joint work with David L\u00f3pez-Paz2,3 and Zoubin Ghahramani1 1Department of Engineering, Cambridge University, Cambridge, UK 3Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany April 29, 2013 2Both authors are equal contributors. 1 Page 2 What is a Copula? Informal Definition A copula is a function that links univariate marginal distributions into a joint multivariate one. \u22123 \u22122 \u22121 0 1 2 3 0.0 0.1 0.2 0.3 0.4 x 0 2 4 6 8 10 0.0 0.1 0.2 0.3 y Marginal Densities Copula Joint Density The copula specifies the dependencies among the random variables. 2 Page 3 What is a Copula? Formal Definition A copula is a distribution function with marginals uniform in [0,1] . Let U1,...,Ud be rv uniformly distributed in [0,1] with copula C then C(u1,...,ud ) = \u2026", "An Introduction to Bayesian Machine Learning": "An Introduction to Bayesian Machine Learning Page 1 An Introduction to Bayesian Machine Learning An Introduction to Bayesian Machine Learning Jos\u00e9 Miguel Hern\u00e1ndez-Lobato Department of Engineering, Cambridge University April 8, 2013 1 Page 2 An Introduction to Bayesian Machine Learning What is Machine Learning? The design of computational systems that discover patterns in a collection of data instances in an automated manner. The ultimate goal is to use the discovered patterns to make predictions on new data instances not seen before. 1 ? 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 0 0 0 0 0 0 0 ? ? ? ? ? ? ? ? Instead of manually encoding patterns in computer programs, we make computers learn these patterns without explicitly programming them . Figure source [Hinton et al. 2006]. 2 Page 3 An Introduction to Bayesian Machine Learning Model-based Machine Learning We design a probabilistic model which \u2026", "An Introduction to Sum Product Networks": "GM are limited in some aspects: Many compact distributions cannot be represented as a GM, eg, uniform distribution over binary vectors with even number of 1\u2019s. The cost of exact inference in GM is exponential in the worst case. This means that we will often have to use approximate techniques. Because learning requires inference, learning GM will be difficult. Some distributions require GM with many layers of hidden variables to be compactly encoded. However, intractable inference makes learning these models extremely challenging. An alternative are sum product networks [Poon and Domingos, 2011]: New deep model with many layers of hidden variables. Exact inference is tractable (linear in the size of the model).", "Ensemble Methods in Machine Learning": "Ensemble Methods in Machine Learning Page 1 Ensemble Methods in Machine Learning Jos\u00e9 Miguel Hern\u00e1ndez-Lobato Department of Engineering, University of Cambridge, Trumpington Street, Cambridge, CB2 1PZ, UK, jmh233@cam.ac.uk December 19, 2012 JM Hern\u00e1ndez-Lobato (Cambridge) Ensemble Methods in Machine Learning December 19, 2012 1 / 34 Page 2 Motivation Machine learning is about making predictions from data. Prediction is often implemented using a single learning method which assumes a specific model that has to be adjusted to the data. Two main difficulties in this approach: What method should we use to make predictions? What values of the model parameters are the optimal ones? 1 Decision tree? 2 Neural network? 3 Gaussian process? 4 ... Ensemble methods can be used to address these difficulties! JM Hern\u00e1ndez-Lobato (Cambridge) Ensemble Methods in Machine \u2026", "Expectation Propagation for the Estimation of Conditional Bivariate Copulas": "Results\nEP-CC: The EP method for the estimation of conditional copulas. MLL: Non-parametric maximum local likelihood estimator. GC: Parametric Gaussian copula with constant \u03c4.", "Time Series Models for Measuring Market Risk Technical Report": "The task of measuring market risk requires to make use of a probabilistic model that captures the statistical properties of price variations in financial assets. The most important of these properties are autocorrelations, time-dependent volatility and extreme events. GARCH processes are financial models that can successfully account for the time-dependent volatility. However, they assume Gaussian errors whereas empirical studies generally lead to residuals which exhibit more extreme events than those implied by a Gaussian distribution. In this document we analyze the performance of different models which try to solve this deficiency of standard GARCH processes. The first group of models is based on mixtures of autoregressive experts which work together following three possible strategies: collaboration, soft competition or hard competition. Mixtures of soft competitive experts produce the best estimates of risk because their hypothesis space is a mixture of Gaussian distribution which can account for extreme events. Finally, we study a model which improves standard GARCH processes by means of modelling the innovations in a non-parametric way. The resulting model turns out to provide very precise measurements of market risk and outperforms soft competitive mixtures with 2 experts.", "3D Molecular Design with Covariant Neural Networks": "Automating molecular design using deep reinforcement learning (RL) has the potential to greatly accelerate the search for novel materials. Despite recent progress on leveraging graph representations to design molecules, such methods are fundamentally limited by the lack of three-dimensional (3D) information. In light of this, we propose a novel actor-critic architecture for 3D molecular design that can generate highly symmetric molecular structures. This is achieved by exploiting the symmetries of the design process through a rotationally covariant state-action representation based on a spherical harmonics series expansion. We demonstrate that building in such symmetries allows our agent to generate molecules that were unattainable with previous approaches.", "Reinforcement Learning for Molecular Design Guided by Quantum Mechanics Open Website": "Automating molecular design using deep reinforcement learning (RL) holds the promise of accelerating the discovery of new chemical compounds. Existing approaches work with molecular graphs and thus ignore the location of atoms in space, which restricts them to 1) generating single organic molecules and 2) heuristic reward functions. To address this, we present a novel RL formulation for molecular design in Cartesian coordinates, thereby extending the class of molecules that can be built. Our reward function is directly based on fundamental physical properties such as the energy, which we approximate via fast quantum-chemical methods. To enable progress towards de-novo molecular design, we introduce MolGym, an RL environment comprising several challenging molecular design tasks along with baselines. In our experiments, we show that our agent can efficiently learn to solve these tasks from \u2026", "Predictive Complexity Priors: Supplementary Materials": "This mixture should achieve the same interpolation behavior as the log-Cauchy, balancing preferences for p0 and p+. See Figure 1 (a) for a visualization of the ECP for the gamma-exponential mixture as the mixing coefficient is varied. Figure 1 (b) shows the log-Cauchy ECP as its scale is varied.", "Sample-Efficient Optimization in the Latent Space of Deep Generative Models via Weighted Retraining Download PDF": "Many important problems in science and engineering, such as drug design, involve optimizing an expensive black-box objective function over a complex, high-dimensional, and structured input space. Although machine learning techniques have shown promise in solving such problems, existing approaches substantially lack sample efficiency. We introduce an improved method for efficient black-box optimization, which performs the optimization in the low-dimensional, continuous latent manifold learned by a deep generative model. In contrast to previous approaches, we actively steer the generative model to maintain a latent manifold that is highly useful for efficiently optimizing the objective. We achieve this by periodically retraining the generative model on the data points queried along the optimization trajectory, as well as weighting those data points according to their objective function value. This weighted \u2026", "Appendix for: Barking up the right tree: an approach to search over molecule synthesis DAGs": "Section D The fourth section provides further experimental results, such as additional sanity checks for the generation task as well as further figures and tables for the optimization tasks. We also provide details of the best molecules found when optimizing for QED and penalized logP, given the popularity of these metrics in previous work.", "Successor Uncertainties: exploration and uncertainty in temporal difference learning Download PDF": "Posterior sampling for reinforcement learning (PSRL) is an effective method for balancing exploration and exploitation in reinforcement learning. Randomised value functions (RVF) can be viewed as a promising approach to scaling PSRL. However, we show that most contemporary algorithms combining RVF with neural network function approximation do not possess the properties which make PSRL effective, and provably fail in sparse reward problems. Moreover, we find that propagation of uncertainty, a property of PSRL previously thought important for exploration, does not preclude this failure. We use these insights to design Successor Uncertainties (SU), a cheap and easy to implement RVF algorithm that retains key properties of PSRL. SU is highly effective on hard tabular exploration benchmarks. Furthermore, on the Atari 2600 domain, it surpasses human performance on 38 of 49 games tested (achieving a \u2026", "VARIATIONAL DEPTH SEARCH IN RESNETS Download PDF": "One-shot neural architecture search allows joint learning of weights and network architecture, reducing computational cost. We limit our search space to the depth of residual networks and formulate an analytically tractable variational objective that allows for obtaining an unbiased approximate posterior over depths in one-shot. We propose a heuristic to prune our networks based on this distribution. We compare our proposed method against manual search over network depths on the MNIST, Fashion-MNIST, SVHN datasets. We find that pruned networks do not incur a loss in predictive performance, obtaining accuracies competitive with unpruned networks. Marginalising over depth allows us to obtain better-calibrated test-time uncertainty estimates than regular networks, in a single forward pass.", "Depth Uncertainty in Neural Networks Download PDF": "Existing methods for estimating uncertainty in deep learning tend to require multiple forward passes, making them unsuitable for applications where computational resources are limited. To solve this, we perform probabilistic reasoning over the depth of neural networks. Different depths correspond to subnetworks which share weights and whose predictions are combined via marginalisation, yielding model uncertainty. By exploiting the sequential structure of feed-forward networks, we are able to both evaluate our training objective and make predictions with a single forward pass. We validate our approach on real-world regression and image classification tasks. Our approach provides uncertainty calibration, robustness to dataset shift, and accuracies competitive with more computationally expensive baselines.", "Getting a CLUE: A Method for Explaining Uncertainty Estimates Download PDF": "Both uncertainty estimation and interpretability are important factors for trustwor-thy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the input\u2019s prediction. We validate CLUE through 1) a novel framework for evaluating counterfactual explanations of uncertainty, 2) a series of ablation experiments, and 3) a user study. Our experiments show that CLUE outperforms baselines and enables practitioners to better understand which input patterns are responsible for predictive uncertainty.", "Amortised Variational Inference for Hierarchical Mixture Models": "Hierarchical Mixtures of Experts (HME) are flexible and interpretable probabilistic models. However, existing approaches to learning treestructured decision rules are prone to poor local optima. This work introduces an end-to-end differentiable amortised variational inference algorithm for HMEs. We use an RNN (dubbed RNN-Tree) to approximate the posterior distribution over tree node routing decisions. We show that our RNN-Tree finds better decision rules than greedily learnt trees, resulting in better generalisation performance. We also show how RNN-Trees\u2019 differentiability facilitates their integration into existing machine learning pipelines.", "Investigating Inference in Bayesian Neural Networks via Active Learning": "Bayesian deep learning is an active area of study where approximate Bayesian inference is applied to neural architectures. In the past decade, an abundance of inferential methods have been proposed; none of which have been tested on rigorous nor tailored benchmarks of statistical modelling.", "Sum-Product Copulas": "Sum-Product Networks are a promising deep probabilistic model with tractable inference. Copulas are a powerful and flexible framework widely used in finance, from which the machine learning community can benefit. Most known parametric copulas are hard to extend apart from the bivariate case. We introduce a sum-product network that satisfies the copula constraints as a way to construct flexible and high-dimensional copulas, which we call Sum-Product Copulas. Sum-Product Copulas achieve competitive results on a range of general-purpose real-valued density estimation tasks.", "Bayesian Batch Active Learning as Sparse Subset Approximation": "Batch active learning (AL) allows to:\u25b6 Scale to large datasets and models\u25b6 Parallelize data acquisition How to construct the batch? We leverage Bayesian coresets [1, 2].", "Supplementary Materials: Dropout as a Structured Shrinkage Prior": "Huang et al.(2016) proposed stochastic depth resnets by applying dropout to the resnet block, ie al= \u03bblFl (hl\u2212 1)+ hl\u2212 1 where F (\u00b7) denotes a whole resnet block and \u03bbl is a Bernoulli random variable. For simplicity, if we assume the resnet block consists of just one non-linear transformation, then applying Bernoulli ADD in MN form to the resnet architecture yields a similar expression, as seen in Equation 1. The only difference is that the Bernoulli variable is within fl for ADD whereas it is outside the block for stochastic depth. However, if we assume fl is the ReLU function, then the activation is scale equivariant. If the noise\u2019s support is non-negative, then it is equivalent to move the noise variable outside the activation: ReLU (\u03c4lhl\u2212 1Wl)+ hl\u2212 1= \u03c4lReLU (hl\u2212 1Wl)+ hl\u2212 1 for \u03c4\u2265 0. Thus, we can derive the previously proposed stochastic depth regularization as a special case of our ADD framework.", "Automatic Chemical Design using Variational Autoencoders": "We train a variational autoencoder to convert discrete representations of molecules to and from a multidimensional continuous representation. This continuous representation allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the design of drug-like molecules as well as organic light-emitting diodes.", "Black-box \u03b1-divergence for Deep Generative Models": "We propose using the black-box \u03b1-divergence [1] as a flexible alternative to variational inference in deep generative models. By simply switching the objective function from the variational free-energy to the black-box \u03b1-divergence objective we are able to learn better generative models, which is demonstrated by a considerable improvement of the test log-likelihood in several preliminary experiments.", "Deep Gaussian Processes for Regression using Approximate Expectation Propagation: Supplementary material": "Given the approximate posterior and a new test input x\u2217, we wish to make a prediction about the test output y\u2217. That is to find p (y\u2217| x\u2217, X, Y)\u2248\u222b du p (y\u2217| x\u2217, u) q (u| X, Y). This predictive distribution is not analytically tractable, but fortunately, we can approximate it by a Gaussian in a similar fashion to the method described in the main text. That is, a single forward pass is performed, in which each layer takes in a Gaussian distribution over the input, incorporates the approximate posterior of the inducing outputs and approximates the output distribution by a Gaussian. An alternative to obtain the prediction is to forward sample from the model, but we do not use this approach in the experiments.", "Black-Box \u03b1-Divergence Minimization: Supplementary": "This section revisits the original EP algorithm as a min-max optimization problem. Recall in the main text that we approximate the true posterior distribution p (\u03b8| P) with a distribution in exponential family form given by q (\u03b8) o exp1s (\u03b8) T \u03bbql. Now we define a set of unnormalized cavity distributions q\\n (\u03b8)= exp1s (\u03b8) T \u03bb\\nl for every data point xn. Then according to [Minka, 2001], the EP energy function is", "Supplementary Material for: Predictive Entropy Search for Multi-objective Bayesian Optimization": "In this section we describe in detail the specific steps of the EP algorithm that is required for the evaluation of the proposed acquisition function, PESMO. More precisely, we show how to compute the EP approximation to the conditional predictive distribution of each objective fk. From the main manuscript we know that that this distribution is obtained by multiplying the GP posteriors by the product of all the approximate factors. We also show how to implement the EP updates to refine each approximate factor. In our implementation we assume independence among the K objective functions.", "Supplementary Material for: Scalable Gaussian Process Classification via Expectation Propagation": "In this document we give all the necessary details to implement the EP algorithm for the proposed method described in the main manuscript, ie SEP. In particular, we describe how to compute the EP posterior approximation from the product of all approximate factors and how to implement the EP updates to refine each approximate factor. We also give an intuitive idea about how to compute the EP approximation to the marginal likelihood and its gradients. Note that the updates described are very similar to the ones in [3].", "Supplementary Material\u2013Ambiguity Helps: Classification with Disagreements in Crowdsourced Annotations": "In this section we give all the necessary details to implement the EP algorithm [1] for the GPCconf method described in the main manuscript. We show how to compute the EP posterior approximation from the product of all approximate factors and how to implement the EP updates to refine each approximate factor. We also show how to compute the EP approximation of the marginal likelihood and its gradients.\nRecall from the main manuscript that in EP the approximate factors replace the corresponding exact factors of the likelihood in the joint distribution p (y| X, Xconf, f, g) p (f) p (g). The resulting approximate joint distribution is then normalized to get the EP posterior approximation, and the normalization constant is the approximation to the marginal likelihood. Furthermore, we would like to recall that in the case of GPCconf, the n-th likelihood factor to be approximated by EP is:", "Stochastic Expectation Propagation: Supplementary Material": "The supplementary material is divided into these sections. Section A details the design of stochastic power EP methods and presents relationships between SEP and SVI. Section B extends the discussion of distributed algorithms and SEP\u2019s applicability to latent variable models. Section C provides experimental details of the Bayesian neural network experiments and presents further emprical evalucations of the method.", "Supplementary Material for: A Probabilistic Model for Dirty Multi-task Feature Selection": "In this section we give all the necessary details to implement the EP algorithm for the proposed method in the main manuscript, ie DMTFS. We describe how to compute the EP posterior approximation from the product of all approximate factors and how to implement the EP updates to refine each approximate factor. Finally, although not used in the experiments, we also give an intuitive idea about how to compute the EP approximation to the marginal likelihood. Recall from the main manuscript that the approximate factors replace the corresponding factors in the joint distribution p (Y, W, \u2126, \u03c1, \u03c32| \uae30). The resulting approximate joint distribution is then normalized to get the EP posterior approximation, and the normalization constant is the approximation to the marginal likelihood.", "Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks Supplementary Material": "In this section we derive the gradient of the logarithm of the marginal likelihood, that is, log Z, with respect to the means and variances of the network weights in the Gaussian approximation q. In traditional backpropagation we have, for each neuron j, one variable \u03b4j containing the gradient of the network error with respect to the input or activation for neuron j. In PBP, the corresponding algorithm is very similar, with the difference that we now have two variables for each neuron j instead of only one. We have one variable \u03b4m j that contains the gradient of log Z with respect to the mean of the activation for neuron j. Additionally, there is another variable \u03b4v j that contains the gradient of log Z with respect to the variance of the activation for neuron j. The mean and variance of the output of unit j are defined as mz j and vz j, respectively. The mean and variance of the activation or input for unit j are defined as ma j and va j, respectively. We have that, becauseof the ReLU activation function, mz j= \u03a6 (\u03b1j)", "Supplementary Material to Cold-Start Active Learning with Robust Ordinal Matrix Factorization": "We describe the implementation of approximate inference in the proposed heteroskedastic ordinal matrix factorization (HOMF) model. Approximate Bayesian inference is performed using expectation propagation (EP) Minka (2001) and variational Bayes Ghahramani and Beal (2001). We use variational Bayes to approximate some operations within the execution of EP. First, we describe the hyper-parameter values used in the prior distributions of the HOMF model. After that, we describe in detail the implementation of expectation propagation in HOMF, how to make predictions using the EP approximation to the posterior distribution and the specific form the EP update operations.", "Supplementary Material to Probabilistic Matrix Factorization with Non-random Missing Data": "(8) and p (z)= N (z| z0, z0). We fix these priors to have zero-mean and unit variance. We also incorporate a local bias to each row and column. For example, the column h in F contains the biases for the columns of X. In this case, e0 i, h= 1 and e0 i, h= \u03b5, for i= 1,..., n, where \u03b5 is a small positive constant. Similarly, the column h\u2212 1 in E contains the biases for the rows and f0 j, h\u2212 1= 1 and f0 j, h\u2212 1= \u03b5, for j= 1,..., d.", "Supplementary Material for Stochastic Inference for Scalable Probabilistic Modeling of Binary Matrices": "The stochastic update rules for ui, d, vj, d and z are unew i, d= uold i, d+ \u03c1u i\u2207 L (ui, d)=(1\u2212 \u03c1u i) uold i, d+ \u03c1u i u\u22c6 i, d, vnew j, d= vold j, d+ \u03c1v j\u2207 L (vj, d)=(1\u2212 \u03c1v j) vold j, d+ \u03c1v jv\u22c6 j, d, znew= zold+ \u03c1z\u2207 L (z)=(1\u2212 \u03c1z) zold+ \u03c1zz\u22c6, where \u03c1u i, \u03c1v j and \u03c1z are the sizes of the steps taken in the direction of the natural gradient. Our model has exponential family complete conditionals because of the Gaussian lower bound that we use to approximate the logistic function. A complete conditional is the conditional distribution of a variable given all of the other variables and observations (Hoffman et al., 2013). When the complete conditionals are in the exponential family the natural gradient of Equation (9) in the main document with respect to ui, d is given by\u2207 L (ui, d)= u\u22c6 i, d\u2212 ui, d, where u\u22c6 i, d=(u\u22c6 i, d, \u00fc\u22c6 i, d) is the value of ui, d that maximizes Equation (9) when all the other natural parameters are kept fixed at their current values. Similarly, the natural gradient of the variational objective with respect to vj, d is given by\u2207 L (vj, d)= v\u22c6 j, d\u2212 vj, d, where v\u22c6 j, d=(v\u22c6 j, d, v\u22c6 j, d) and\u2207 L (z)= z\u22c6\u2212 z, where z\u22c6=(z\u22c6, z\u22c6). When we subsample the entry xi, j from X, the values u\u22c6 i, d, \u00fc\u22c6 i, d, v\u22c6 i, d, v\u22c6 i, d, z\u22c6, z\u22c6 that maximize", "Supplementary Material for: Learning Feature Selection Dependencies in Multi-task Learning": "The first step in the parallel EP algorithm is to compute the marginals of the current approximate posterior distribution. For this, the formulas for the product of Gaussian distributions have to be used. These formulas are found in the appendix of [1]. Denote by q the posterior approximation, and by \u00b5 and Vw the mean vector and covariance matrix of the Gaussian approximation for w, respectively. Similarly, denote by Vu and Vv the covariance matrices of the Gaussian approximation for u and v, respectively. Then, we have that q (z)= N (w| \u00b5, Vw) N (u| 0, Vu) N (v| 0, Vv) and from the definition of q (z)\u221d f (w) hu (u) hv (v)\u220f j gj we have that", "Section I: Computational Statistics": "Computational Statistics and Data Analysis 55 (2011) vi\u00e2\u20ac\u201cvii Volume 55, issue 6, 1 June 2011 CONTENTS Section I: Computational Statistics SJ Hatjispyros, T. Nicoleris and SG Walker Dependent mixtures of Dirichlet processes 2011 HB Crews, DD Boos and LA Stefanski FSR methods for second-order regression models 2026 JM Hern\u00c3\u00a1 ndez-Lobato and A. Su\u00c3\u00a1 rez Semiparametric bivariate Archimedean copulas 2038 Section II: Statistical Methodology for Data Analysis T. Matsuo, DW Nychka and D. Paul Nonstationary covariance modeling for incomplete data: Monte Carlo EM approach 2059 CQ da-Silva, HS Migon and LT Correia Dynamic Bayesian beta models 2074 P.-L. Li and J.-M. Chiou Identifying cluster number for subspace projected functional data clustering 2090 M. Tomita, N. Hashimoto and Y. Tanaka Association study for the relationship between a haplotype or haplotype set and multiple \u2026", "Collaborative Gaussian Processes for Preference Learning\u2013Supplementary Material": "Corr (g (xi, xj), g (xj, xi))= kpref ((xi, xj),(xj, xi))\n\u221a kpref ((xi, xj),(xi, xj))\u221a kpref ((xj, xi),(xj, xi))=\u2212 1,(3) where we have assumed \u00b5pref= 0 to simplify the derivations. This shows that the value of g at (xi, xj) is perfectly anti-correlated with the value of g at (xj, xi) under the prior. From this fact it can be shown that all elements g of the reproducing kernel Hilbert space (RKHS) corresponding to kpref have the property g (xi, xj)=\u2212 g (xj, xi). Finally, the preference kernel ensures transitivity between pairwise item preferences. In particular, since g (xi, xj)= f (xi)\u2212 f (xj), we have that if g (xi, xj)> 0 then f (xi)> f (xj) and if also g (xj, xk)> 0 then f (xj)> f (xk) and f (xi)> f (xk). Therefore, if g (xi, xj)> 0 and g (xj, xk)> 0 then g (xi, xk)> 0.", "Transfer Learning with Copulas": "Transfer learning methods use the knowledge gained by learning a particular source task to address more efficiently another different but related target task. They relax the frequent assumption in machine learning which states that both training and test data are drawn from the same distribution. This framework is most useful in those learning problems in which the amount of training data is reduced while, at the same time, some related tasks do have large amounts of available data. A recent survey of transfer learning techniques is described in [1].\nCopulas allow to construct multivariate models by separating the modeling of marginals from the modeling of dependence. Under this framework, any form of dependence is represented by a unique copula function [2]. When two regression tasks share the same dependence structure, we may be able to transfer knowledge between them by means of a common copula model, allowing the marginal distributions to vary freely. In this manner, we reduce the problem of learning a multivariate model for the target task into the problem of learning i) the marginal distributions of the data and ii) a copula model which is shared by the two tasks.", "Netbox: a probabilistic method for modeling and mining market basket data": null, "Time series models for measuring market risk": null, "Smart, automated, generation of molecular libraries for high-throughput virtual screening": null, "Reverse-engineering Transcription Control Networks": null, "Transcription Networks, Microarray Chips and Sparse Linear Methods": null, "Learning a Generative Model for Validity in Complex Discrete Structures": null, "First MSR-MLG Joint Meeting": null}, "Carl Rasmussen": {"Gaussian processes in machine learning": "We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.", "Gaussian Processes for Machine Learning": "We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.", "The infinite hidden Markov model": "We show that it is possible to extend hidden Markov models to have a countably infinite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the infinitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters define a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a finite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be infinite\u2014consider, for example, symbols being possible words appearing in English text.", "A unifying view of sparse approximate Gaussian process regression": "We provide a new unifying view, including all existing proper probabilistic sparse approximations for Gaussian process regression. Our approach relies on expressing the effective prior which the methods are using. This allows new insights to be gained, and highlights the relationship between existing methods. It also allows for a clear theoretically justified ranking of the closeness of the known approximations to the corresponding full GPs. Finally we point directly to designs of new better sparse approximations, combining the best of the existing strategies, within attractive computational constraints.", "The infinite Gaussian mixture model": "In a Bayesian mixture model it is not necessary a priori to limit the number of components to be finite. In this paper an infinite Gaussian mixture model is presented which neatly sidesteps the difficult problem of finding the \u201cright\u201d number of mixture components. Inference in the model is done using an efficient parameter-free Markov Chain that relies entirely on Gibbs sampling.", "Gaussian processes for regression": "The Bayesian analysis of neural networks is difficult because a simple prior over weights implies a complex prior distribution over functions. In this paper we investigate the use of Gaussian process priors over functions, which permit the predictive Bayesian analysis for fixed values of hyperparameters to be carried out exactly using matrix operations. Two methods, using optimization and averaging (via Hybrid Monte Carlo) over hyperparameters have been tested on a number of challenging problems and have produced excellent results.", "PILCO: A model-based and data-efficient approach to policy search": "In this paper, we introduce pilco, a practical, data-efficient model-based policy search method. Pilco reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, pilco can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-ofthe-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks.", "Gaussian processes for machine learning (GPML) toolbox": "The GPML toolbox provides a wide range of functionality for Gaussian process (GP) inference and prediction. GPs are specified by mean and covariance functions; we offer a library of simple mean and covariance functions and mechanisms to compose more complex ones. Several likelihood functions are supported including Gaussian and heavy-tailed for regression as well as others suitable for classification. Finally, a range of inference methods is provided, including exact and variational inference, Expectation Propagation, and Laplace\u2019s method dealing with non-Gaussian likelihoods and FITC for dealing with large regression tasks.", "Evaluation of Gaussian processes and other methods for non-linear regression": "This thesis develops two Bayesian learning methods relying on Gaussian processes and a rigorous statistical approach for evaluating such methods. In these experimental designs the sources of uncertainty in the estimated generalisation performances due to both variation in training and test sets are accounted for. The framework allows for estimation of generalisation performance as well as statistical tests of significance for pairwise comparisons. Two experimental designs are recommended and supported by the DELVE software environment.\nTwo new non-parametric Bayesian learning methods relying on Gaussian process priors over functions are developed. These priors are controlled by hyperparameters which set the characteristic length scale for each input dimension. In the simplest method, these parameters are fit from the data using optimization. In the second, fully Bayesian method, a Markov chain Monte Carlo technique is used to integrate over the hyperparameters. One advantage of these Gaussian process methods is that the priors and hyperparameters of the trained models are easy to interpret.", "Infinite mixtures of Gaussian process experts": "We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an infinite number of Experts. Inference in this model may be done efficiently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets-thus potentially overcoming two of the biggest hurdles with GP models. Simulations show the viability of this approach.", "Gaussian Processes for Data-Efficient Learning in Robotics and Control": "Autonomous learning has been a promising direction in control and robotics for more than a decade since data-driven learning allows to reduce the amount of engineering knowledge, which is otherwise required. However, autonomous reinforcement learning (RL) approaches typically require many interactions with the system to learn controllers, which is a practical limitation in real systems, such as robots, where many interactions can be impractical and time consuming. To address this problem, current learning approaches typically require task-specific knowledge in form of expert demonstrations, realistic simulators, pre-shaped policies, or specific knowledge about the underlying dynamics. In this paper, we follow a different approach and speed up learning by extracting more information from data. In particular, we learn a probabilistic, non-parametric Gaussian process transition model of the system. By explicitly \u2026", "Gaussian Process priors with uncertain inputs - Application to multiple-step ahead time series forecasting": "We consider the problem of multi-step ahead prediction in time series analysis using the non-parametric Gaussian process model. k-step ahead forecasting of a discrete-time non-linear dynamic system can be performed by doing repeated one-step ahead predictions. For a state-space model of the form y t = f(Yt-1 ,..., Yt-L ), the prediction of y at time t + k is based on the point estimates of the previous outputs. In this paper, we show how, using an analytical Gaussian approximation, we can formally incorporate the uncertainty about intermediate regressor values, thus updating the uncertainty on the current prediction.", "Sparse spectrum Gaussian process regression": "We present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to sparsify the spectral representation of the GP. This leads to a simple, practical algorithm for regression tasks. We compare the achievable trade-offs between predictive accuracy and computational requirements, and show that these are typically superior to existing state-of-the-art sparse approximations. We discuss both the weight space and function space representations, and note that the new construction implies priors over functions which are always stationary, and can approximate any covariance function in this class.", "Approximations for binary Gaussian process classification": "We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classification. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches.", "Assessing approximate inference for binary Gaussian process classification": "Gaussian process priors can be used to define flexible, probabilistic classification models. Unfortunately exact Bayesian inference is analytically intractable and various approximation techniques have been proposed. In this work we review and compare Laplace\u2019s method and Expectation Propagation for approximate Bayesian inference in the binary Gaussian process classification model. We present a comprehensive comparison of the approximations, their predictive performance and marginal likelihood estimates to results obtained by MCMC sampling. We explain theoretically and corroborate empirically the advantages of Expectation Propagation compared to Laplace\u2019s method.", "Warped Gaussian processes": "We generalise the Gaussian process (GP) framework for regression by learning a nonlinear transformation of the GP outputs. This allows for non-Gaussian processes and non-Gaussian noise. The learning algo-rithm chooses a nonlinear transformation such that transformed data is well-modelled by a GP. This can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem, rather than as an ad-hoc step. We demonstrate on several real regression problems that learning the transformation can lead to significantly better performance than using a regular GP, or a GP with a fixed transformation.", "Derivative observations in Gaussian process models of dynamic systems": "Gaussian processes provide an approach to nonparametric modelling which allows a straightforward combination of function and derivative observations in an empirical model. This is of particular importance in identification of nonlinear dynamic systems from experimental data. 1)It allows us to combine derivative information, and associated uncertainty with normal function observations into the learning and inference process. This derivative information can be in the form of priors specified by an expert or identified from perturbation data close to equilibrium. 2) It allows a seamless fusion of multiple local linear models in a consistent manner, inferring consistent models and ensuring that integrability constraints are met. 3) It improves dramatically the computational efficiency of Gaussian process models for dynamic system identification, by summarising large quantities of near-equilibrium data by a handful of linearisations, reducing the training size - traditionally a problem for Gaussian process models.", "Bayesian monte Carlo": "We investigate Bayesian alternatives to classical Monte Carlo methods for evaluating integrals. Bayesian Monte Carlo (BMC) allows the incorporation of prior knowledge, such as smoothness of the integrand, into the estimation. In a simple problem we show that this outperforms any classical importance sampling method. We also attempt more challenging multidimensional integrals involved in computing marginal likelihoods of statistical models (aka partition functions and model evidences). We find that Bayesian Monte Carlo outperformed Annealed Importance Sampling, although for very high dimensional problems or problems with massive multimodality BMC may be less adequate. One advantage of the Bayesian approach to Monte Carlo is that samples can be drawn from any distribution. This allows for the possibility of active design of sample points so as to maximise information gain.", "Occam's razor": "Razor; at other times very large models perform well. We give simple examples of both kinds of behaviour. The two views are reconciled when measuring complexity of functions, rather than of the machinery used to implement them. We analyze the complexity of functions for some linear in the parameter models that are equivalent to Gaussian Processes, and always find Occam's Razor at work.", "Additive Gaussian Processes": "We introduce a Gaussian process model of functions which are additive. An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels. Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive but tractable parameterization of the kernel function, which allows efficient evaluation of all input interaction terms, whose number is exponential in the input dimension. The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks.", "Gaussian process dynamic programming": "Reinforcement learning (RL) and optimal control of systems with continuous states and actions require approximation techniques in most interesting cases. In this article, we introduce Gaussian process dynamic programming (GPDP), an approximate value function-based RL algorithm. We consider both a classic optimal control problem, where problem-specific prior knowledge is available, and a classic RL problem, where only very general priors can be used. For the classic optimal control problem, GPDP models the unknown value functions with Gaussian processes and generalizes dynamic programming to continuous-valued states and actions. For the RL problem, GPDP starts from a given initial state and explores the state space using Bayesian active learning. To design a fast learner, available data have to be used efficiently. Hence, we propose to learn probabilistic models of the a priori unknown transition \u2026", "Learning to Control a Low-Cost Manipulator using Data-Efficient Reinforcement Learning": "Over the last years, there has been substantial progress in robust manipulation in unstructured environments. The long-term goal of our work is to get away from precise, but very expensive robotic systems and to develop affordable, potentially imprecise, self-adaptive manipulator systems that can interactively perform tasks such as playing with children. In this paper, we demonstrate how a low-cost off-the-shelf robotic system can learn closed-loop policies for a stacking task in only a handful of trials\u2014from scratch. Our manipulator is inaccurate and provides no pose feedback. For learning a controller in the work space of a Kinect-style depth camera, we use a model-based reinforcement learning technique. Our learning method is data efficient, reduces model bias, and deals with several noise sources in a principled way during long-term planning. We present a way of incorporating state-space constraints into the learning process and analyze the learning gain by exploiting the sequential structure of the stacking task.", "The need for open source software in machine learning": "Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the field of machine learning has developed a large body of powerful learning algorithms for diverse applications. However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be significantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientific community.", "Gaussian process model based predictive control": "Gaussian process models provide a probabilistic non-parametric modelling approach for black-box identification of non-linear dynamic systems. The Gaussian processes can highlight areas of the input space where prediction quality is poor, due to the lack of data or its complexity, by indicating the higher variance around the predicted mean. Gaussian process models contain noticeably less coefficients to be optimized. This paper illustrates possible application of Gaussian process models within model-based predictive control. The extra information provided within Gaussian process model is used in predictive control, where optimization of control signal takes the variance information into account. The predictive control principle is demonstrated on control of pH process benchmark.", "Dirichlet Process Gaussian mixture models: choice of the base distribution": "In the Bayesian mixture modeling framework it is possible to infer the necessary number of components to model the data and therefore it is unnecessary to explicitly restrict the number of components. Nonparametric mixture models sidestep the problem of finding the \u201ccorrect\u201d number of mixture components by assuming infinitely many components. In this paper Dirichlet process mixture (DPM) models are cast as infinite mixture models and inference using Markov chain Monte Carlo is described. The specification of the priors on the model parameters is often guided by mathematical and practical convenience. The primary goal of this paper is to compare the choice of conjugate and non-conjugate base distributions on a particular class of DPM models which is widely used in applications, the Dirichlet process Gaussian mixture model (DPGMM). We compare computational efficiency and modeling \u2026", "Gaussian Process Training with Input Noise": "In standard Gaussian Process regression input locations are assumed to be noise free. We present a simple yet effective GP model for training on input points corrupted by iid Gaussian noise. To make computations tractable we use a local linear expansion about each input point. This allows the input noise to be recast as output noise proportional to the squared gradient of the GP posterior mean. The input noise variances are inferred from the data as extra hyperparameters. They are trained alongside other hyperparameters by the usual method of maximisation of the marginal likelihood. Training uses an iterative scheme, which alternates between optimising the hyperparameters and calculating the posterior gradient. Analytic predictive moments can then be found for Gaussian distributed test points. We compare our model to others over a range of different regression problems and show that it improves over current methods.", "Understanding Probabilistic Sparse Gaussian Process Approximations": "Good sparse approximations are essential for practical inference in Gaussian Processes as the computational cost of exact methods is prohibitive for large datasets. The Fully Independent Training Conditional (FITC) and the Variational Free Energy (VFE) approximations are two recent popular methods. Despite superficial similarities, these approximations have surprisingly different theoretical properties and behave differently in practice. We thoroughly investigate the two methods for regression both analytically and through illustrative examples, and draw conclusions to guide practical application.", "Manifold Gaussian Processes for Regression": "Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness assumptions on the structure of the function to be modeled. To model complex and non-differentiable functions, these smoothness assumptions are often too restrictive. One way to alleviate this limitation is to find a different representation of the data by introducing a feature space. This feature space is often learned in an unsupervised way, which might lead to data representations that are not useful for the overall regression task. In this paper, we propose Manifold Gaussian Processes, a novel supervised method that jointly learns a transformation of the data into a feature space and a GP regression from the feature space to observed space. The Manifold GP is a full GP and allows to learn data representations, which are useful for the overall regression task. As a proof-of-concept, we evaluate our approach on complex non-smooth \u2026", "Gaussian processes to speed up Hybrid Monte Carlo for expensive Bayesian integrals": "Hybrid Monte Carlo (HMC) is often the method of choice for computing Bayesian integrals that are not analytically tractable. However the success of this method may require a very large number of evaluations of the (un-normalized) posterior and its partial derivatives. In situations where the posterior is computationally costly to evaluate, this may lead to an unacceptable computational load for HMC. I propose to use a Gaussian Process model of the (log of the) posterior for most of the computations required by HMC. Within this scheme only occasional evaluation of the actual posterior is required to guarantee that the samples generated have exactly the desired distribution, even if the GP model is somewhat inaccurate. The method is demonstrated on a 10 dimensional problem, where 200 evaluations suffice for the generation of 100 roughly independent points from the posterior. Thus, the proposed scheme allows Bayesian treatment of models with posteriors that are computationally demanding, such as models involving computer simulation.", "Approximation methods for Gaussian process regression": "A wealth of computationally efficient approximation methods for Gaussian process regression have been recently proposed. We give a unifying overview of sparse approximations, following Qui\u00f1onero-Candela and Rasmussen (2005), and a brief review of approximate matrix-vector multiplication methods.", "Propagation of uncertainty in Bayesian kernel models-application to multiple-step ahead forecasting": "The object of Bayesian modelling is predictive distribution, which, in a forecasting scenario, enables evaluation of forecasted values and their uncertainties. We focus on reliably estimating the predictive mean and variance of forecasted values using Bayesian kernel based models such as the Gaussian process and the relevance vector machine. We derive novel analytic expressions for the predictive mean and variance for Gaussian kernel shapes under the assumption of a Gaussian input distribution in the static case, and of a recursive Gaussian predictive density in iterative forecasting. The capability of the method is demonstrated for forecasting of time-series and compared to approximate methods.", "Distributed variational inference in sparse Gaussian process regression and latent variable models": "Gaussian processes (GPs) are a powerful tool for probabilistic inference over functions. They have been applied to both regression and non-linear dimensionality reduction, and offer desirable properties such as uncertainty estimates, robustness to over-fitting, and principled ways for tuning hyper-parameters. However the scalability of these models to big datasets remains an active topic of research. We introduce a novel re-parametrisation of variational inference for sparse GP regression and latent variable models that allows for an efficient distributed algorithm. This is done by exploiting the decoupling of the data given the inducing points to re-formulate the evidence lower bound in a Map-Reduce setting. We show that the inference scales well with data and computational resources, while preserving a balanced distribution of the load among the nodes. We further demonstrate the utility in scaling Gaussian processes to big data. We show that GP performance improves with increasing amounts of data in regression (on flight data with 2 million records) and latent variable modelling (on MNIST). The results show that GPs perform better than many common models often used for big data.", "Gaussian Process change point models": "We combine Bayesian online change point detection with Gaussian processes to create a nonparametric time series model which can handle change points. The model can be used to locate change points in an online manner; and, unlike other Bayesian online change point detection algorithms, is applicable when temporal correlations in a regime are expected. We show three variations on how to apply Gaussian processes in the change point context, each with their own advantages. We present methods to reduce the computational burden of these models and demonstrate it on several real world data sets.", "Deep convolutional networks as shallow Gaussian processes": "We show that the output of a (residual) convolutional neural network (CNN) with an appropriate prior over the weights and biases is a Gaussian process (GP) in the limit of infinitely many convolutional filters, extending similar results for dense networks. For a CNN, the equivalent kernel can be computed exactly and, unlike \"deep kernels\", has very few parameters: only the hyperparameters of the original CNN. Further, we show that this kernel has two properties that allow it to be computed efficiently; the cost of evaluating the kernel for a pair of images is similar to a single forward pass through the original CNN with only one filter per layer. The kernel equivalent to a 32-layer ResNet obtains 0.84% classification error on MNIST, a new record for GPs with a comparable number of parameters.", "Variational Gaussian process state-space models": "State-space models have been successfully used for more than fifty years in different areas of science and engineering. We present a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes. The result of learning is a tractable posterior over nonlinear dynamical systems. In comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting. Our main algorithm uses a hybrid inference approach combining variational Bayes and sequential Monte Carlo. We also present stochastic variational inference and online learning approaches for fast learning with long time series.", "Robust filtering and smoothing with Gaussian processes": "We propose a principled algorithm for robust Bayesian filtering and smoothing in nonlinear stochastic dynamic systems when both the transition function and the measurement function are described by non-parametric Gaussian process (GP) models. GPs are gaining increasing importance in signal processing, machine learning, robotics, and control for representing unknown system functions by posterior probability distributions. This modern way of system identification is more robust than finding point estimates of a parametric function representation. Our principled filtering/smoothing approach for GP dynamic systems is based on analytic moment matching in the context of the forward-backward algorithm. Our numerical evaluations demonstrate the robustness of the proposed approach in situations where other state-of-the-art Gaussian filters and smoothers can fail.", "Bayesian inference and learning in Gaussian process state-space models with particle MCMC": "State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. We present a fully Bayesian approach to inference \\emph{and learning} (i.e. state estimation and system identification) in nonlinear nonparametric state-space models. We place a Gaussian process prior over the state transition dynamics, resulting in a flexible model able to capture complex dynamical phenomena. To enable efficient inference, we marginalize over the transition dynamics function and infer directly the joint smoothing distribution using specially tailored Particle Markov Chain Monte Carlo samplers. Once a sample from the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. Our approach preserves the full nonparametric expressivity of the model and can make use of sparse Gaussian processes to greatly reduce computational complexity.", "Active Learning of Model Evidence Using Bayesian Quadrature": "Numerical integration is a key component of many problems in scientific computing, statistical modelling, and machine learning. Bayesian Quadrature is a modelbased method for numerical integration which, relative to standard Monte Carlo methods, offers increased sample efficiency and a more robust estimate of the uncertainty in the estimated integral. We propose a novel Bayesian Quadrature approach for numerical integration when the integrand is non-negative, such as the case of computing the marginal likelihood, predictive distribution, or normalising constant of a probabilistic model. Our approach approximately marginalises the quadrature model\u2019s hyperparameters in closed form, and introduces an active learning scheme to optimally select function evaluations, as opposed to using Monte Carlo samples. We demonstrate our method on both a number of synthetic benchmarks and a real scientific problem from astronomy.", "Convolutional Gaussian Processes": "We present a practical way of introducing convolutional structure into Gaussian processes, making them more suited to high-dimensional inputs like images. The main contribution of our work is the construction of an inter-domain inducing point approximation that is well-tailored to the convolutional kernel. This allows us to gain the generalisation benefit of a convolutional kernel, together with fast but accurate posterior inference. We investigate several variations of the convolutional kernel, and apply it to MNIST and CIFAR-10, which have both been known to be challenging for Gaussian processes. We also show how the marginal likelihood can be used to find an optimal weighting between convolutional and RBF kernels to further improve performance. We hope that this illustration of the usefulness of a marginal likelihood will help automate discovering architectures in larger models.", "Healing the relevance vector machine through augmentation": "The Relevance Vector Machine (RVM) is a sparse approximate Bayesian kernel method. It provides full predictive distributions for test cases. However, the predictive uncertainties have the unintuitive property, that they get smaller the further you move away from the training cases. We give a thorough analysis. Inspired by the analogy to non-degenerate Gaussian Processes, we suggest augmentation to solve the problem. The purpose of the resulting model, RVM*, is primarily to corroborate the theoretical and experimental analysis. Although RVM* could be used in practical applications, it is no longer a truly sparse model. Experiments show that sparsity comes at the expense of worse predictive. distributions.", "State-space inference and learning with Gaussian processes": "State-space inference and learning with Gaussian processes (GPs) is an unsolved problem. We propose a new, general methodology for inference and learning in nonlinear state-space models that are described probabilistically by non-parametric GP models. We apply the expectation maximization algorithm to iterate between inference in the latent state-space and learning the parameters of the underlying GP dynamics model.", "Perfusion quantification using Gaussian process deconvolution": "The quantification of perfusion using dynamic susceptibility contrast MRI (DSC\u2010MRI) requires deconvolution to obtain the residual impulse response function (IRF). In this work, a method using the Gaussian process for deconvolution (GPD) is proposed. The fact that the IRF is smooth is incorporated as a constraint in the method. The GPD method, which automatically estimates the noise level in each voxel, has the advantage that model parameters are optimized automatically. The GPD is compared to singular value decomposition (SVD) using a common threshold for the singular values, and to SVD using a threshold optimized according to the noise level in each voxel. The comparison is carried out using artificial data as well as data from healthy volunteers. It is shown that GPD is comparable to SVD with a variable optimized threshold when determining the maximum of the IRF, which is directly related to the \u2026", "The DELVE manual": "DELVE| Data for Evaluating Learning in Valid Experiments| is a collection of datasets from many sources, an environment within which this data can be used to assess the performance of methods for learning relationships from data, and a repository for the results of such assessments.\nMany methods for learning relationships from empirical data have been developed by researchers in statistics, pattern recognition, arti cial intelligence, neural networks, and other elds. Methods in common use include simple linear models, nearest neighbor methods, decision trees, multilayer perceptron networks, and many others of varying degrees of complexity. Properly comparing the performance of these learning methods in realistic contexts is a surprisingly di cult task, requiring both an extensive collection of real-world data, and a carefully-designed scheme for performing experiments.", "Evaluating predictive uncertainty challenge": "This Chapter presents the PASCAL Evaluating Predictive Uncertainty Challenge, introduces the contributed Chapters by the participants who obtained outstanding results, and provides a discussion with some lessons to be learnt. The Challenge was set up to evaluate the ability of Machine Learning algorithms to provide good \u201cprobabilistic predictions\u201d, rather than just the usual \u201cpoint predictions\u201d with no measure of uncertainty, in regression and classification problems. Parti-cipants had to compete on a number of regression and classification tasks, and were evaluated by both traditional losses that only take into account point predictions and losses we proposed that evaluate the quality of the probabilistic predictions.", "Model based learning of sigma points in unscented Kalman filtering": "The unscented Kalman filter (UKF) is a widely used method in control and time series applications. The UKF suffers from arbitrary parameters necessary for sigma point placement, potentially causing it to perform poorly in nonlinear problems. We show how to treat sigma point placement in a UKF as a learning problem in a model based view. We demonstrate that learning to place the sigma points correctly from data can make sigma point collapse much less likely. Learning can result in a significant increase in predictive performance over default settings of the parameters in the UKF and other filters designed to avoid the problems of the UKF, such as the GP-ADF. At the same time, we maintain a lower computational complexity than the other methods. We call our method UKF-L.", "Predictive control with Gaussian process models": "This paper describes model-based predictive control based on Gaussian processes. Gaussian process models provide a probabilistic non-parametric modelling approach for black-box identification of nonlinear dynamic systems. It offers more insight in variance of obtained model response, as well as fewer parameters to determine than other models. The Gaussian processes can highlight areas of the input space where prediction quality is poor, due to the lack of data or its complexity, by indicating the higher variance around the predicted mean. This property is used in predictive control, where optimisation of control signal takes the variance information into account. The predictive control principle is demonstrated on a simulated example of nonlinear system.", "Adaptive, cautious, predictive control with Gaussian process priors": "Nonparametric Gaussian Process models, a Bayesian statistics approach, are used to implement a nonlinear adaptive control law. Predictions, including propagation of the state uncertainty are made over a K-step horizon. The expected value of a quadratic cost function is minimised, over this prediction horizon, without ignoring the variance of the model predictions. The general method and its main features are illustrated on a simulation example.", "A choice model with infinitely many latent features": "Elimination by aspects (EBA) is a probabilistic choice model describing how humans decide between several options. The options from which the choice is made are characterized by binary features and associated weights. For instance, when choosing which mobile phone to buy the features to consider may be: long lasting battery, color screen, etc. Existing methods for inferring the parameters of the model assume pre-specified features. However, the features that lead to the observed choices are not always known. Here, we present a non-parametric Bayesian model to infer the features of the options and the corresponding weights from choice data. We use the Indian buffet process (IBP) as a prior over the features. Inference using Markov chain Monte Carlo (MCMC) in conjugate IBP models has been previously described. The main contribution of this paper is an MCMC algorithm for the EBA model that can also \u2026", "Pruning from adaptive regularization": "Inspired by the recent upsurge of interest in Bayesian methods we consider adaptive regularization. A generalization based scheme for adaptation of regularization parameters is introduced and compared to Bayesian regularization. We show that pruning arises naturally within both adaptive regularization schemes. As model example we have chosen the simplest possible: estimating the mean of a random variable with known variance. Marked similarities are found between the two methods in that they both involve a \u201cnoise limit,\u201d below which they regularize with infinite weight decay, i.e., they prune . However, pruning is not always beneficial. We show explicitly that both methods in some cases may increase the generalization error. This corresponds to situations where the underlying assumptions of the regularizer are poorly matched to the environment.", "Learning Depth From Stereo": "We compare two approaches to the problem of estimating the depth of a point in space from observing its image position in two different cameras: 1. The classical photogrammetric approach explicitly models the two cameras and estimates their intrinsic and extrinsic parameters using a tedious calibration procedure; 2. A generic machine learning approach where the mapping from image to spatial coordinates is directly approximated by a Gaussian Process regression. Our results show that the generic learning approach, in addition to simplifying the procedure of calibration, can lead to higher depth accuracies than classical calibration although no specific domain knowledge is used.", "Approximate dynamic programming with Gaussian processes": "In general, it is difficult to determine an optimal closed-loop policy in nonlinear control problems with continuous-valued state and control domains. Hence, approximations are often inevitable. The standard method of discretizing states and controls suffers from the curse of dimensionality and strongly depends on the chosen temporal sampling rate. In this paper, we introduce Gaussian process dynamic programming (GPDP) and determine an approximate globally optimal closed-loop policy. In GPDP, value functions in the Bellman recursion of the dynamic programming algorithm are modeled using Gaussian processes. GPDP returns an optimal state- feedback for a finite set of states. Based on these outcomes, we learn a possibly discontinuous closed-loop policy on the entire state space by switching between two independently trained Gaussian processes. A binary classifier selects one Gaussian process to \u2026", "Prediction at an Uncertain Input for Gaussian Processes and Relevance Vector Machines Application to Multiple-Step Ahead Time-Series Forecasting": "This report non-linear models that map an input D-dimensional column vector x into a single dimensional output f (x). The non-linear mapping f () is implemented by means of a Gaussian process (GP) or a Relevance Vector Machine (RVM), see for example [Rasmussen, 1996] and [Tipping, 2001]. We are given a training data set D= fx i; yig N i= 1 where the target yi relates to the input xi through yi= f (xi)+(1) where N (0;) is additive iid Gaussian noise of variance.", "Observations on the Nystr\u00f6m method for Gaussian process prediction": "A number of methods for speeding up Gaussian Process (GP) prediction have been proposed, including the Nystr\u00f6m method of Williams and Seeger (2001). In this paper we focus on two issues (1) the relationship of the Nystr\u00f6m method to the Subset of Regressors method (Poggio and Girosi 1990; Luo and Wahba, 1997) and (2) understanding in what circumstances the Nystr\u00f6m approximation would be expected to provide a good approximation to exact GP regression.", "Adaptive sequential Bayesian change point detection": "Results\nOur method performs well on three real world data sets. The well log data is univariate while the others are multivariate. In all of the examples we use the three parameter logistic hazard, H (t)= h\u03c3 (at+ b), to have a more flexible inter-arrival distribution than the geometric distribution implied by the constant hazard function.\nWell Log Data We applied our method to the well log data set for detecting changes in the rocks stratification, described in [4]. We used the logistic hazard and used a normal for the data in each regime, with changing mean and variance. After learning the parameters our method has a better predictive likelihood than [4]. We summarize our results in Table (1).", "Modeling and visualizing uncertainty in gene expression clusters using Dirichlet process mixtures": "Although the use of clustering methods has rapidly become one of the standard computational approaches in the literature of microarray gene expression data, little attention has been paid to uncertainty in the results obtained. Dirichlet process mixture (DPM) models provide a nonparametric Bayesian alternative to the bootstrap approach to modeling uncertainty in gene expression clustering. Most previously published applications of Bayesian model-based clustering methods have been to short time series data. In this paper, we present a case study of the application of nonparametric Bayesian clustering methods to the clustering of high-dimensional nontime series gene expression data using full Gaussian covariances. We use the probability that two genes belong to the same cluster in a DPM model as a measure of the similarity of these gene expression profiles. Conversely, this probability can be used to define \u2026", "Presynaptic and postsynaptic competition in models for the development of neuromuscular connections": "In the establishment of connections between nerve and muscle there is an initial stage when each muscle fibre is innervated by several different motor axons. Withdrawal of connections then takes place until each fibre has contact from just a single axon. The evidence suggests that the withdrawal process involves competition between nerve terminals. We examine in formal models several types of competitive mechanism that have been proposed for this phenomenon. We show that a model which combines competition for a presynaptic resource with competition for a postsynaptic resource is superior to others. This model accounts for many anatomical and physiological findings and has a biologically plausible implementation. Intrinsic withdrawal appears to be a side effect of the competitive mechanism rather than a separate non-competitive feature. The model's capabilities are confirmed by theoretical \u2026", "Bayesian modelling of fMRI time series": "We present a Hidden Markov Model (HMM) for inferring the hidden psychological state (or neural activity) during single trial fMRI activation experiments with blocked task paradigms. Inference is based on Bayesian methodology, using a combination of analytical and a variety of Markov Chain Monte Carlo (MCMC) sampling techniques. The advantage of this method is that detection of short time learning effects between repeated trials is possible since inference is based only on single trial experiments.", "Prediction on spike data using kernel algorithms": "We report and compare the performance of different learning algorithms based on data from cortical recordings. The task is to predict the orientation of visual stimuli from the activity of a population of simultaneously recorded neurons. We compare several ways of improving the coding of the input (ie, the spike data) as well as of the output (ie, the orientation), and report the results obtained using different kernel algorithms.", "PIPPS: Flexible model-based policy search robust to the curse of chaos": "Previously, the exploding gradient problem has been explained to be central in deep learning and model-based reinforcement learning, because it causes numerical issues and instability in optimization. Our experiments in model-based reinforcement learning imply that the problem is not just a numerical issue, but it may be caused by a fundamental chaos-like nature of long chains of nonlinear computations. Not only do the magnitudes of the gradients become large, the direction of the gradients becomes essentially random. We show that reparameterization gradients suffer from the problem, while likelihood ratio gradients are robust. Using our insights, we develop a model-based policy search framework, Probabilistic Inference for Particle-Based Policy Search (PIPPS), which is easily extensible, and allows for almost arbitrary models and policies, while simultaneously matching the performance of previous data-efficient learning algorithms. Finally, we invent the total propagation algorithm, which efficiently computes a union over all pathwise derivative depths during a single backwards pass, automatically giving greater weight to estimators with lower variance, sometimes improving over reparameterization gradients by times.", "A practical Monte Carlo implementation of Bayesian learning": "A practical method for Bayesian training of feed-forward neural networks using sophisticated Monte Carlo methods is presented and evaluated. In reasonably small amounts of computer time this approach outperforms other state-of-the-art methods on 5 datalimited tasks from real world domains.", "Analysis of some methods for reduced rank Gaussian process regression": "While there is strong motivation for using Gaussian Processes (GPs) due to their excellent performance in regression and classification problems, their computational complexity makes them impractical when the size of the training set exceeds a few thousand cases. This has motivated the recent proliferation of a number of cost-effective approximations to GPs, both for classification and for regression. In this paper we analyze one popular approximation to GPs for regression: the reduced rank approximation. While generally GPs are equivalent to infinite linear models, we show that Reduced Rank Gaussian Processes (RRGPs) are equivalent to finite sparse linear models. We also introduce the concept of degenerate GPs and show that they correspond to inappropriate priors. We show how to modify the RRGP to prevent it from being degenerate at test time. Training RRGPs consists both in learning the \u2026", "Clustering protein sequence and structure space with infinite Gaussian mixture models": "We describe a novel approach to the problem of automatically clustering protein sequences and discovering protein families, subfamilies etc., based on the theory of infinite Gaussian mixtures models. This method allows the data itself to dictate how many mixture components are required to model it, and provides a measure of the probability that two proteins belong to the same cluster. We illustrate our methods with application to three data sets: globin sequences, globin sequences with known three-dimensional structures and G-protein coupled receptor sequences. The consistency of the clusters indicate that our method is producing biologically meaningful results, which provide a very good indication of the underlying families and subfamilies. With the inclusion of secondary structure and residue solvent accessibility information, we obtain a classification of sequences of known structure which both reflects and \u2026", "A case based comparison of identification with neural network and Gaussian process models": "In this paper an alternative approach to black-box identification of non-linear dynamic systems is compared with the more established approach of using artificial neural networks. The Gaussian process prior approach is a representative of non-parametric modelling approaches. It was compared on a pH process modelling case study. The purpose of modelling was to use the model for control design. The comparison revealed that even though Gaussian process models can be effectively used for modelling dynamic systems caution has to be axercised when signals are selected.", "Nonstationary Gaussian process regression using a latent extension of the input space": "Robert Bosch GmbH, Corporate Research and Advance Engineering, Stuttgart Max Planck Institute for Biological Cybernetics, T\u00fcbingen {Tobias. Pfingsten, Malte. Kuss, Carl}@ tuebingen. mpg. de Introduction Gaussian Processes (GPs) can be used to specify a prior over latent functions in non-parametric Bayesian models, eg for regression and classification. For this abstract we assume familiarity with the basic concepts of Gaussian Process models, see for example the introduction by Mackay [1]. A GP is defined by a mean and a covariance function, the latter describing dependencies k (x, x)= cov (f (x), f (x)) between function values as a function of the corresponding inputs x and x. A common assumption when specifying a GP prior is stationarity, ie that the covariance between function values only depends on the distances| x\u2212 x|, not on their location. It is far more difficult to specify a GP prior allowing the function to have different properties in different parts of the input space. In this work we describe new techniques for non-parametric Bayesian regression for, eg discontinuous, functions where the stationarity assumption does not hold. Several approaches to the problem of how to specify nonstationary GP models can be found in the literature. Sampson and Guttorp [2] propose to use multidimensional scaling for spatio-temporal Processes to map a nonstationary spatial Process into a latent space in which the problem becomes approximately stationary. Schmidt and O\u2019Hagan [3] pick up the idea and use GPs to implement the mapping. In comparison to a direct definition of a nonstationary covariance function, as proposed by [4], the detour via a \u2026", "Modelling and control of nonlinear systems using Gaussian processes with partial model information": "Gaussian processes are gaining increasing popularity among the control community, in particular for the modelling of discrete time state space systems. However, it has not been clear how to incorporate model information, in the form of known state relationships, when using a Gaussian process as a predictive model. An obvious example of known prior information is position and velocity related states. Incorporation of such information would be beneficial both computationally and for faster dynamics learning. This paper introduces a method of achieving this, yielding faster dynamics learning and a reduction in computational effort from O(Dn 2 ) to O((D - F)n 2 ) in the prediction stage for a system with D states, F known state relationships and n observations. The effectiveness of the method is demonstrated through its inclusion in the PILCO learning algorithm with application to the swing-up and balance of a torque \u2026", "Designer networks for time series processing": "The conventional tapped-delay neural net may be analyzed using statistical methods and the results of such analysis can be applied to model optimization. The authors review and extend efforts to demonstrate the power of this strategy within time series processing. They attempt to design compact networks using the so-called optima brain damage (OBD) method. The benefits from compact architectures are three-fold. Their generalization ability is at least comparable,they involve less computational burden, and they are faster to adapt if the environment changes. It is shown that the generalization error of the network may be estimated, without extensive cross-validation, using a modification of Akaike's final prediction error (FPE) estimate (1969).< >", "Efficient reinforcement learning for motor control": "Efficient Reinforcement Learning for Motor Control Page 1 Introduction Problem Setup Three Key Steps Results Conclusion Efficient Reinforcement Learning for Motor Control Marc Peter Deisenroth joint work with Carl Edward Rasmussen 10th International PhD Workshop on Systems and Control Hlubok\u00e1 nad Vltavou, Czech Republic September 23, 2009 MP Deisenroth (Engineering, Cambridge, UK) Efficient Learning in Control 1 Page 2 Introduction Problem Setup Three Key Steps Results Conclusion Why learning for control? Figure: Robots assembling a car. borrowed from www.harting-mitronics.ch machines can execute very complicated control commands MP Deisenroth (Engineering, Cambridge, UK) Efficient Learning in Control 2 Page 3 Introduction Problem Setup Three Key Steps Results Conclusion Why learning for control? Figure: Kasparov (left) vs. DeepBlue (right), 1996/1997 with permission from http://\u2026", "Identification of Gaussian Process State-Space Models with Particle Stochastic Approximation EM": "Gaussian process state-space models (GP-SSMs) are a very flexible family of models of nonlinear dynamical systems. They comprise a Bayesian nonparametric representation of the dynamics of the system and additional (hyper-)parameters governing the properties of this nonparametric representation. The Bayesian formalism enables systematic reasoning about the uncertainty in the system dynamics. We present an approach to maximum likelihood identification of the parameters in GP-SSMs, while retaining the full nonparametric description of the dynamics. The method is based on a stochastic approximation version of the EM algorithm that employs recent developments in particle Markov chain Monte Carlo for efficient identification.", "Gaussian Processes for time-marked time-series data": "In many settings, data is collected as multiple time series, where each recorded time series is an observation of some underlying dynamical process of interest. These observations are often time-marked with known event times, and one desires to do a range of standard analyses. When there is only one time marker, one simply aligns the observations temporally on that marker. When multiple time-markers are present and are at different times on different time series observations, these analyses are more difficult. We describe a Gaussian Process model for analyzing multiple time series with multiple time markings, and we test it on a variety of data.", "Probabilistic inference for fast learning in control": "We provide a novel framework for very fast model-based reinforcement learning in continuous state and action spaces. The framework requires probabilistic models that explicitly characterize their levels of confidence. Within this framework, we use flexible, non-parametric models to describe the world based on previously collected experience. We demonstrate learning on the cart-pole problem in a setting where we provide very limited prior knowledge about the task. Learning progresses rapidly, and a good policy is found after only a hand-full of iterations.", "Model-based reinforcement learning with continuous states and actions": "Finding an optimal policy in a reinforcement learning (RL) framework with continuous state and action spaces is challenging. Approximate solutions are often inevitable. GPDP is an approximate dynamic programming algorithm based on Gaussian process (GP) models for the value functions. In this paper, we extend GPDP to the case of unknown transition dynamics. After building a GP model for the transition dynamics, we apply GPDP to this model and determine a continuous-valued policy in the entire state space. We apply the resulting controller to the underpowered pendulum swing up. Moreover, we compare our results on this RL task to a nearly optimal discrete DP solution in a fully known environment.", "Approximate inference for robust Gaussian process regression": "Gaussian process (GP) priors have been successfully used in non-parametric Bayesian regression and classification models. Inference can be performed analytically only for the regression model with Gaussian noise. For all other likelihood models inference is intractable and various approximation techniques have been proposed. In recent years expectation-propagation (EP) has been developed as a general method for approximate inference. This article provides a general summary of how expectation-propagation can be used for approximate inference in Gaussian process models. Furthermore we present a case study describing its implementation for a new robust variant of Gaussian process regression. To gain further insights into the quality of the EP approximation we present experiments in which we compare to results obtained by Markov chain Monte Carlo (MCMC) sampling.", "Adaptive regularization": "Regularization, e.g., in the form of weight decay, is important for training and optimization of neural network architectures. In this work the authors provide a tool based on asymptotic sampling theory, for iterative estimation of weight decay parameters. The basic idea is to do a gradient descent in the estimated generalization error with respect to the regularization parameters. The scheme is implemented in the authors' Designer Net framework for network training and pruning, i.e., is based on the diagonal Hessian approximation. The scheme does not require essential computational overhead in addition to what is needed for training and pruning. The viability of the approach is demonstrated in an experiment concerning prediction of the chaotic Mackey-Glass series. The authors find that the optimized weight decays are relatively large for densely connected networks in the initial pruning phase, while they decrease as \u2026", "Advances in Gaussian processes": "\u2022 Model fitting\u2022 how do I fit the parameters?\u2022 what about overfitting?\u2022 Model Selection\u2022 how to I find out which model to use?\u2022 how sure can I be?\u2022 Interpretation\u2022 what is the accuracy of the predictions?\u2022 can I trust the predictions, even if\u2022... I am not sure about the parameters?\u2022... I am not sure of the model structure?", "Policy search for learning robot control using sparse data": "In many complex robot applications, such as grasping and manipulation, it is difficult to program desired task solutions beforehand, as robots are within an uncertain and dynamic environment. In such cases, learning tasks from experience can be a useful alternative. To obtain a sound learning and generalization performance, machine learning, especially, reinforcement learning, usually requires sufficient data. However, in cases where only little data is available for learning, due to system constraints and practical issues, reinforcement learning can act suboptimally. In this paper, we investigate how model-based reinforcement learning, in particular the probabilistic inference for learning control method (Pilco), can be tailored to cope with the case of sparse data to speed up learning. The basic idea is to include further prior knowledge into the learning process. As Pilco is built on the probabilistic Gaussian processes \u2026", "Modelling spikes with mixtures of factor analysers": "Identifying the action potentials of individual neurons from extracellular recordings, known as spike sorting, is a challenging problem. We consider the spike sorting problem using a generative model, mixtures of factor analysers, which concurrently performs clustering and feature extraction. The most important advantage of this method is that it quantifies the certainty with which the spikes are classified. This can be used as a means for evaluating the quality of clustering and therefore spike isolation. Using this method, nearly simultaneously occurring spikes can also be modelled which is a hard task for many of the spike sorting methods. Furthermore, modelling the data with a generative model allows us to generate simulated data.", "Model-based design analysis and yield optimization": "Fluctuations are inherent to any fabrication process. Integrated circuits and microelectromechanical systems are particularly affected by these variations, and due to high-quality requirements the effect on the devices' performance has to be understood quantitatively. In recent years, it has become possible to model the performance of such complex systems on the basis of design specifications, and model-based sensitivity analysis has made its way into industrial engineering. We show how an efficient Bayesian approach, using a Gaussian process prior, can replace the commonly used brute-force Monte Carlo scheme, making it possible to apply the analysis to computationally costly models. We introduce a number of global, statistically justified sensitivity measures for design analysis and optimization. Two models of integrated systems serve us as case studies to introduce the analysis and to assess its convergence \u2026", "Data-efficient reinforcement learning in continuous-state POMDPs": "We present a data-efficient reinforcement learning algorithm resistant to observation noise. Our method extends the highly data-efficient PILCO algorithm (Deisenroth & Rasmussen, 2011) into partially observed Markov decision processes (POMDPs) by considering the filtering process during policy evaluation. PILCO conducts policy search, evaluating each policy by first predicting an analytic distribution of possible system trajectories. We additionally predict trajectories w.r.t. a filtering process, achieving significantly higher performance than combining a filter with a policy optimised by the original (unfiltered) framework. Our test setup is the cartpole swing-up task with sensor noise, which involves nonlinear dynamics and requires nonlinear control.", "System identification in Gaussian process dynamical systems": "Results\nWe evaluate our EM based system identification on both real and synthetic data sets using onestep-ahead prediction. We compare GPIL predictions to eight other methods, the time independent model (TIM) with yt\u223c N (\u00b5c, \u03a3c), the Kalman filter, the UKF, the EKF, NDFA, GPDM, the Autoregressive GP (ARGP) trained on a set of pairs (yi, yi+ 1), and the GP-UKF [2], which uses the GPIL pseudo training set. Note that the EKF, the UKF, and the GP-UKF require access to the true functions f and g. For synthetic data, f and g are known, for the real data set, we used \u201ctrue\u201d functions that resemble the mean functions of the GPIL learned GP models.\nReal data. We use historical snowfall data in Whistler, BC, Canada to evaluate GPIL on real data. We evaluate the models\u2019 ability to predict next day\u2019s snowfall using 35 years of test data; we trained on daily snowfall from Jan. 1 1972\u2013Dec. 31 1973 and tested on next day predictions for 1974\u20132008. The results are shown in table 1. Snowfall time series have many observations of zero, corresponding to days when it does not snow.", "Nonparametric mixtures of factor analyzers": "The mixtures of factor analyzers (MFA) model allows data to be modeled as a mixture of Gaussians with a reduced parametrization. We present the formulation of a nonparametric form of the MFA model, the Dirichlet process MFA (DPMFA). The proposed model can be used for density estimation or clustering of high dimensional data. We utilize the DPMFA for clustering the action potentials of different neurons from extracellular recordings, a problem known as spike sorting. DPMFA model is compared to Dirichlet process mixtures of Gaussians model (DPGMM) which has a higher computational complexity. We show that DPMFA has similar modeling performance in lower dimensions when compared to DPGMM, and is able to work in higher dimensions.", "Reinforcement learning with reference tracking control in continuous state spaces": "The contribution described in this paper is an algorithm for learning nonlinear, reference tracking, control policies given no prior knowledge of the dynamical system and limited interaction with the system through the learning process. Concepts from the field of reinforcement learning, Bayesian statistics and classical control have been brought together in the formulation of this algorithm which can be viewed as a form of indirect self tuning regulator. On the task of reference tracking using a simulated inverted pendulum it was shown to yield generally improved performance on the best controller derived from the standard linear quadratic method using only 30 s of total interaction with the system. Finally, the algorithm was shown to work on the simulated double pendulum proving its ability to solve nontrivial control tasks.", "Closed-form inference and prediction in Gaussian process state-space models": "We examine an analytic variational inference scheme for the Gaussian Process State Space Model (GPSSM) - a probabilistic model for system identification and time-series modelling. Our approach performs variational inference over both the system states and the transition function. We exploit Markov structure in the true posterior, as well as an inducing point approximation to achieve linear time complexity in the length of the time series. Contrary to previous approaches, no Monte Carlo sampling is required: inference is cast as a deterministic optimisation problem. In a number of experiments, we demonstrate the ability to model non-linear dynamics in the presence of both process and observation noise as well as to impute missing information (e.g. velocities from raw positions through time), to de-noise, and to estimate the underlying dimensionality of the system. Finally, we also introduce a closed-form method for multi-step prediction, and a novel criterion for assessing the quality of our approximate posterior.", "Gaussian mixture modeling with Gaussian process latent variable models": "Density modeling is notoriously difficult for high dimensional data. One approach to the problem is to search for a lower dimensional manifold which captures the main characteristics of the data. Recently, the Gaussian Process Latent Variable Model (GPLVM) has successfully been used to find low dimensional manifolds in a variety of complex data. The GPLVM consists of a set of points in a low dimensional latent space, and a stochastic map to the observed space. We show how it can be interpreted as a density model in the observed space. However, the GPLVM is not trained as a density model and therefore yields bad density estimates. We propose a new training strategy and obtain improved generalisation performance and better density estimates in comparative evaluations on several benchmark data sets.", "Semi-supervised kernel regression using whitened function classes": "The use of non-orthonormal basis functions in ridge regression leads to an often undesired non-isotropic prior in function space. In this study, we investigate an alternative regularization technique that results in an implicit whitening of the basis functions by penalizing directions in function space with a large prior variance. The regularization term is computed from unlabelled input data that characterizes the input distribution. Tests on two datasets using polynomial basis functions showed an improved average performance compared to standard ridge regression.", "A Bayesian approach to modeling uncertainty in gene expression clusters": "The use of clustering methods has rapidly become one of the standard computational approaches to understanding microarray gene expression data [3, 1, 7]. In clustering, the patterns of expression of different genes across time, treatments, and tissues are grouped into distinct clusters (perhaps organized hierarchically) in which genes in the same cluster are assumed to be potentially functionally related or to be influenced by a common upstream factor. Such cluster structure can be used to aid in the elucidation of regulatory networks. For example, a compendium of gene expression profiles corresponding to mutants and chemical treatments can be used as a systematic tool to identify gene functions because mutants or drug targets that display similar profiles are likely to share cellular functions [5].\nOne commonly used computational method of non-hierarchical clustering based on measuring Euclidean distance between gene expression profiles is given by the k-means algorithm [4]. However, the k-means algorithm is inadequate for describing clusters of unequal size or shape [6]. A generalization of k-means can be derived from the theory of maximum likelihood estimation of Gaussian mixture models [8]. In a Gaussian mixture model, the data (eg gene expression profiles, which can be arranged into p-dimensional vectors y) is assumed to have been generated from a finite number (k) of Gaussians,", "Learning with Gaussian Processes": "\u2022 Model fitting\u2022 how do I fit the parameters?\u2022 what about overfitting?\u2022 Model Selection\u2022 how to I find out which model to use?\u2022 how sure can I be?\u2022 Interpretation\u2022 what is the accuracy of the predictions?\u2022 can I trust the predictions, even if\u2022... I am not sure about the parameters?\u2022... I am not sure of the model structure?", "Efficient adaptive sampling of the psychometric function by maximizing information gain": "A common task in psychophysics is to measure the psychometric function. A psychometric function can be described by its shape and four parameters: offset or threshold, slope or width, false alarm rate or chance level and miss or lapse rate. Depending on the parameters of interest some points on the psychometric function may be more informative than others. Adaptive methods attempt to place trials on the most informative points based on the data collected in previous trials. A new Bayesian adaptive psychometric method placing trials by minimising the expected entropy of the posterior probabilty dis-tribution over a set of possible stimuli is introduced. The method is more flexible, faster and at least as efficient as the established method (Kontsevich and Tyler, 1999). Comparably accurate (2dB) threshold and slope estimates can be obtained after about 30 and 500 trials, respectively. By using a dynamic termination criterion the efficiency can be further improved. The method can be applied to all experimental designs including yes/no designs and allows acquisition of any set of free parameters. By weighting the importance of parameters one can include nuisance parameters and adjust the relative expected errors. Use of nuisance parameters may lead to more accurate estimates than assuming a guessed fixed value. Block designs are supported and do not harm the performance if a sufficient number of trials are performed. The method was evaluated by computer simulations in which the role of parametric assumptions, its robustness, the quality of different point estimates, the effect of dynamic termination criteria and many other settings were \u2026", "Non-Factorised Variational Inference in Dynamical Systems": "We focus on variational inference in dynamical systems where the discrete time transition function (or evolution rule) is modelled by a Gaussian process. The dominant approach so far has been to use a factorised posterior distribution, decoupling the transition function from the system states. This is not exact in general and can lead to an overconfident posterior over the transition function as well as an overestimation of the intrinsic stochasticity of the system (process noise). We propose a new method that addresses these issues and incurs no additional computational costs.", "Nonlinear Set Membership Regression with Adaptive Hyper-Parameter Estimation for Online Learning and Control": "Methods known as Lipschitz Interpolation or Nonlinear Set Membership regression have become established tools for nonparametric system-identification and data-based control. They utilise presupposed Lipschitz properties to compute inferences over unobserved function values. Unfortunately, they rely on the a priori knowledge of a Lipschitz constant of the underlying target function which serves as a hyper-parameter. We propose a closed-form estimator of the Lipschitz constant that is robust to bounded observational noise in the data. The merger of Lipschitz Interpolation with the new hyper-parameter estimator gives a new nonparametric machine learning method for which we derive online learning convergence guarantees. Furthermore, we apply our learning method to model-reference adaptive control and provide a convergence guarantee on the closed-loop dynamics. In a simulated flight manoeuvre \u2026", "Generalization in neural networks": "This report is concerned with methods for optimizing the generalization ability of neural networks. The framework is developed to deal with regression type problems, where the networks are trained on a limited amount of noisy data. In this context the problem can be formulated as nding the optimal trade o between data t and model complexity.\nTwo paradigms for reducing model complexity are discussed: pruning and weight decay. It is shown by numerical experiments that application of weight decay is essential for obtaining good generalization performance. This is explained by the way in which weight decay con nes the space of possible networks to a space ofreasonable'networks.", "The countably infinite Bayesian Gaussian mixture density model": "In a Bayesian mixture model, there is no need a priori to restrict the number of components to be nite. In nite mixture models sidestep the problem of nding the\\correct\" number of components, and may be handled using a nite amount of computation. In this paper it is demonstrated how inference may be done in in nite mixture models using a Markov Chain whose implementation relies entirely on Gibbs sampling. An example is given of application to multivariate density estimation.", "Bayesian Inference for Efficient Learning in Control": "In contrast to humans or animals, artificial learners often require more trials when learning motor control tasks solely based on experience. Efficient autonomous learners will reduce the amount of engineering required to solve control problems. By using probabilistic forward models, we can employ two key ingredients of biological learning systems to speed up artificial learning. We present a consistent and coherent Bayesian framework that allows for efficient autonomous experience-based learning. We demonstrate the success of our learning algorithm by applying it to challenging nonlinear control problems in simulation and in hardware.", "Automated Bayesian System Identification with NARX Models": "We introduce GP-FNARX: a new model for nonlinear system identification based on a nonlinear autoregressive exogenous model (NARX) with filtered regressors (F) where the nonlinear regression problem is tackled using Gaussian processes (GP). We integrate data pre-processing with system identification into a fully automated procedure that goes from raw data to an identified model without human intervention. Moreover, we obtain a Bayesian model of the system's dynamics which is able to report its uncertainty in regions where the data is scarce. The automated approach, the modeling of uncertainty and its relatively low computational cost make GP-FNARX a good candidate for applications in robotics and adaptive control tasks.", "Reducing Model Bias in Reinforcement Learning": "Model bias is one of the main reasons why reinforcement learning (RL) algorithms often need so many trials to successfully learn a task. Model bias has been known for decades, but no general solution to this problem has yet been proposed. We shed some light on the challenges of learning models from data and propose learning probabilistic models to reduce model bias by faitfully incorporating the model\u2019s uncertainty into planning and policy learning.\nConsider the problem of autonomously learning control tasks from scratch, ie, without using specific prior knowledge such as a known dynamics model and/or demonstrations by a \u201cteacher\u201d. Solving these scenarios using model-free RL, often requires many interaction with the system, which can be impractical or infeasible in robotic systems, for instance. Model-based RL uses information from trials (which are used to build a model of the system) more efficiently than model-free learning; the policy, however, is always optimized in light of the model, not the true environment (model bias). Researchers often shy away from model-based methods since they rely on an \u201caccurate\u201d dynamics model, which is difficult to obtain. A key challenge in model-based RL is therefore to determine a faithful representation of the surrounding world, in particular, when we have only few interactions with the system", "A Practical and Conceptual Framework for Learning in Control": "We propose a fully Bayesian approach for efficient reinforcement learning (RL) in Markov decision processes with continuous-valued state and action spaces when no expert knowledge is available. Our framework is based on well-established ideas from statistics and machine learning and learns fast since it carefully models, quantifies, and incorporates available knowledge when making decisions. The key ingredient of our framework is a probabilistic model, which is implemented using a Gaussian process (GP), a distribution over functions. In the context of dynamic systems, the GP models the transition function. By considering all plausible transition functions simultaneously, we reduce model bias, a problem that frequently occurs when deterministic models are used. Due to its generality and efficiency, our RL framework can be considered a conceptual and practical approach to learning models and controllers when expert knowledge is difficult to obtain or simply not available, which makes system identification hard.", "Multiple\u2013step ahead predictions of time series models with gaussian processes by propagating uncertainty": "We present an approach to multiple-step ahead prediction using Gaussian process prior models, propagating the uncertainty as we predict.\nOver the last decade, the non parametric Gaussian process (GPs) modelling approach (see (O\u2019Hagan, 1978),(Williams and Rasmussen, 1996)) has grown in popularity for nonlinear regression problems. GPs naturally provide the uncertainty (variance) associated with each estimated point. Furthermore, they have demonstrated a powerful predictive performance, comparable to other modelling approaches like neural networks or local learning methods. Currently, the multiple-step ahead prediction task using GPs has been achieved by either training the model to learn how to make k-step ahead predictions, direct method, or by doing repeated one-step ahead predictions, iterative method. One obvious reason to prefer the iterative approach to the direct method is that it provides us with predictions for any k-step ahead, unlike the direct method which is only valid for the k-step ahead points. In what follows, we focus on the iterative method. In this framework, one computes the \u00d8 prediction based on the previous predicted points. Usually, in doing so, only the predicted means (estimate of the output) are used. In this paper, we take account of the variance associated with each point predicted. Therefore, we do not base the new prediction on the previous predicted mean value only but also on the uncertainty attached to it; which is equivalent to considering random inputs. In this context, the predictive distribution for the next point to be predicted is then obtained by integrating over all possible inputs. Since this \u2026", "Bayesian Learning in Feed Forward Neural Networks": "Bayesian methods are applicable to complex modeling tasks. In this review, the principles of Bayesian inference are presented and applied to neural network models. Several approximate implementations are discussed, and their advantages over conventional frequentist model training and selection are outlined. It is argued that Bayesian methods are preferable to traditional approaches, although empirical evidence for this is still sparse.", "PILCO Code Documentation v0. 9": "Reinforcement learning (RL) is a general paradigm for learning (optimal) policies for stochastic sequential decision making processes [10]. In many practical engineering applications, such as robotics and control, RL methods are difficult to apply: First, the state and action spaces are often continuous valued and high dimensional. Second, the number of interactions that can be performed with a real system is practically limited. Therefore, learning methods that efficiently extract valuable information from available data are important. Policy search methods have been playing an increasingly important role in robotics as they consider a simplified RL problem and search (optimal) policies in a constrained policy space [1, 3], typically in an episodic set-up, ie, a finite-horizon set-up with a fixed initial state (distribution).\nWe present the pilco software package for data-efficient policy search that allows to learn (non) linear controllers with hundreds of parameters for high-dimensional systems. A key element is a learned probabilistic GP dynamics model. Uncertainty about the learned dynamics model (expressed by the GP posterior) is explicitly taken into account for multiple-step ahead predictions, policy evaluation, and policy improvement.", "Machine Learning, Probabilistic Inference, System Identification and Control": "In this talk, I will explore the use of recent techniques from probabilistic non-parametric machine learning to system identification and control. In this framework, the simultaneous tasks of inferring dynamics and designing a controller is thought of as a statistical inference problem. Conventionally, stochastic models (such as Gaussians) have been used ubiquitously to characterise noise and disturbances; here I will show how the Gaussian (slightly extended to a Gaussian process), can also naturally be used to model the (partially unknown) system dynamics.\nThus, the inference problem is solved with Gaussian processes, using no task specific prior knowledge. Traditional wisdom may suggest that learning without strong prior information will be impractically slow, but I show that provided that uncertainties in the learned dynamics are carefully characterised, learning can be extremely rapid, and result in highly practical \u2026", "Filtering (and Smoothing) in Gaussian Process Dynamic Systems": "Filtering (and Smoothing) in Gaussian Process Dynamic Systems Page 1 Introduction Filtering in Nonlinear Systems GP Filters Illustrations References Filtering (and Smoothing) in Gaussian Process Dynamic Systems Marc Peter Deisenroth joint work with MF Huber, R Turner, CE Rasmussen, and UD Hanebeck Talk at Machine Learning and Optimisation Group University of Manchester, UK November 23, 2009 MP Deisenroth (Engineering, Cambridge, UK) Filtering and Smoothing with GPs 1 Page 2 Introduction Filtering in Nonlinear Systems GP Filters Illustrations ReferencesForward-Backward Algorithm Motivation measurement device (sensor) system position, velocity g(position,noise) p(position, velocity) throttle filter controller estimating (latent) states from noisy measurements MP Deisenroth (Engineering, Cambridge, UK) Filtering and Smoothing with GPs 2 Page 3 Introduction Filtering in Nonlinear Systems GP \u2026", "MCMC inference in (Conditionally) Conjugate Dirichlet Process Gaussian Mixture Models": "We compare the predictive accuracy of the Dirichlet Process Gaussian mixture models using conjugate and conditionally conjugate priors and show that better density models result from using the wider class of priors. We explore several MCMC schemes exploiting conditional conjugacy and show their computational merits on several multidimensional density estimation problems.", "Gaussian Processes": "\u2022 Let f=(f (x1), f (x2),..., f (xN)) be an N-dimensional vector of function values evaluated at N points xi\u2208 X.\u2022 Note that f is a random variable.\u2022 Definition: P (f) is a Gaussian process if for any finite subset {x1,..., xN}\u2282 X, the marginal distribution over that finite subset P (f) has a multivariate Gaussian distribution.\n\u2013p. 7", "Sampling for Non-conjugate Infinite Latent Feature Models": "Latent variable models are powerful tools to model the underlying structure in data. Infinite latent variable models can be defined using Bayesian nonparametrics. Dirichlet process (DP) models constitute an example of infinite latent class models in which each object is assumed to belong to one of the, mutually exclusive, infinitely many classes. Recently, the Indian buffet process (IBP) has been defined as an extension of the DP. IBP is a distribution over sparse binary matrices with infinitely many columns which can be used as a distribution for non-exclusive features. Inference using Markov chain Monte Carlo (MCMC) in conjugate IBP models has been previously described, however requiring conjugacy restricts the use of IBP. We describe an MCMC algorithm for non-conjugate IBP models. Modelling the choice behaviour is an important topic in psychology, economics and related fields. Elimination by Aspects (EBA) is a choice model that assumes each alternative has latent features with associated weights that lead to the observed choice outcomes. We formulate a non-parametric version of EBA by using IBP as the prior over the latent binary features. We infer the features of objects that lead to the choice data by using our sampling scheme for inference.", "The DELVE user manual": "This manual describes the preliminary release of the DELVE environment. Some features described here have not yet implemented, as noted. Support for regression tasks is presently somewhat more developed than that for classification tasks. We recommend that you exercise caution when using this version of DELVE for real work, as it is possible that bugs remain in the software. We hope that you will send us reports of any problems you encounter, as well as any other comments you may have on the software or manual, at the e-mail address below. Please mention the version number of the manual and/or the software with any comments you send.", "Bayesian learning in multi-layer perceptron neural network using Monte Carlo: mlp-mc-1": "A Bayesian implementation of learning in neural networks using Monte Carlo sampling has been developed by Neal (1996). This computation intensive method has shown encouraging performance in (Neal 1996) and in a study using several datasets in (Rasmussen 1996). For a full description of the method the reader is referred to (Neal 1996). Here a brief description of the algorithm will be given, along with the heuristics employed. A feed forward multi-layer perceptron neural network with a single hidden layer of hyperbolic tangent units is used; the network is fully connected, including direct connections from the input to the output layer. The output units are linear. All units have biases. A network with a single output, I inputs and H hidden units implements the function f (x)=", "MARS3. 6 with Bagging: mars3. 6-bag-1": "The Multivariate Adaptive Regression Splines (MARS) method of Friedman (Friedman 1991) has also been tested. This method is a fairly well known method for non-linear regression for high dimensional data from the statistics community. A detailed description of MARS will not be given here, see (Friedman 1991). Following is a simplistic account of MARS gives a avor of the method. The input space is carved up into several (overlapping) regions in which splines are t. The t is built using rst a constructive phase which introduces input regions and splines followed by a pruning phase. The nal model has the form of a sum of products of univariate splines; it is a continuous function (with continuous derivatives) and is additive in the sets of variables allowed to interact.\nFriedman has supplied his FORTRAN implementation of MARS (version 3.6). Since MARS is not very computationally demanding it is used in \u2026", "Electronics Institute Technical University of Denmark Email: ed@ eileen. ei. dth. dk 18 August 1993": "This report is concerned with methods for optimizing the generalization ability of neural networks. The framework is developed to deal with regression type problems, where the networks are trained on a limited amount of noisy data. In this context the problem can be formulated as nding the optimal trade o between data t and model complexity. Two paradigms for reducing model complexity are discussed: pruning and weight decay. It is shown by numerical experiments that application of weight decay is essential for obtaining good generalization performance. This is explained by the way in which weight decay connes the space of possible networks to a space ofreasonable'networks. Two methods for making statistical estimates of the generalization performance without use of validation sets are presented: the Generalizationmethod and the Bayesian method. The advantage of not needing validation sets is that all \u2026", "Mathematical Background": "Mathematical Background | MIT Press books | IEEE Xplore Mathematical Background Article #: ISBN Information: Online ISBN: 9780262256834 INSPEC Accession Number: Persistent Link: https://xplorestaging.ieee.org/servlet/opac?punumber=6267323 More \u00bb Publisher: MIT Press IEEE Account Change Username/Password Update Address Purchase Details Payment Options Order History View Purchased Documents Profile Information Communications Preferences Profession and Education Technical Interests Need Help? US & Canada: +1 800 678 4333 Worldwide: +1 732 981 0060 Contact & Support About IEEE Xplore Contact Us Help Accessibility Terms of Use Nondiscrimination Policy Sitemap Privacy & Opting Out of Cookies A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. \u00a9 Copyright 2021 IEEE - All \u2026", "Dynamic Programming with Gaussian Process Models": "Reinforcement learning with continuous state and action spaces is a challenging problem. In case of nonlinear stochastic dynamics no general closed-form solutions exist to determine an optimal policy. Here, we attempt to learn a model of unknown stochastic dynamics of the environment. Using this model we apply an approximate dynamic programming approach based on Gaussian processes, which we call GPDP [1]. We explicitly take sources of uncertainty into account. The optimal state-feedbacks returned by this algorithm are generalized to a discontinuous policy on the entire state space by means of a mixture-of-experts-like algorithm where we switch between two Gaussian process models trained on subsets of state-feedbacks.", "Linear Projections and Gaussian Process Reconstructions": "Linear map from latent to data The reconstruction of the yi from the xi is also linear Reconstructed hyperplane is spanned by principal eigenvectors This is often a poor reconstruction! But most dimensional reduction methods don\u2019t even offer a map between latent and data\nExample: hand-written digits 16\u00d7 16 gray-scale images of the 2, 3, 4 and 5s 2-dimensional PCA projection Linear reconstruction from PCA", "Empirical and Theoretical Analysis of Bayesian Inference in Gaussian Process Models": "The Gaussian process (GP) predictor is a popular kernel method allowing to express uncertainty about smooth functions [5]. In Bayesian inference, a GP prior is combined with the data to yield the posterior reflecting the remaining uncertainty about the function. Only for the case of regression with Gaussian noise, this procedure is analytically feasible. In all other cases, including probabilistic classification, one has to resort to approximations in order to make predictions. Even in regression, Bayesian model averaging is prohibitive. Approximate model selection can be done in two ways: The evidence framework [2] optimizes the marginal likelihood and the predictive approach [7] uses cross validation to approximate and optimize the predictive performance of a model.\nExtending the work of [1] on the benefits of Expectation Propagation (EP) over the Laplace approximation (LA) in approximate classification, we analyze direct Kullback-Leibler divergence minimization (KL) and Variational Bounding (VB) which is an analytically convenient special case of KL [3]. Furthermore, the Factorial Variational (FV) algorithm, a meanfield approach as well as a heuristic approach termed label regression (LR) are included. In a unified treatment, we analytically characterize expected properties of the algorithms, we later confirm and broaden by extensive simulations including MCMC sampling on several benchmark datasets. We provide a clear statement about both theoretical and algorithmic advantages and shortcomings of the algorithms (Figure 1). In practice, an approximation method has to satisfy a wide range of requirements. If runtime is the major concern or \u2026", "A Bayesian Perspec\\ ve on Sta\\ s\\ cal Learning Theory": "P (C= h)= 1 \u201cI\u2019m absolutely certain the coin is heads\u201d P (C= h)= 0 \u201cI\u2019m absolutely certain the coin is tails\u201d P (C= h)= 1/2 \u201cI\u2019m completely uncertain\u201d P (C= h)= p \u201cThe probability that the coin is heads is p.\u201d p is an uncertain quantity; ie, we model it as random and put a distribution on it representing our uncertainty before learning C", "Parametrik Olmayan Etmen \u00c7\u00f6z\u00fcmlemesi Kar\u0131s\u0131mlar\u0131 Nonparametric Mixtures of Factor Analyzers": "The mixtures of factor analyzers (MFA) model allows data to be modeled as a mixture of Gaussians with a reduced parametrization. We present the formulation of a nonparametric form of the MFA model, the Dirichlet process MFA (DPMFA). The proposed model can be used for density estimation or clustering of high dimensiona data. We utilize the DPMFA for clustering the action potentials of different neurons from extracellular recordings, a problem known as spike sorting. DPMFA model is compared to Dirichlet process mixtures of Gaussians model (DPGMM) which has a higher computational complexity. We show that DPMFA has similar modeling performance in lower dimensions when compared to DPGMM, and is able to work in higher dimensions.", "Gaussian process priors with uncertain inputs: Multiple-step ahead prediction": null, "Gaussian Processes in Reinforcement Learning": null, "Assessing Approximations for Gaussian Process Classi\ufb01cation": null, "Integrated Pre-Processing for Bayesian Nonlinear System Identification with Gaussian Processes": null, "Machine Learning Challenges: evaluating predictive uncertainty, visual object classification and recognising textual entailment}": null, "Nonparametric mixtures of factor analyzers| Parametrik olmayan etmen \u00e7\u00f6z\u00fcmlemesi kari\u015fimlari": null, "Department of Mathematical Modelling Technical University of Denmark": null, "Pattern recognition: 26th DAGM Symposium, T\u00fcbingen, Germany, August 30-1 September 2004: proceedings": null, "Solving Challenging Non-linear Regression Problems by Manipulating a Gaussian Distribution": null, "Feature Selection for Trouble Shooting": null, "Bayesian Inference and Gaussian Processes": null, "of Book: Switching and Learning in Feedback Systems.": null, "Pattern Recognition: Proceedings of the 26th DAGM Symposium on Pattern Recognition held in Tubingen, Germany, August 2004": null, "Pattern recognition 26th dagm symposium, august 30 september 1, 2004, proceedings lecture notes in computer science vol 3175": null, "Prediction at an Uncertain Input for Gaussian Processes and Relevance Vector Machines-Application to Time-Series Forecasting": null, "May 3, 1996": null, "Prediction on Spike Data": null, "Bayesian Monte Carlo": null, "Pattern recognition(T\u00fcbingen, 30 August- 1 September 2004)": null, "Description: a Bayesian adaptive psychometric methods toolbox which implements a new universal method and QUEST, ZEST, slope-ZEST, MUEST, YAAP etc. Version: 0.4 Homepage": null, "Pattern Recognition:... DAGM Symposium; Proceedings": null}, "Adrian Weller": {"Accountability of AI under the law: The role of explanation": "The ubiquity of systems using artificial intelligence or \"AI\" has brought increasing attention to how those systems should be regulated. The choice of how to regulate AI systems will require care. AI systems have the potential to synthesize large amounts of data, allowing for greater levels of personalization and precision than ever before---applications range from clinical decision support to autonomous driving and predictive policing. That said, there exist legitimate concerns about the intentional and unintentional negative consequences of AI systems. There are many ways to hold AI systems accountable. In this work, we focus on one: explanation. Questions about a legal right to explanation from AI systems was recently debated in the EU General Data Protection Regulation, and thus thinking carefully about when and how explanation from AI systems might improve accountability is timely. In this work, we review contexts in which explanation is currently required under the law, and then list the technical considerations that must be considered if we desired AI systems that could provide kinds of explanations that are currently required of humans.", "Rethinking Attention with Performers": "We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.", "Concrete problems for autonomous vehicle safety: Advantages of Bayesian deep learning": "Autonomous vehicle (AV) software is typically composed of a pipeline of individual components, linking sensor inputs to motor outputs. Erroneous component outputs propagate downstream, hence safe AV software must consider the ultimate effect of each component\u2019s errors. Further, improving safety alone is not sufficient. Passengers must also feel safe to trust and use AV systems. To address such concerns, we investigate three under-explored themes for AV research: safety, interpretability, and compliance. Safety can be improved by quantifying the uncertainties of component outputs and propagating them forward through the pipeline. Interpretability is concerned with explaining what the AV observes and why it makes the decisions it does, building reassurance with the passenger. Compliance refers to maintaining some control for the passenger. We discuss open challenges for research within these themes. We highlight the need for concrete evaluation metrics, propose example problems, and highlight possible solutions.", "Explainable machine learning in deployment": "Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a \u2026", "A Unified Approach to Quantifying Algorithmic Unfairness: Measuring Individual & Group Unfairness via Inequality Indices": "Discrimination via algorithmic decision making has received considerable attention. Prior work largely focuses on defining conditions for fairness, but does not define satisfactory measures of algorithmic unfairness. In this paper, we focus on the following question: Given two unfair algorithms, how should we determine which of the two is more unfair? Our core idea is to use existing inequality indices from economics to measure how unequally the outcomes of an algorithm benefit different individuals or groups in a population. Our work offers a justified and general framework to compare and contrast the (un) fairness of algorithmic predictors. This unifying approach enables us to quantify unfairness both at the individual and the group level. Further, our work reveals overlooked tradeoffs between different fairness notions: using our proposed measures, the overall individual-level unfairness of an algorithm can be \u2026", "From parity to preference-based notions of fairness in classification": "The adoption of automated, data-driven decision making in an ever expanding range of applications has raised concerns about its potential unfairness towards certain social groups. In this context, a number of recent studies have focused on defining, detecting, and removing unfairness from data-driven decision systems. However, the existing notions of fairness, based on parity (equality) in treatment or outcomes for different social groups, tend to be quite stringent, limiting the overall decision making accuracy. In this paper, we draw inspiration from the fair-division and envy-freeness literature in economics and game theory and propose preference-based notions of fairness -- given the choice between various sets of decision treatments or outcomes, any group of users would collectively prefer its treatment or outcomes, regardless of the (dis)parity as compared to the other groups. Then, we introduce tractable proxies to design margin-based classifiers that satisfy these preference-based notions of fairness. Finally, we experiment with a variety of synthetic and real-world datasets and show that preference-based fairness allows for greater decision accuracy than parity-based fairness.", "Challenges for Transparency": "Transparency is often deemed critical to enable effective real-world deployment of intelligent systems. Yet the motivations for and benefits of different types of transparency can vary significantly depending on context, and objective measurement criteria are difficult to identify. We provide a brief survey, suggesting challenges and related concerns. We highlight and review settings where transparency may cause harm, discussing connections across privacy, multi-agent game theory, economics, fairness and trust.", "Human perceptions of fairness in algorithmic decision making: A case study of criminal risk prediction": "As algorithms are increasingly used to make important decisions that affect human lives, ranging from social benefit assignment to predicting risk of criminal recidivism, concerns have been raised about the fairness of algorithmic decision making. Most prior works on algorithmic fairness normatively prescribe how fair decisions ought to be made. In contrast, here, we descriptively survey users for how they perceive and reason about fairness in algorithmic decision making. A key contribution of this work is the framework we propose to understand why people perceive certain features as fair or unfair to be used in algorithms. Our framework identifies eight properties of features, such as relevance, volitionality and reliability, as latent considerations that inform people\u00bb s moral judgments about the fairness of feature use in decision-making algorithms. We validate our framework through a series of scenario-based surveys \u2026", "Beyond distributive fairness in algorithmic decision making: Feature selection for procedurally fair learning": "With widespread use of machine learning methods in numerous domains involving humans, several studies have raised questions about the potential for unfairness towards certain individuals or groups. A number of recent works have proposed methods to measure and eliminate unfairness from machine learning models. However, most of this work has focused on only one dimension of fair decision making: distributive fairness, ie, the fairness of the decision outcomes. In this work, we leverage the rich literature on organizational justice and focus on another dimension of fair decision making: procedural fairness, ie, the fairness of the decision making process. We propose measures for procedural fairness that consider the input features used in the decision process, and evaluate the moral judgments of humans regarding the use of these features. We operationalize these measures on two real world datasets using human surveys on the Amazon Mechanical Turk (AMT) platform, demonstrating that our measures capture important properties of procedurally fair decision making. We provide fast submodular mechanisms to optimize the tradeoff between procedural fairness and prediction accuracy. On our datasets, we observe empirically that procedural fairness may be achieved with little cost to outcome fairness, but that some loss of accuracy is unavoidable.", "Leveraging data science to combat COVID-19: A comprehensive review": "COVID-19, an infectious disease caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organisation (WHO) in March 2020. By mid-August 2020, more than 21 million people have tested positive worldwide. Infections have been growing rapidly and tremendous efforts are being made to fight the disease. In this paper, we attempt to systematise the various COVID-19 research activities leveraging data science, where we define data science broadly to encompass the various methods and tools-including those from artificial intelligence (AI), machine learning (ML), statistics, modeling, simulation, and data visualization-that can be used to store, process, and extract insights from data. In addition to reviewing the rapidly growing body of recent research, we survey public datasets and repositories that can be used for further work to track COVID-19 spread and mitigation strategies. As part of this \u2026", "The Case for Process Fairness in Learning: Feature Selection for Fair Decision Making": "Machine learning methods are increasingly being used to inform, or sometimes even directly to make, important decisions about humans. A number of recent works have focussed on the fairness of the outcomes of such decisions, particularly on avoiding decisions that affect users of different sensitive groups (eg, race, gender) disparately. In this paper, we propose to consider the fairness of the process of decision making. Process fairness can be measured by estimating the degree to which people consider various features to be fair to use when making an important legal decision. We examine the task of predicting whether or not a prisoner is likely to commit a crime again once released by analyzing the dataset considered by ProPublica relating to the COMPAS system. We introduce new measures of people\u2019s discomfort with using various features, show how these measures can be estimated, and consider the effect of removing the uncomfortable features on prediction accuracy and on outcome fairness. Our empirical analysis suggests that process fairness may be achieved with little cost to outcome fairness, but that some loss of accuracy is unavoidable.", "Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims": "With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.", "Structured Evolution with Compact Architectures for Scalable Policy Optimization": "We present a new method of blackbox optimization via gradient approximation with the use of structured random orthogonal matrices, providing more accurate estimators than baselines and with provable theoretical guarantees. We show that this algorithm can be successfully applied to learn better quality compact policies than those using standard gradient estimation techniques. The compact policies we learn have several advantages over unstructured ones, including faster training algorithms and faster inference. These benefits are important when the policy is deployed on real hardware with limited resources. Further, compact policies provide more scalable architectures for derivative-free optimization (DFO) in high-dimensional spaces. We show that most robotics tasks from the OpenAI Gym can be solved using neural networks with less than 300 parameters, with almost linear time complexity of the inference phase, with up to 13x fewer parameters relative to the Evolution Strategies (ES) algorithm introduced by Salimans et al.(2017). We do not need heuristics such as fitness shaping to learn good quality policies, resulting in a simple and theoretically motivated training mechanism.", "Blind Justice: Fairness with Encrypted Sensitive Attributes": "Recent work has explored how to train machine learning models which do not discriminate against any subgroup of the population as determined by sensitive attributes such as gender or race. To avoid disparate treatment, sensitive attributes should not be considered. On the other hand, in order to avoid disparate impact, sensitive attributes must be examined, eg, in order to learn a fair model, or to check if a given model is fair. We introduce methods from secure multi-party computation which allow us to avoid both. By encrypting sensitive attributes, we show how an outcome-based fair model may be learned, checked, or have its outputs verified and held to account, without users revealing their sensitive attributes.", "Transparency: motivations and challenges": "Transparency is often deemed critical to enable effective real-world deployment of intelligent systems. Yet the motivations for and benefits of different types of transparency can vary significantly depending on context, and objective measurement criteria are difficult to identify. We provide a brief survey, suggesting challenges and related concerns, particularly when agents have misaligned interests. We highlight and review settings where transparency may cause harm, discussing connections across privacy, multi-agent game theory, economics, fairness and trust.", "Discovering Interpretable Representations for Both Deep Generative and Discriminative Models": "Interpretability of representations in both deep generative and discriminative models is highly desirable. Current methods jointly optimize an objective combining accuracy and interpretability. However, this may reduce accuracy, and is not applicable to already trained models. We propose two interpretability frameworks. First, we provide an interpretable lens for an existing model. We use a generative model which takes as input the representation in an existing (generative or discriminative) model, weakly supervised by limited side information. Applying a flexible and invertible transformation to the input leads to an interpretable representation with no loss in accuracy. We extend the approach using an active learning strategy to choose the most useful side information to obtain, allowing a human to guide what\" interpretable\" means. Our second framework relies on joint optimization for a representation which is both maximally informative about the side information and maximally compressive about the non-interpretable data factors. This leads to a novel perspective on the relationship between compression and regularization. We also propose a new interpretability evaluation metric based on our framework. Empirically, we achieve state-of-the-art results on three datasets using the two proposed algorithms.", "The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings": "We examine a class of embeddings based on structured random matrices with orthogonal rows which can be applied in many machine learning applications including dimensionality reduction and kernel approximation. For both the Johnson-Lindenstrauss transform and the angular kernel, we show that we can select matrices yielding guaranteed improved performance in accuracy and/or speed compared to earlier methods. We introduce matrices with complex entries which give significant further accuracy improvement. We provide geometric and Markov chain-based perspectives to help understand the benefits, and empirical results which suggest that the approach is helpful in a wider range of applications.", "One-network adversarial fairness": "There is currently a great expansion of the impact of machine learning algorithms on our lives, prompting the need for objectives other than pure performance, including fairness. Fairness here means that the outcome of an automated decisionmaking system should not discriminate between subgroups characterized by sensitive attributes such as gender or race. Given any existing differentiable classifier, we make only slight adjustments to the architecture including adding a new hidden layer, in order to enable the concurrent adversarial optimization for fairness and accuracy. Our framework provides one way to quantify the tradeoff between fairness and accuracy, while also leading to strong empirical performance.", "Structured prediction models for chord transcription of music audio": "Chord sequences are a compact and useful description of music, representing each beat or measure in terms of a likely distribution over individual notes without specifying the notes exactly. Transcribing music audio into chord sequences is essential for harmonic analysis, and would be an important component in content-based retrieval and indexing, but accuracy rates remain fairly low. In this paper, the existing 2008 LabROSA Supervised Chord Recognition System is modified by using different machine learning methods for decoding structural information, thereby achieving significantly superior results. Specifically, the hidden Markov model is replaced by a large margin structured prediction approach (SVMstruct) using an enlarged feature space. Performance is significantly improved by incorporating features from future (but not past) frames. The benefit of SVMstruct increases with the size of the training set, as \u2026", "Approximating the Bethe Partition Function": "When belief propagation (BP) converges, it does so to a stationary point of the Bethe free energy , and is often strikingly accurate. However, it may converge only to a local optimum or may not converge at all. An algorithm was recently introduced for attractive binary pairwise MRFs which is guaranteed to return an -approximation to the global minimum of in polynomial time provided the maximum degree , where is the number of variables. Here we significantly improve this algorithm and derive several results including a new approach based on analyzing first derivatives of , which leads to performance that is typically far superior and yields a fully polynomial-time approximation scheme (FPTAS) for attractive models without any degree restriction. Further, the method applies to general (non-attractive) models, though with no polynomial time guarantee in this case, leading to the important result that approximating of the Bethe partition function, , for a general model to additive -accuracy may be reduced to a discrete MAP inference problem. We explore an application to predicting equipment failure on an urban power network and demonstrate that the Bethe approximation can perform well even when BP fails to converge.", "Evaluating and Aggregating Feature-based Model Explanations": "A feature-based model explanation denotes how much each input feature contributes to a model's output for a given data point. As the number of proposed explanation functions grows, we lack quantitative evaluation criteria to help practitioners know when to use which explanation function. This paper proposes quantitative evaluation criteria for feature-based explanations: low sensitivity, high faithfulness, and low complexity. We devise a framework for aggregating explanation functions. We develop a procedure for learning an aggregate explanation function with lower complexity and then derive a new aggregate Shapley value explanation function that minimizes sensitivity.", "Understanding the Bethe Approximation: When and How can it go Wrong?": "Belief propagation is a remarkably effective tool for inference, even when applied to networks with cycles. It may be viewed as a way to seek the minimum of the Bethe free energy, though with no convergence guarantee in general. A variational perspective shows that, compared to exact inference, this minimization employs two forms of approximation:(i) the true entropy is approximated by the Bethe entropy, and (ii) the minimization is performed over a relaxation of the marginal polytope termed the local polytope. Here we explore when and how the Bethe approximation can fail for binary pairwise models by examining each aspect of the approximation, deriving results both analytically and with new experimental methods.", "Motivations and risks of machine ethics": "This paper surveys reasons for and against pursuing the field of machine ethics, understood as research aiming to build \u201cethical machines.\u201d We clarify the nature of this goal, why it is worth pursuing, and the risks involved in its pursuit. First, we survey and clarify some of the philosophical issues surrounding the concept of an \u201cethical machine\u201d and the aims of machine ethics. Second, we argue that while there are good prima facie reasons for pursuing machine ethics, including the potential to improve the ethical alignment of both humans and machines, there are also potential risks that must be considered. Third, we survey these potential risks and point to where research should be devoted to clarifying and managing potential risks. We conclude by making some recommendations about the questions that future work could address.", "Mapping intelligence: Requirements and possibilities": "New types of artificial intelligence (AI), from cognitive assistants to social robots, are challenging meaningful comparison with other kinds of intelligence. How can such intelligent systems be catalogued, evaluated, and contrasted, with representations and projections that offer meaningful insights? To catalyse the research in AI and the future of cognition, we present the motivation, requirements and possibilities for an atlas of intelligence: an integrated framework and collaborative open repository for collecting and exhibiting information of all kinds of intelligence, including humans, non-human animals, AI systems, hybrids and collectives thereof. After presenting this initiative, we review related efforts and present the requirements of such a framework. We survey existing visualisations and representations, and discuss which criteria of inclusion should be used to configure an atlas of intelligence.", "You shouldn\u2019t trust me: Learning models which conceal unfairness from multiple explanation methods": "Transparency of algorithmic systems has been discussed as a way for end-users and regulators to develop appropriate trust in machine learning models. One popular approach, LIME (Ribeiro, Singh, and Guestrin 2016), even suggests that model explanations can answer the question \u201cWhy should I trust you?\u201d. Here we show a straightforward method for modifying a pre-trained model to manipulate the output of many popular feature importance explanation methods with little change in accuracy, thus demonstrating the danger of trusting such explanation methods. We show how this explanation attack can mask a model\u2019s discriminatory use of a sensitive feature, raising strong concerns about using such explanation methods to check the fairness of a model.", "The Sensitivity of Counterfactual Fairness to Unmeasured Confounding": "Causal approaches to fairness have seen substantial recent interest, both from the machine learning community and from wider parties interested in ethical prediction algorithms. In no small part, this has been due to the fact that causal models allow one to simultaneously leverage data and expert knowledge to remove discriminatory effects from predictions. However, one of the primary assumptions in causal modeling is that you know the causal graph. This introduces a new opportunity for bias, caused by misspecifying the causal model. One common way for misspecification to occur is via unmeasured confounding: the true causal effect between variables is partially described by unobserved quantities. In this work we design tools to assess the sensitivity of fairness measures to this confounding for the popular class of non-linear additive noise models (ANMs). Specifically, we give a procedure for computing the maximum difference between two counterfactually fair predictors, where one has become biased due to confounding. For the case of bivariate confounding our technique can be swiftly computed via a sequence of closed-form updates. For multivariate confounding we give an algorithm that can be efficiently solved via automatic differentiation. We demonstrate our new sensitivity analysis tools in real-world fairness scenarios to assess the bias arising from confounding.", "Masked language modeling for proteins via linearly scalable long-context transformers": "Transformer models have achieved state-of-the-art results across a diverse range of domains. However, concern over the cost of training the attention mechanism to learn complex dependencies between distant inputs continues to grow. In response, solutions that exploit the structure and sparsity of the learned attention matrix have blossomed. However, real-world applications that involve long sequences, such as biological sequence analysis, may fall short of meeting these assumptions, precluding exploration of these models. To address this challenge, we present a new Transformer architecture, Performer, based on Fast Attention Via Orthogonal Random features (FAVOR). Our mechanism scales linearly rather than quadratically in the number of tokens in the sequence, is characterized by sub-quadratic space complexity and does not incorporate any sparsity pattern priors. Furthermore, it provides strong theoretical guarantees: unbiased estimation of the attention matrix and uniform convergence. It is also backwards-compatible with pre-trained regular Transformers. We demonstrate its effectiveness on the challenging task of protein sequence modeling and provide detailed theoretical analysis.", "Tightness of LP relaxations for almost balanced models": "Linear programming (LP) relaxations are widely used to attempt to identify a most likely configuration of a discrete graphical model. In some cases, the LP relaxation attains an optimum vertex at an integral location and thus guarantees an exact solution to the original optimization problem. When this occurs, we say that the LP relaxation is tight. Here we consider binary pairwise models and derive sufficient conditions for guaranteed tightness of (i) the standard LP relaxation on the local polytope LP+ LOC, and (ii) the LP relaxation on the triplet-consistent polytope LP+ TRI (the next level in the Sherali-Adams hierarchy). We provide simple new proofs of earlier results and derive significant novel results including that LP+ TRI is tight for any model where each block is balanced or almost balanced, and a decomposition theorem that may be used to break apart complex models into smaller pieces. An almost balanced (sub-) model is one that contains no frustrated cycles except through one privileged variable.", "Train and Test Tightness of LP Relaxations in Structured Prediction": "Structured prediction is used in areas such as computer vision and natural language processing to predict structured outputs such as segmentations or parse trees. In these settings, prediction is performed by MAP inference or, equivalently, by solving an integer linear program. Because of the complex scoring functions required to obtain accurate predictions, both learning and inference typically require the use of approximate solvers. We propose a theoretical explanation to the striking observation that approximations based on linear programming (LP) relaxations are often tight on real-world instances. In particular, we show that learning with LP relaxed inference encourages integrality of training instances, and that tightness generalizes from train to test data.", "Advancing COVID-19 diagnosis with privacy-preserving collaboration in artificial intelligence": "Artificial intelligence provides a promising solution for streamlining COVID-19 diagnoses; however, concerns surrounding security and trustworthiness impede the collection of large-scale representative medical data, posing a considerable challenge for training a well-generalized model in clinical practices. To address this, we launch the Unified CT-COVID AI Diagnostic Initiative (UCADI), where the artificial intelligence (AI) model can be distributedly trained and independently executed at each host institution under a federated learning framework without data sharing. Here we show that our federated learning framework model considerably outperformed all of the local models (with a test sensitivity/specificity of 0.973/0.951 in China and 0.730/0.942 in the United Kingdom), achieving comparable performance with a panel of professional radiologists. We further evaluated the model on the hold-out (collected from \u2026", "Getting a CLUE: A Method for Explaining Uncertainty Estimates": "Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the input's prediction. We validate CLUE through 1) a novel framework for evaluating counterfactual explanations of uncertainty, 2) a series of ablation experiments, and 3) a user study. Our experiments show that CLUE outperforms baselines and enables practitioners to better understand which input patterns are responsible for predictive uncertainty.", "Geometrically coupled Monte Carlo sampling": "Monte Carlo sampling in high-dimensional, low-sample settings is important in many machine learning tasks. We improve current methods for sampling in Euclidean spaces by avoiding independence, and instead consider ways to couple samples. We show fundamental connections to optimal transport theory, leading to novel sampling algorithms, and providing new theoretical grounding for existing strategies. We compare our new strategies against prior methods for improving sample efficiency, including quasi-Monte Carlo, by studying discrepancy. We explore our findings empirically, and observe benefits of our sampling schemes for reinforcement learning and generative modelling.", "\u201cExplaining\u201d machine learning reveals policy challenges": "The need to make objectives explicit may expose policy trade-offs that had previously been implicit and obscured", "The geometry of random features": "We present an in-depth examination of the effectiveness of radial basis function kernel (beyond Gaussian) estimators based on orthogonal random feature maps. We show that orthogonal estimators outperform state-of-the-art mechanisms that use iid sampling under weak conditions for tails of the associated Fourier distributions. We prove that for the case of many dimensions, the superiority of the orthogonal transform can be accurately measured by a property we define called the charm of the kernel, and that orthogonal random features provide optimal (in terms of mean squared error) kernel estimators. We provide the first theoretical results which explain why orthogonal random features outperform unstructured on downstream tasks such as kernel ridge regression by showing that orthogonal random features provide kernel algorithms with better spectral properties than the previous state-of-the-art. Our results enable practitioners more generally to estimate the benefits from applying orthogonal transforms.", "Clamping variables and approximate inference": "Background\nSudderth, Wainwright and Willsky (NIPS 2007) examined ZB vs Z for attractive binary pairwise models:", "Purple feed: Identifying high consensus news posts on social media": "Although diverse news stories are actively posted on social media, readers often focus on the news which reinforces their pre-existing views, leading to'filter bubble'effects. To combat this, some recent systems expose and nudge readers toward stories with different points of view. One example is the Wall Street Journal's' Blue Feed, Red Feed'system, which presents posts from biased publishers on each side of a topic. However, these systems have had limited success. We present a complementary approach which identifies high consensus' purple'posts that generate similar reactions from both'blue'and'red'readers. We define and operationalize consensus for news posts on Twitter in the context of US politics. We show that high consensus posts can be identified and discuss their empirical properties. We present a method for automatically identifying high and low consensus news posts on Twitter, which can work at \u2026", "Lost relatives of the Gumbel trick": "The Gumbel trick is a method to sample from a discrete probability distribution, or to estimate its normalizing partition function. The method relies on repeatedly applying a random perturbation to the distribution in a particular way, each time solving for the most likely configuration. We derive an entire family of related methods, of which the Gumbel trick is one member, and show that the new methods have superior properties in several settings with minimal additional computational cost. In particular, for the Gumbel trick to yield computational benefits for discrete graphical models, Gumbel perturbations on all configurations are typically replaced with so-called low-rank perturbations. We show how a subfamily of our new methods adapts to this setting, proving new upper and lower bounds on the log partition function and deriving a family of sequential samplers for the Gibbs distribution. Finally, we balance the discussion by showing how the simpler analytical form of the Gumbel trick enables additional theoretical results.", "An Empirical Study on Learning Fairness Metrics for COMPAS Data with Human Supervision": "The notion of individual fairness requires that similar people receive similar treatment. However, this is hard to achieve in practice since it is difficult to specify the appropriate similarity metric. In this work, we attempt to learn such similarity metric from human annotated data. We gather a new dataset of human judgments on a criminal recidivism prediction (COMPAS) task. By assuming the human supervision obeys the principle of individual fairness, we leverage prior work on metric learning, evaluate the performance of several metric learning methods on our dataset, and show that the learned metrics outperform the Euclidean and Precision metric under various criteria. We do not provide a way to directly learn a similarity metric satisfying the individual fairness, but to provide an empirical study on how to derive the similarity metric from human supervisors, then future work can use this as a tool to understand human supervision.", "Bethe Bounds and Approximating the Global Optimum": "Inference in general Markov random fields (MRFs) is NP-hard, though identifying the maximum a posteriori (MAP) configuration of pairwise MRFs with submodular cost functions is efficiently solvable using graph cuts. Marginal inference, however, even for this restricted class, is# P-hard. Restricting to binary pairwise models, we prove new formulations of derivatives of the Bethe free energy, provide bounds on the derivatives and bracket the locations of stationary points. Several results apply whether the model is associative or not. Applying these to discretized pseudo-marginals in the associative case, we present a polynomial time approximation scheme for global optimization of the Bethe free energy provided the maximum degree\u2206= O (\\log n), where n is the number of variables. Runtime is guaranteed O (\u03b5^-3/2 n^ 6 \u03a3^ 3/4 \u03a9^ 3/2), where \u03a3= O (\u2206/n) is the fraction of possible edges present and \u03a9is a function of MRF parameters. We examine use of the algorithm in practice, demonstrating runtime that is typically much faster, and discuss several extensions.", "The 2010 LABROSA chord recognition system": "For the MIREX 2010 Audio Chord Extraction task, we submitted a total of four systems. Our base system is a trainable chord recognizer based on two-band chroma representations and using a Structured SVM classifier to replace the more familiar hidden Markov model. We submit two versions of this system, one which transposes all training data through all 12 possible chords to maximize the training data available for each chord (and hence improve generalization to rarely-seen chords and keys), and one which simply trains on the chords in their original transposition, leading to a smaller model and possible learning of key-specific features. We also submit two pre-trained models, based on these two frameworks, trained in-house on the 180 Beatles and 20 Queen tracks for which groundtruth chord labels have been made available.", "Unifying Orthogonal Monte Carlo Methods": "Many machine learning methods making use of Monte Carlo sampling in vector spaces have been shown to be improved by conditioning samples to be mutually orthogonal. Exact orthogonal coupling of samples is computationally intensive, hence approximate methods have been of great interest. In this paper, we present a unifying perspective of many approximate methods by considering Givens transformations, propose new approximate methods based on this framework, and demonstrate the first statistical guarantees for families of approximate methods in kernel approximation. We provide extensive empirical evaluations with guidance for practitioners.", "Machine Learning Explainability for External Stakeholders": "As machine learning is increasingly deployed in high-stakes contexts affecting people's livelihoods, there have been growing calls to open the black box and to make machine learning algorithms more explainable. Providing useful explanations requires careful consideration of the needs of stakeholders, including end-users, regulators, and domain experts. Despite this need, little work has been done to facilitate inter-stakeholder conversation around explainable machine learning. To help address this gap, we conducted a closed-door, day-long workshop between academics, industry experts, legal scholars, and policymakers to develop a shared language around explainability and to understand the current shortcomings of and potential solutions for deploying explainable machine learning in service of transparency goals. We also asked participants to share case studies in deploying explainable machine learning at scale. In this paper, we provide a short summary of various case studies of explainable machine learning, lessons from those studies, and discuss open challenges.", "Orthogonal Estimation of Wasserstein Distances": "Wasserstein distances are increasingly used in a wide variety of applications in machine learning. Sliced Wasserstein distances form an important subclass which may be estimated efficiently through one-dimensional sorting operations. In this paper, we propose a new variant of sliced Wasserstein distance, study the use of orthogonal coupling in Monte Carlo estimation of Wasserstein distances and draw connections with stratified sampling, and evaluate our approaches experimentally in a range of large-scale experiments in generative modelling and reinforcement learning.", "On Fairness, Diversity and Randomness in Algorithmic Decision Making": "Consider a binary decision making process where a single machine learning classifier replaces a multitude of humans. We raise questions about the resulting loss of diversity in the decision making process. We study the potential benefits of using random classifier ensembles instead of a single classifier in the context of fairness-aware learning and demonstrate various attractive properties: (i) an ensemble of fair classifiers is guaranteed to be fair, for several different measures of fairness, (ii) an ensemble of unfair classifiers can still achieve fair outcomes, and (iii) an ensemble of classifiers can achieve better accuracy-fairness trade-offs than a single classifier. Finally, we introduce notions of distributional fairness to characterize further potential benefits of random classifier ensembles.", "Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty": "Algorithmic transparency entails exposing system properties to various stakeholders for purposes that include understanding, improving, and contesting predictions. Until now, most research into algorithmic transparency has predominantly focused on explainability. Explainability attempts to provide reasons for a machine learning model's behavior to stakeholders. However, understanding a model's specific behavior alone might not be enough for stakeholders to gauge whether the model is wrong or lacks sufficient knowledge to solve the task at hand. In this paper, we argue for considering a complementary form of transparency by estimating and communicating the uncertainty associated with model predictions. First, we discuss methods for assessing uncertainty. Then, we characterize how uncertainty can be used to mitigate model unfairness, augment decision-making, and build trustworthy systems. Finally, we \u2026", "Clamping Improves TRW and Mean Field Approximations": "We examine the effect of clamping variables for approximate inference in undirected graphical models with pairwise relationships and discrete variables. For any number of variable labels, we demonstrate that clamping and summing approximate sub-partition functions can lead only to a decrease in the partition function estimate for TRW, and an increase for the naive mean field method, in each case guaranteeing an improvement in the approximation and bound. We next focus on binary variables, add the Bethe approximation to consideration and examine ways to choose good variables to clamp, introducing new methods. We show the importance of identifying highly frustrated cycles, and of checking the singleton entropy of a variable. We explore the value of our methods by empirical analysis and draw lessons to guide practitioners.", "Bethe and Related Pairwise Entropy Approximations": "For undirected graphical models, belief propagation often performs remarkably well for approximate marginal inference, and may be viewed as a heuristic to minimize the Bethe free energy. Focusing on binary pairwise models, we demonstrate that several recent results on the Bethe approximation may be generalized to a broad family of related pairwise free energy approximations with arbitrary counting numbers. We explore approximation error and shed light on the empirical success of the Bethe approximation.", "Characterizing tightness of LP relaxations by forbidding signed minors": "We consider binary pairwise graphical models and provide an exact characterization (necessary and sufficient conditions observing signs of potentials) of tightness for the LP relaxation on the triplet-consistent polytope of the MAP inference problem, by forbidding an odd-K (complete graph on 5 variables with all edges repulsive) as a signed minor in the signed suspension graph. This captures signs of both singleton and edge potentials in a compact and efficiently testable condition, and improves significantly on earlier results. We provide other results on tightness of LP relaxations by forbidding minors, draw connections and suggest paths for future research.", "On MAP inference by MWSS on Perfect Graphs": "Finding the most likely (MAP) configuration of a Markov random field (MRF) is NP-hard in general. A promising, recent technique is to reduce the problem to finding a maximum weight stable set (MWSS) on a derived weighted graph, which if perfect, allows inference in polynomial time. We derive new results for this approach, including a general decomposition theorem for MRFs of any order and number of labels, extensions of results for binary pairwise models with submodular cost functions to higher order, and an exact characterization of which binary pairwise MRFs can be efficiently solved with this method. This defines the power of the approach on this class of models, improves our toolbox and expands the range of tractable models.", "Human-Centered Approaches to Fair and Responsible AI": "As AI changes the way decisions are made in organizations and governments, it is ever more important to ensure that these systems work according to values that diverse users and groups find important. Researchers have proposed numerous algorithmic techniques to formalize statistical fairness notions, but emerging work suggests that AI systems must account for the real-world contexts in which they will be embedded in order to actually work fairly. These findings call for an expanded research focus beyond statistical fairness to that which includes fundamental understandings of human use and the social impact of AI systems, a theme central to the HCI community. The HCI community can contribute novel understandings, methods, and techniques for incorporating human values and cultural norms into AI systems; address human biases in developing and using AI; and empower individual users and society to \u2026", "Orthogonal over-parameterized training": "The inductive bias of a neural network is largely determined by the architecture and the training algorithm. To achieve good generalization, how to effectively train a neural network is of great importance. We propose a novel orthogonal over-parameterized training (OPT) framework that can provably minimize the hyperspherical energy which characterizes the diversity of neurons on a hypersphere. By maintaining the minimum hyperspherical energy during training, OPT can greatly improve the empirical generalization. Specifically, OPT fixes the randomly initialized weights of the neurons and learns an orthogonal transformation that applies to these neurons. We consider multiple ways to learn such an orthogonal transformation, including unrolling orthogonalization algorithms, applying orthogonal parameterization, and designing orthogonality-preserving gradient descent. For better scalability, we propose the stochastic OPT which performs orthogonal transformation stochastically for partial dimensions of neurons. Interestingly, OPT reveals that learning a proper coordinate system for neurons is crucial to generalization. We provide some insights on why OPT yields better generalization. Extensive experiments validate the superiority of OPT over the standard training.", "Revisiting the Limits of MAP Inference by MWSS on Perfect Graphs": "A recent, promising approach to identifying a configuration of a discrete graphical model with highest probability (termed MAP inference) is to reduce the problem to finding a maximum weight stable set (MWSS) in a derived weighted graph, which, if perfect, allows a solution to be found in polynomial time. Weller and Jebara (2013) investigated the class of binary pairwise models where this method may be applied. However, their analysis made a seemingly innocuous assumption which simplifies analysis but led to only a subset of possible reparameterizations being considered. Here we introduce novel techniques and consider all cases, demonstrating that this greatly expands the set of tractable models. We provide a simple, exact characterization of the new, enlarged set and show how such models may be efficiently identified, thus settling the power of the approach on this class.", "Fair Enough: Improving Fairness in Budget-Constrained Decision Making Using Confidence Thresholds.": "Increasing concern about discrimination and bias in datadriven decision making systems has led to a growth in academic and popular interest in algorithmic fairness. Prior work on fairness in machine learning has focused primarily on the setting in which all the information (features) needed to make a confident decision about an individual is readily available. In practice, however, many applications allow further information to be acquired at a feature-specific cost. For example, when diagnosing a patient, the doctor starts with only a handful of symptoms but progressively improves the diagnosis by acquiring additional information before making a final decision. We show that we can achieve fairness by leveraging a natural affordance of this setting: the decision on when to stop acquiring more features and proceeding to predict. First, we show that by setting a single set of confidence thresholds for stopping, we can attain equal error rates across arbitrary groups. Second, we extend the framework to a set of group-specific confidence thresholds which ensure that a classifier achieves equal opportunity (equal falsepositive or false-negative rates). The confidence thresholds naturally achieve fairness by redistributing the budget across individuals. This leads to statistical fairness across groups but also addresses the limitation that current statistical fairness methods fail to provide any guarantees to individuals. Finally, using two public datasets, we confirm the effectiveness of our methods empirically and investigate the limitations.", "Conditions Beyond Treewidth for Tightness of Higher-order LP Relaxations": "Linear programming (LP) relaxations are a popular method to attempt to find a most likely configuration of a discrete graphical model. If a solution to the relaxed problem is obtained at an integral vertex then the solution is guaranteed to be exact and we say that the relaxation is tight. We consider binary pairwise models and introduce new methods which allow us to demonstrate refined conditions for tightness of LP relaxations in the Sherali-Adams hierarchy. Our results include showing that for higher order LP relaxations, treewidth is not precisely the right way to characterize tightness. This work is primarily theoretical, with insights that can improve efficiency in practice.", "Uprooting and rerooting graphical models": "We show how any binary pairwise model may be \u201cuprooted\u201d to a fully symmetric model, wherein original singleton potentials are transformed to potentials on edges to an added variable, and then \u201crerooted\u201d to a new model on the original number of variables. The new model is essentially equivalent to the original model, with the same partition function and allowing recovery of the original marginals or a MAP configuration, yet may have very different computational properties that allow much more efficient inference. This meta-approach deepens our understanding, may be applied to any existing algorithm to yield improved methods in practice, generalizes earlier theoretical results, and reveals a remarkable interpretation of the triplet-consistent polytope.", "Now You See Me (CME): Concept-based Model Extraction": "Deep Neural Networks (DNNs) have achieved remarkable performance on a range of tasks. A key step to further empowering DNN-based approaches is improving their explainability. In this work we present CME: a concept-based model extraction framework, used for analysing DNN models via concept-based extracted models. Using two case studies (dSprites, and Caltech UCSD Birds), we demonstrate how CME can be used to (i) analyse the concept information learned by a DNN model (ii) analyse how a DNN uses this concept information when predicting output labels (iii) identify key concept information that can further improve DNN predictive performance (for one of the case studies, we showed how model accuracy can be improved by over 14%, using only 30% of the available concepts).", "On the Fairness of Causal Algorithmic Recourse": "Many recent works have studied the problem of algorithmic fairness from the perspective of predictions. Instead, here we investigate the fairness of recourse actions recommended to individuals to recover from an unfavourable classification. We propose two new fairness criteria at the group and individual level which -- unlike prior work on equalising the average distance from the decision boundary across protected groups -- explicitly account for the causal relationships between input features, thereby allowing us to capture downstream effects of recourse actions performed in the physical world. We explore how our criteria relate to others, such as counterfactual fairness, and show that fairness of recourse (both causal and non-causal) is complementary to fairness of prediction. We then investigate how to enforce fair causal recourse in the training of a classifier. Finally, we discuss whether fairness violations in the data generating process revealed by our criteria may be better addressed by societal interventions as opposed to constraints on the classifier.", "Ode to an ODE": "We present a new paradigm for Neural ODE algorithms, called ODEtoODE, where time-dependent parameters of the main flow evolve according to a matrix flow on the orthogonal group O(d). This nested system of two flows, where the parameter-flow is constrained to lie on the compact manifold, provides stability and effectiveness of training and provably solves the gradient vanishing-explosion problem which is intrinsically related to training deep neural network architectures such as Neural ODEs. Consequently, it leads to better downstream models, as we show on the example of training reinforcement learning policies with evolution strategies, and in the supervised learning setting, by comparing with previous SOTA baselines. We provide strong convergence results for our proposed mechanism that are independent of the depth of the network, supporting our empirical studies. Our results show an intriguing connection between the theory of deep neural networks and the field of matrix flows on compact manifolds.", "Time Dependence in Non-Autonomous Neural ODEs": "Neural Ordinary Differential Equations (ODEs) are elegant reinterpretations of deep networks where continuous time can replace the discrete notion of depth, ODE solvers perform forward propagation, and the adjoint method enables efficient, constant memory backpropagation. Neural ODEs are universal approximators only when they are non-autonomous, that is, the dynamics depends explicitly on time. We propose a novel family of Neural ODEs with time-varying weights, where time-dependence is non-parametric, and the smoothness of weight trajectories can be explicitly controlled to allow a tradeoff between expressiveness and efficiency. Using this enhanced expressiveness, we outperform previous Neural ODE variants in both speed and representational capacity, ultimately outperforming standard ResNet and CNN models on select image classification and video prediction tasks.", "Sub-linear memory: How to make performers SLiM": "The Transformer architecture has revolutionized deep learning on sequential data, becoming ubiquitous in state-of-the-art solutions for a wide variety of applications. Yet vanilla Transformers are notoriously resource-expensive, requiring in serial time and memory as functions of input length . Recent works proposed various linear self-attention mechanisms, scaling only as for serial computation. We perform a thorough analysis of recent Transformer mechanisms with linear self-attention, Performers, in terms of overall computational complexity. We observe a remarkable computational flexibility: forward and backward propagation can be performed with no approximations using sublinear memory as a function of (in addition to negligible storage for the input sequence), at a cost of greater time complexity in the parallel setting. In the extreme case, a Performer consumes only memory during training, and still requires time. This discovered time-memory tradeoff can be used for training or, due to complete backward-compatibility, for fine-tuning on a low-memory device, e.g. a smartphone or an earlier-generation GPU, thus contributing towards decentralized and democratized deep learning.", "Learning with Hyperspherical Uniformity": "Due to the over-parameterization nature, neural networks are a powerful tool for nonlinear function approximation. In order to achieve good generalization on unseen data, a suitable inductive bias is of great importance for neural networks. One of the most straightforward ways is to regularize the neural network with some additional objectives. L2 regularization serves as a standard regularization for neural networks. Despite its popularity, it essentially regularizes one dimension of the individual neuron, which is not strong enough to control the capacity of highly over-parameterized neural networks. Motivated by this, hyperspherical uniformity is proposed as a novel family of relational regularizations that impact the interaction among neurons. We consider several geometrically distinct ways to achieve hyperspherical uniformity. The effectiveness of hyperspherical uniformity is justified by theoretical insights and empirical evaluations.", "Improving Interpretability in Medical Imaging Diagnosis using Adversarial Training": "We investigate the influence of adversarial training on the interpretability of convolutional neural networks (CNNs), specifically applied to diagnosing skin cancer. We show that gradient-based saliency maps of adversarially trained CNNs are significantly sharper and more visually coherent than those of standardly trained CNNs. Furthermore, we show that adversarially trained networks highlight regions with significant color variation within the lesion, a common characteristic of melanoma. We find that fine-tuning a robust network with a small learning rate further improves saliency maps' sharpness. Lastly, we provide preliminary work suggesting that robustifying the first layers to extract robust low-level features leads to visually coherent explanations.", "Adversarial Graph Embeddings for Fair Influence Maximization over Social Networks": "Influence maximization is a widely studied topic in network science, where the aim is to reach the maximum possible number of nodes, while only targeting a small initial set of individuals. It has critical applications in many fields, including viral marketing, information propagation, news dissemination, and vaccinations. However, the objective does not usually take into account whether the final set of influenced nodes is fair with respect to sensitive attributes, such as race or gender. Here we address fair influence maximization, aiming to reach minorities more equitably. We introduce Adversarial Graph Embeddings: we co-train an auto-encoder for graph embedding and a discriminator to discern sensitive attributes. This leads to embeddings which are similarly distributed across sensitive attributes. We then find a good initial set by clustering the embeddings. We believe we are the first to use embeddings for the task of fair influence maximization. While there are typically trade-offs between fairness and influence maximization objectives, our experiments on synthetic and real-world datasets show that our approach dramatically reduces disparity while remaining competitive with state-of-the-art influence maximization methods.", "Dimensions of Diversity in Human Perceptions of Algorithmic Fairness": "Algorithms are increasingly involved in making decisions that affect human lives. Prior work has explored how people believe algorithmic decisions should be made, but there is little understanding of which individual factors relate to variance in these beliefs across people. As an increasing emphasis is put on oversight boards and regulatory bodies, it is important to understand the biases that may affect human judgements about the fairness of algorithms. Building on factors found in moral foundations theory and egocentric fairness literature, we explore how people's perceptions of fairness relate to their (i) demographics (age, race, gender, political view), and (ii) personal experiences with the algorithmic task being evaluated. Specifically, we study human beliefs about the fairness of using different features in an algorithm designed to assist judges in making decisions about granting bail. Our analysis suggests that political views and certain demographic factors, such as age and gender, exhibit a significant relation to people's beliefs about fairness. Additionally, we find that people beliefs about the fairness of using demographic features such as age, gender and race, for making bail decisions about others, vary egocentrically: that is they vary depending on their own age, gender and race respectively.", "Gauged Mini-Bucket Elimination for Approximate Inference": "Computing the partition function Z of a discrete graphical model is a fundamental inference challenge. Since this is computationally intractable, variational approximations are often used in practice. Recently, so-called gauge transformations were used to improve variational lower bounds on Z. In this paper, we propose a new gauge-variational approach, termed WMBE-G, which combines gauge transformations with the weighted mini-bucket elimination (WMBE) method. WMBE-G can provide both upper and lower bounds on Z, and is easier to optimize than the prior gauge-variational algorithm. We show that WMBE-G strictly improves the earlier WMBE approximation for symmetric models including Ising models with no magnetic field. Our experimental results demonstrate the effectiveness of WMBE-G even for generic, nonsymmetric models.", "UFO-BLO: Unbiased First-Order Bilevel Optimization": "Bilevel optimization (BLO) is a popular approach with many applications including hyperparameter optimization, neural architecture search, adversarial robustness and model-agnostic meta-learning. However, the approach suffers from time and memory complexity proportional to the length of its inner optimization loop, which has led to several modifications being proposed. One such modification is \\textit{first-order} BLO (FO-BLO) which approximates outer-level gradients by zeroing out second derivative terms, yielding significant speed gains and requiring only constant memory as varies. Despite FO-BLO's popularity, there is a lack of theoretical understanding of its convergence properties. We make progress by demonstrating a rich family of examples where FO-BLO-based stochastic optimization does not converge to a stationary point of the BLO objective. We address this concern by proposing a new FO-BLO-based unbiased estimate of outer-level gradients, enabling us to theoretically guarantee this convergence, with no harm to memory and expected time complexity. Our findings are supported by experimental results on Omniglot and Mini-ImageNet, popular few-shot meta-learning benchmarks.", "Stochastic Flows and Geometric Optimization on the Orthogonal Group": "We present a new class of stochastic, geometrically-driven optimization algorithms on the orthogonal group O (d) and naturally reductive homogeneous manifolds obtained from the action of the rotation group SO (d). We theoretically and experimentally demonstrate that our methods can be applied in various fields of machine learning including deep, convolutional and recurrent neural networks, reinforcement learning, normalizing flows and metric learning. We show an intriguing connection between efficient stochastic optimization on the orthogonal group and graph theory (eg matching problem, partition functions over graphs, graph-coloring). We leverage the theory of Lie groups and provide theoretical results for the designed class of algorithms. We demonstrate broad applicability of our methods by showing strong performance on the seemingly unrelated tasks of learning world models to obtain stable policies for the most difficult Humanoid agent from OpenAI Gym and improving convolutional neural networks.", "Optimal experimental design via Bayesian optimization: active causal structure learning for Gaussian process networks": "We study the problem of causal discovery through targeted interventions. Starting from few observational measurements, we follow a Bayesian active learning approach to perform those experiments which, in expectation with respect to the current model, are maximally informative about the underlying causal structure. Unlike previous work, we consider the setting of continuous random variables with non-linear functional relationships, modelled with Gaussian process priors. To address the arising problem of choosing from an uncountable set of possible interventions, we propose to use Bayesian optimisation to efficiently maximise a Monte Carlo estimate of the expected information gain.", "Leader Stochastic Gradient Descent for Distributed Training of Deep Learning Models": "We consider distributed optimization under communication constraints for training deep learning models. We propose a new algorithm, whose parameter updates rely on two forces: a regular gradient step, and a corrective direction dictated by the currently best-performing worker (leader). Our method differs from the parameter-averaging scheme EASGD in a number of ways:(i) our objective formulation does not change the location of stationary points compared to the original optimization problem;(ii) we avoid convergence decelerations caused by pulling local workers descending to different local minima to each other (ie to the average of their parameters);(iii) our update by design breaks the curse of symmetry (the phenomenon of being trapped in poorly generalizing sub-optimal solutions in symmetric non-convex landscapes); and (iv) our approach is more communication efficient since it broadcasts only parameters of the leader rather than all workers. We provide theoretical analysis of the batch version of the proposed algorithm, which we call Leader Gradient Descent (LGD), and its stochastic variant (LSGD). Finally, we implement an asynchronous version of our algorithm and extend it to the multi-leader setting, where we form groups of workers, each represented by its own local leader (the best performer in a group), and update each worker with a corrective direction comprised of two attractive forces: one to the local, and one to the global leader (the best performer among all workers). The multi-leader setting is well-aligned with current hardware architecture, where local workers forming a group lie within a single computational node and \u2026", "Proceedings of the 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017)": "This is the Proceedings of the 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017), which was held in Sydney, Australia, August 10, 2017. Invited speakers were Tony Jebara, Pang Wei Koh, and David Sontag.", "On the Expressive Power of Self-Attention Matrices": "Transformer networks are able to capture patterns in data coming from many domains (text, images, videos, proteins, etc.) with little or no change to architecture components. We perform a theoretical analysis of the core component responsible for signal propagation between elements, i.e. the self-attention matrix. In practice, this matrix typically exhibits two properties: (1) it is sparse, meaning that each token only attends to a small subset of other tokens; and (2) it changes dynamically depending on the input to the module. With these considerations in mind, we ask the following question: Can a fixed self-attention module approximate arbitrary sparse patterns depending on the input? How small is the hidden size required for such approximation? We make progress in answering this question and show that the self-attention matrix can provably approximate sparse matrices, where sparsity is in terms of a bounded number of nonzero elements in each row and column. While the parameters of self-attention are fixed, various sparse matrices can be approximated by only modifying the inputs. Our proof is based on the random projection technique and uses the seminal Johnson-Lindenstrauss lemma. Our proof is constructive, enabling us to propose an algorithm for finding adaptive inputs and fixed self-attention parameters in order to approximate a given matrix. In particular, we show that, in order to approximate any sparse matrix up to a given precision defined in terms of preserving matrix element ratios, grows only logarithmically with the sequence length (i.e. ).", "Do Concept Bottleneck Models Learn as Intended?": "Concept bottleneck models map from raw inputs to concepts, and then from concepts to targets. Such models aim to incorporate pre-specified, high-level concepts into the learning procedure, and have been motivated to meet three desiderata: interpretability, predictability, and intervenability. However, we find that concept bottleneck models struggle to meet these goals. Using post hoc interpretability methods, we demonstrate that concepts do not correspond to anything semantically meaningful in input space, thus calling into question the usefulness of concept bottleneck models in their current form.", "DADI: Dynamic Discovery of Fair Information with Adversarial Reinforcement Learning": "We introduce a framework for dynamic adversarial discovery of information (DADI), motivated by a scenario where information (a feature set) is used by third parties with unknown objectives. We train a reinforcement learning agent to sequentially acquire a subset of the information while balancing accuracy and fairness of predictors downstream. Based on the set of already acquired features, the agent decides dynamically to either collect more information from the set of available features or to stop and predict using the information that is currently available. Building on previous work exploring adversarial representation learning, we attain group fairness (demographic parity) by rewarding the agent with the adversary's loss, computed over the final feature set. Importantly, however, the framework provides a more general starting point for fair or private dynamic information discovery. Finally, we demonstrate empirically, using two real-world datasets, that we can trade-off fairness and predictive performance", "Exploring Properties of the Deep Image Prior": "The Deep Image Prior (DIP, Ulyanov et al., 2017) is a fascinating recent approach for recovering images which appear natural, yet is not fully understood. This work aims at shedding some further light on this approach by investigating the properties of the early outputs of the DIP. First, we show that these early iterations demonstrate invariance to adversarial perturbations by classifying progressive DIP outputs and using a novel saliency map approach. Next we explore using DIP as a defence against adversaries, showing good potential. Finally, we examine the adversarial invariancy of the early DIP outputs, and hypothesize that these outputs may remove non-robust image features. By comparing classification confidence values we show some evidence confirming this hypothesis.", "Self-Guided Belief Propagation--A Homotopy Continuation Method": "Belief propagation (BP) is a popular method for performing probabilistic inference on graphical models. In this work, we enhance BP and propose self-guided belief propagation (SBP) that incorporates the pairwise potentials only gradually. This homotopy continuation method converges to a unique solution and increases the accuracy without increasing the computational burden. We provide a formal analysis to demonstrate that SBP finds the global optimum of the Bethe approximation for attractive models where all variables favor the same state. Moreover, we apply SBP to various graphs with random potentials and empirically show that: (i) SBP is superior in terms of accuracy whenever BP converges, and (ii) SBP obtains a unique, stable, and accurate solution whenever BP does not converge.", "Bucket Renormalization for Approximate Inference": "Probabilistic graphical models are a key tool in machine learning applications. Computing the partition function, ie, normalizing constant, is a fundamental task of statistical inference but is generally computationally intractable, leading to extensive study of approximation methods. Iterative variational methods are a popular and successful family of approaches. However, even state of the art variational methods can return poor results or fail to converge on difficult instances. In this paper, we instead consider computing the partition function via sequential summation over variables. We develop robust approximate algorithms by combining ideas from mini-bucket elimination with tensor network and renormalization group methods from statistical physics. The resulting \u201cconvergence-free\u201d methods show good empirical performance on both synthetic and real-world benchmark models, even for difficult instances.", "Bounding the Integrality Distance of LP Relaxations for Structured Prediction": "In structured prediction, a predictor optimizes an objective function over a combinatorial search space, such as the set of all image segmentations, or the set of all part-of-speech taggings. Unfortunately, finding the optimal structured labeling\u2014sometimes referred to as maximum a posteriori (MAP) inference\u2014is, in general, NP-hard [12], due to the combinatorial structure of the problem. Many inference approximations have been proposed, some of which are based on linear programming (LP) relaxations [eg, 5, 1, 15, 16, 4, 13], which \u201crelax\u201d the combinatorial search space to a convex polytope with a polynomial number of constraints. These LP relaxations can be solved efficiently, but may result in fractional solutions, ie, non-integral labelings. The approximation quality of an LP relaxation is traditionally measured by a quantity known as the integrality gap, defined as the difference in the objective values obtained at the optima of the relaxed and exact problems. When the integrality gap is zero, the solution is said to be tight.\nWhile studying the integrality gap is useful from an optimization perspective, it is arguably less important in structured prediction, wherein the solution to the optimization, the inferred labeling, is more important than its objective value. We do not really care whether the optimum of the relaxed problem equals that of the integral one; we just want relaxed inference to yield the optimal integral assignment\u2014or, lacking that, an assignment that is \u201cclose to\u201d the optimal integral one. If we assume that the relaxed problem has a unique solution, then tightness implies that the assignments are the same. However, lacking this assumption, there \u2026", "Methods for Inference in Graphical Models": "Graphical models provide a flexible, powerful and compact way to model relationships between random variables, and have been applied with great success in many domains.", "DIVINE: Diverse Influential Training Points for Data Visualization and Model Refinement": "As the complexity of machine learning (ML) models increases, resulting in a lack of prediction explainability, several methods have been developed to explain a model's behavior in terms of the training data points that most influence the model. However, these methods tend to mark outliers as highly influential points, limiting the insights that practitioners can draw from points that are not representative of the training data. In this work, we take a step towards finding influential training points that also represent the training data well. We first review methods for assigning importance scores to training points. Given importance scores, we propose a method to select a set of DIVerse INfluEntial (DIVINE) training points as a useful explanation of model behavior. As practitioners might not only be interested in finding data points influential with respect to model accuracy, but also with respect to other important metrics, we show how to evaluate training data points on the basis of group fairness. Our method can identify unfairness-inducing training points, which can be removed to improve fairness outcomes. Our quantitative experiments and user studies show that visualizing DIVINE points helps practitioners understand and explain model behavior better than earlier approaches.", "Is Disentanglement all you need? Comparing Concept-based & Disentanglement Approaches": "Concept-based explanations have emerged as a popular way of extracting human-interpretable representations from deep discriminative models. At the same time, the disentanglement learning literature has focused on extracting similar representations in an unsupervised or weakly-supervised way, using deep generative models. Despite the overlapping goals and potential synergies, to our knowledge, there has not yet been a systematic comparison of the limitations and trade-offs between concept-based explanations and disentanglement approaches. In this paper, we give an overview of these fields, comparing and contrasting their properties and behaviours on a diverse set of tasks, and highlighting their potential strengths and limitations. In particular, we demonstrate that state-of-the-art approaches from both classes can be data inefficient, sensitive to the specific nature of the classification/regression task, or sensitive to the employed concept representation.", "-CLUE: Diverse Sets of Explanations for Uncertainty Estimates": "To interpret uncertainty estimates from differentiable probabilistic models, recent work has proposed generating Counterfactual Latent Uncertainty Explanations (CLUEs). However, for a single input, such approaches could output a variety of explanations due to the lack of constraints placed on the explanation. Here we augment the original CLUE approach, to provide what we call -CLUE. CLUE indicates way to change an input, while remaining on the data manifold, such that the model becomes more confident about its prediction. We instead return a of plausible CLUEs: multiple, diverse inputs that are within a ball of the original input in latent space, all yielding confident predictions.", "CWY Parametrization: a Solution for Parallelized Optimization of Orthogonal and Stiefel Matrices": "We introduce an efficient approach for optimization over orthogonal groups on highly parallel computation units such as GPUs or TPUs. As in earlier work, we parametrize an orthogonal matrix as a product of Householder reflections. However, to overcome low parallelization capabilities of computing Householder reflections sequentially, we propose employing an accumulation scheme called the compact WY (or CWY) transform\u2013a compact parallelization-friendly matrix representation for the series of Householder reflections. We further develop a novel Truncated CWY (or T-CWY) approach for Stiefel manifold parametrization which has a competitive complexity and, again, yields benefits when computed on GPUs and TPUs. We prove that our CWY and T-CWY methods lead to convergence to a stationary point of the training objective when coupled with stochastic gradient descent. We apply our methods to train recurrent neural network architectures in the tasks of neural machine translation and video prediction.", "Unlocking Pixels for Reinforcement Learning via Implicit Attention": "There has recently been significant interest in training reinforcement learning (RL) agents in vision-based environments. This poses many challenges, such as high dimensionality and potential for observational overfitting through spurious correlations. A promising approach to solve both of these problems is a self-attention bottleneck, which provides a simple and effective framework for learning high performing policies, even in the presence of distractions. However, due to poor scalability of attention architectures, these methods do not scale beyond low resolution visual inputs, using large patches (thus small attention matrices). In this paper we make use of new efficient attention algorithms, recently shown to be highly effective for Transformers, and demonstrate that these new techniques can be applied in the RL setting. This allows our attention-based controllers to scale to larger visual inputs, and facilitate the use of smaller patches, even individual pixels, improving generalization. In addition, we propose a new efficient algorithm approximating softmax attention with what we call hybrid random features, leveraging the theory of angular kernels. We show theoretically and empirically that hybrid random features is a promising approach when using attention for vision-based RL.", "COUNTERFACTUAL ACCURACIES FOR ALTERNATIVE MODELS": "Typically we fit a model by optimizing performance on training data. Here we focus on the case of a binary classifier that predicts \u2018yes\u2019 or \u2018no\u2019for any given test point. We explore a notion of confidence in a particular prediction by asking: If we were to fit an alternative classifier from our model class to the same training data, how much training accuracy would we have to give up so that the prediction for the test point would change?", "HOW IS \u2018TRANSPARENCY\u2019 UNDERSTOOD BY LEGAL SCHOLARS AND THE MACHINE LEARNING COMMUNITY?": "Algorithmic decision-making systems are increasingly in use yet often lack transparency. The opacity of these \u2018black boxes\u2019 leads to decisions that can be hard to understand and contest, creating substantial risks of injustice. ML researchers are developing methods for improving the transparency of these systems (\u2018explainable AI\u2019). Unless this \u2018challenge of transparency\u2019(Weller 2017) can be addressed appropriately, alongside concerns including reliability,\u2018fairness\u2019 and \u2018algorithmic accountability\u2019, the public is unlikely to trust these systems (RSA 2018), despite their many benefits.", "Uprooting and rerooting higher-order graphical models": "The idea of uprooting and rerooting graphical models was introduced specifically for binary pairwise models by Weller [19] as a way to transform a model to any of a whole equivalence class of related models, such that inference on any one model yields inference results for all others. This is very helpful since inference, or relevant bounds, may be much easier to obtain or more accurate for some model in the class. Here we introduce methods to extend the approach to models with higher-order potentials and develop theoretical insights. In particular, we show that the triplet-consistent polytope TRI is unique in being \u2018universally rooted\u2019. We demonstrate empirically that rerooting can significantly improve accuracy of methods of inference for higher-order models at negligible computational cost.", "Network Ranking With Bethe Pseudomarginals": "Network structure often contains information that can be useful for ranking algorithms. We incorporate network structure by formulating ranking as marginal inference in a Markov random field (MRF). Though inference is generally NP-hard, we apply a recently-developed polynomial-time approximation scheme (PTAS) to infer Bethe pseudomarginals. As a case study, we investigate the problem of ranking failing transformers that are physically connected in a network. Compared to independent score-based ranking, the current state of the art, we show superior ranking results. We conclude by discussing an empirical phenomenon of critical parameter regions, with implications for new algorithms.", "Iterative teaching by label synthesis": "In this paper, we consider the problem of iterative machine teaching, where a teacher provides examples sequentially based on the current iterative learner. In contrast to previous methods that have to scan over the entire pool and select teaching examples from it in each iteration, we propose a label synthesis teaching framework where the teacher randomly selects input teaching examples (eg, images) and then synthesizes suitable outputs (eg, labels) for them. We show that this framework can avoid costly example selection while still provably achieving exponential teachability. We propose multiple novel teaching algorithms in this framework. Finally, we empirically demonstrate the value of our framework.", "Robust inverse reinforcement learning under transition dynamics mismatch": "We study the inverse reinforcement learning (IRL) problem under a transition dynamics mismatch between the expert and the learner. Specifically, we consider the Maximum Causal Entropy (MCE) IRL learner model and provide a tight upper bound on the learner's performance degradation based on the -distance between the two transition dynamics of the expert and the learner. Then, by leveraging insights from the Robust RL literature, we propose a robust MCE IRL algorithm, which is a principled approach to help with this mismatch. Finally, we empirically demonstrate the stable performance of our algorithm compared to the standard MCE IRL algorithm under transition mismatches in finite MDP problems.", "Machine Learning and the Meaning of Equal Treatment": "Approaches to non-discrimination are generally informed by two principles: striving for equality of treatment, and advancing various notions of equality of outcome. We consider when and why there are trade-offs in machine learning between respecting formalistic interpretations of equal treatment and advancing equality of outcome. Exploring a hypothetical discrimination suit against Facebook, we argue that interpretations of equal treatment which require blindness to difference may constrain how machine learning can be deployed to advance equality of outcome. When machine learning models predict outcomes that are unevenly distributed across racial groups, using those models to advance racial justice will often require deliberately taking race into account. We then explore the normative stakes of this tension. We describe three pragmatic policy options underpinned by distinct interpretations and applications \u2026", "CrossWalk: Fairness-enhanced Node Representation Learning": "The potential for machine learning systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. Much recent work has focused on developing algorithmic tools to assess and mitigate such unfairness. However, there is little work on enhancing fairness in graph algorithms. Here, we develop a simple, effective and general method, CrossWalk, that enhances fairness of various graph algorithms, including influence maximization, link prediction and node classification, applied to node embeddings. CrossWalk is applicable to any random walk based node representation learning algorithm, such as DeepWalk and Node2Vec. The key idea is to bias random walks to cross group boundaries, by upweighting edges which (1) are closer to the groups' peripheries or (2) connect different groups in the network. CrossWalk pulls nodes that are near groups' peripheries towards their neighbors from other groups in the embedding space, while preserving the necessary structural information from the graph. Extensive experiments show the effectiveness of our algorithm to enhance fairness in various graph algorithms, including influence maximization, link prediction and node classification in synthetic and real networks, with only a very small decrease in performance.", "Effects of Uncertainty on the Quality of Feature Importance Explanations": "Post-hoc feature importance scores are one method for explaining machine learning model outputs. Such explanations are often used by ML practitioners as part of the model selection process, where they select the best model for their task from a set of candidate models. In this paper, we explore the effects of model uncertainty on the quality of these explanations. Specifically, we develop an approach to quantitatively connect the uncertainty in model predictions with explanation quality in terms of (i) variance,(ii) complexity,(iii) monotonicity,(iv) efficiency, and (v) faithfulness of the explanations. We demonstrate that uncertainty in predictions among a set of candidate models propagates to uncertainty in the feature importance explanations, sometimes resulting in arbitrary explanations for a given sample. We conduct experiments across a range of datasets, model types, and feature importance explanation techniques. Our results show that explanation quality is much poorer for out-of-distribution samples compared to in-distribution (ie, uncertain vs. certain) samples. We also analyze the effect of the number of candidate models and subsample size on measures of feature importance. Overall, our findings suggest that in the presence of uncertainty, current feature importance explanation techniques are unreliable.", "TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning": "One of the challenges to reinforcement learning (RL) is scalable transferability among complex tasks. Incorporating a graphical model (GM), along with the rich family of related methods, as a basis for RL frameworks provides potential to address issues such as transferability, generalisation and exploration. Here we propose a flexible GM-based RL framework which leverages efficient inference procedures to enhance generalisation and transfer power. In our proposed transferable and information-based graphical model framework \u2018TibGM\u2019, we show the equivalence between our mutual information-based objective in the GM, and an RL consolidated objective consisting of a standard reward maximisation target and a generalisation/transfer objective. In settings where there is a sparse or deceptive reward signal, our TibGM framework is flexible enough to incorporate exploration bonuses depicting intrinsic rewards. We empirically verify improved performance and exploration power.", "Reliable learning by subsuming a trusted model: Safe exploration of the space of complex models": "Designing machine learning algorithms that are reliable, safe, and trustworthy is an important factor when using predictions to make critical decisions in real-world applications including healthcare, law, and self-driving cars. A fundamental challenge faced by a practitioner is how to trade-off higher accuracy of a complex model with more reliability of a simpler, trusted model. In this paper, we propose a novel learning framework to tackle this challenge\u2014our key idea is to safely explore the space of complex models by subsuming a base model which is already trusted. We achieve this via enforcing a regularization constraint in the learning process of the complex model based on the predictions of a trusted model. Our approach is generic, allowing us to consider different trusted models and different ways to enforce the regularization constraint. We demonstrate these ideas via experiments using synthetic and real-world datasets.", "A First Survey on an Atlas of Intelligence": "In this note, we include the details of a questionnaire we designed in the summer of 2017 and the results of the survey we conducted on experts from different communities: artificial intelligence, animal cognition, psychology, philosophy, design and some others. We analyse the results and the feedback we received.", "Filling gaps in trustworthy development of AI": "Incident sharing, auditing, and other concrete mechanisms could help verify the trustworthiness of actors", "Diverse, Global and Amortised Counterfactual Explanations for Uncertainty Estimates": "To interpret uncertainty estimates from differentiable probabilistic models, recent work has proposed generating a single Counterfactual Latent Uncertainty Explanation (CLUE) for a given data point where the model is uncertain, identifying a single, on-manifold change to the input such that the model becomes more certain in its prediction. We broaden the exploration to examine {\\delta}-CLUE, the set of potential CLUEs within a {\\delta} ball of the original input in latent space. We study the diversity of such sets and find that many CLUEs are redundant; as such, we propose DIVerse CLUE ({\\nabla}-CLUE), a set of CLUEs which each propose a distinct explanation as to how one can decrease the uncertainty associated with an input. We then further propose GLobal AMortised CLUE (GLAM-CLUE), a distinct and novel method which learns amortised mappings on specific groups of uncertain inputs, taking them and efficiently transforming them in a single function call into inputs for which a model will be certain. Our experiments show that {\\delta}-CLUE, {\\nabla}-CLUE, and GLAM-CLUE all address shortcomings of CLUE and provide beneficial explanations of uncertainty estimates to practitioners.", "PolyViT: Co-training Vision Transformers on Images, Videos and Audio": "Can we train a single transformer model capable of processing multiple modalities and datasets, whilst sharing almost all of its learnable parameters? We present PolyViT, a model trained on image, audio and video which answers this question. By co-training different tasks on a single modality, we are able to improve the accuracy of each individual task and achieve state-of-the-art results on 5 standard video- and audio-classification datasets. Co-training PolyViT on multiple modalities and tasks leads to a model that is even more parameter-efficient, and learns representations that generalize across multiple domains. Moreover, we show that co-training is simple and practical to implement, as we do not need to tune hyperparameters for each combination of datasets, but can simply adapt those from standard, single-task training.", "Provable Defense Against Clustering Attacks on 3D Point Clouds": "Lately, the literature on adversarial robustness spans from images to other domains such as point clouds. In this work, we consider clustering attacks on 3D point clouds and devise a provable defense mechanism to counter them. Specifically, we adopt a randomized smoothing strategy for 3D point clouds and derive a robustness certificate based on the cluster radius rather than the number of adversarial points. Our experiments on ModelNet40 and ScanObjectNN datasets using the PointNet classifier demonstrate the effectiveness of our defense mechanism against targeted and untargeted clustering attacks with a large number of adversarial points.", "Hybrid Random Features": "We propose a new class of random feature methods for linearizing softmax and Gaussian kernels called hybrid random features (HRFs) that automatically adapt the quality of kernel estimation to provide most accurate approximation in the defined regions of interest. Special instantiations of HRFs lead to well-known methods such as trigonometric (Rahimi and Recht, 2007) or (recently introduced in the context of linear-attention Transformers) positive random features (Choromanski et al., 2021). By generalizing Bochner's Theorem for softmax/Gaussian kernels and leveraging random features for compositional kernels, the HRF-mechanism provides strong theoretical guarantees - unbiased approximation and strictly smaller worst-case relative errors than its counterparts. We conduct exhaustive empirical evaluation of HRF ranging from pointwise kernel estimation experiments, through tests on data admitting clustering structure to benchmarking implicit-attention Transformers (also for downstream Robotics applications), demonstrating its quality in a wide spectrum of machine learning problems.", "An Algorithmic Framework for Positive Action": "Positive action is defined within anti-discrimination legislation as voluntary, legal action taken to address an imbalance of opportunity affecting individuals belonging to under-represented groups. Within this theme, we propose a novel algorithmic fairness framework to advance equal representation while respecting anti-discrimination legislation and equal-treatment rights. We use a counterfactual fairness approach to assign one of three outcomes to each candidate: accept; reject; or flagged as a positive action candidate.", "Fast conformal classification using influence functions": "We use influence functions from robust statistics to speed up full conformal prediction. Traditionally, conformal prediction requires retraining multiple leave-one-out classifiers to calculate p-values for each test point. By using influence functions, we are able to approximate this procedure and to speed up considerably the time complexity.", "SphereFace Revived: Unifying Hyperspherical Face Recognition": "This paper addresses the deep face recognition problem under an open-set protocol, where ideal face features are expected to have smaller maximal intra-class distance than minimal inter-class distance under a suitably chosen metric space. To this end, hyperspherical face recognition, as a promising line of research, has attracted increasing attention and gradually become a major focus in face recognition research. As one of the earliest works in hyperspherical face recognition, SphereFace explicitly proposed to learn face embeddings with large inter-class angular margin. However, SphereFace still suffers from severe training instability which limits its application in practice. In order to address this problem, we introduce a unified framework to understand large angular margin in hyperspherical face recognition. Under this framework, we extend the study of SphereFace and propose an improved variant with substantially better training stability -- SphereFace-R. Specifically, we propose two novel ways to implement the multiplicative margin, and study SphereFace-R under three different feature normalization schemes (no feature normalization, hard feature normalization and soft feature normalization). We also propose an implementation strategy -- \"characteristic gradient detachment\" -- to stabilize training. Extensive experiments on SphereFace-R show that it is consistently better than or competitive with state-of-the-art methods.", "SphereFace2: Binary Classification is All You Need for Deep Face Recognition": "State-of-the-art deep face recognition methods are mostly trained with a softmax-based multi-class classification framework. Despite being popular and effective, these methods still have a few shortcomings that limit empirical performance. In this paper, we first identify the discrepancy between training and evaluation in the existing multi-class classification framework and then discuss the potential limitations caused by the \"competitive\" nature of softmax normalization. Motivated by these limitations, we propose a novel binary classification training framework, termed SphereFace2. In contrast to existing methods, SphereFace2 circumvents the softmax normalization, as well as the corresponding closed-set assumption. This effectively bridges the gap between training and evaluation, enabling the representations to be improved individually by each binary classification task. Besides designing a specific well-performing loss function, we summarize a few general principles for this \"one-vs-all\" binary classification framework so that it can outperform current competitive methods. We conduct comprehensive experiments on popular benchmarks to demonstrate that SphereFace2 can consistently outperform current state-of-the-art deep face recognition methods.", "Beyond Reasonable Doubt: Improving Fairness in Budget-Constrained Decision Making using Confidence Thresholds": "Prior work on fairness in machine learning has focused on settings where all the information needed about each individual is readily available. However, in many applications, further information may be acquired at a cost. For example, when assessing a customer's creditworthiness, a bank initially has access to a limited set of information but progressively improves the assessment by acquiring additional information before making a final decision. In such settings, we posit that a fair decision maker may want to ensure that decisions for all individuals are made with similar expected error rate, even if the features acquired for the individuals are different. We show that a set of carefully chosen confidence thresholds can not only effectively redistribute an information budget according to each individual's needs, but also serve to address individual and group fairness concerns simultaneously. Finally, using two public \u2026", "Debiasing a First-order Heuristic for Approximate Bi-level Optimization": "Approximate bi-level optimization (ABLO) consists of (outer-level) optimization problems, involving numerical (inner-level) optimization loops. While ABLO has many applications across deep learning, it suffers from time and memory complexity proportional to the length of its inner optimization loop. To address this complexity, an earlier first-order method (FOM) was proposed as a heuristic that omits second derivative terms, yielding significant speed gains and requiring only constant memory. Despite FOM's popularity, there is a lack of theoretical understanding of its convergence properties. We contribute by theoretically characterizing FOM's gradient bias under mild assumptions. We further demonstrate a rich family of examples where FOM-based SGD does not converge to a stationary point of the ABLO objective. We address this concern by proposing an unbiased FOM (UFOM) enjoying constant memory complexity as a function of . We characterize the introduced time-variance tradeoff, demonstrate convergence bounds, and find an optimal UFOM for a given ABLO problem. Finally, we propose an efficient adaptive UFOM scheme.", "Leader Stochastic Gradient Descent for Distributed Training of Deep Learning Models: Extension": "We consider distributed optimization under communication constraints for training deep learning models. We propose a new algorithm, whose parameter updates rely on two forces: a regular gradient step, and a corrective direction dictated by the currently best-performing worker (leader). Our method differs from the parameter-averaging scheme EASGD in a number of ways:(i) our objective formulation does not change the location of stationary points compared to the original optimization problem;(ii) we avoid convergence decelerations caused by pulling local workers descending to different local minima to each other (ie to the average of their parameters);(iii) our update by design breaks the curse of symmetry (the phenomenon of being trapped in poorly generalizing sub-optimal solutions in symmetric non-convex landscapes); and (iv) our approach is more communication efficient since it broadcasts only \u2026", "Interpretable Continual Learning": "We present a framework for interpretable continual learning (ICL). We show that explanations of previously performed tasks can be used to improve performance on future tasks. ICL generates a good explanation of a finished task, then uses this to focus attention on what is important when facing a new task. The ICL idea is general and may be applied to many continual learning approaches. Here we focus on the variational continual learning framework to take advantage of its flexibility and efficacy in overcoming catastrophic forgetting. We use saliency maps to provide explanations of performed tasks and propose a new metric to assess their quality. Experiments show that ICL achieves state-of-the-art results in terms of overall continual learning performance as measured by average classification accuracy, and also in terms of its explanations, which are assessed qualitatively and quantitatively using the proposed metric.", "Diverse and Amortised Counterfactual Explanations for Uncertainty Estimates": "To interpret uncertainty estimates from differentiable probabilistic models,[1] proposed generating a single Counterfactual Latent Uncertainty Explanation (CLUE) for a given data point where the model is uncertain.[2] formulated \u03b4-CLUE, the set of CLUEs within a \u03b4 ball of the original input in latent space\u2013however, we find that many CLUEs generated by this method are very similar, hence redundant. We propose DI-Verse CLUEs (\u2207-CLUEs), a set of CLUEs which each provide a distinct explanation. We further introduce GLobal AMortised CLUEs (GLAM-CLUEs), which represent amortised mappings that apply to specific groups of uncertain inputs, taking them and efficiently transforming them in a single function call into inputs that a model will be certain about. Our experiments show that\u2207-CLUEs and GLAM-CLUEs both address shortcomings of CLUE and provide beneficial explanations of uncertainty estimates to practitioners.", "MACHINE ETHICS: THE DESIGN AND GOVERNANCE OF ETHICAL AI AND AUTONOMOUS SYSTEMS": "MACHINE ETHICS: THE DESIGN AND GOVERNANCE OF ETHICAL AI AND AUTONOMOUS SYSTEMS Page 1 SPECIAL ISSUE MACHINE ETHICS: THE DESIGN AND GOVERNANCE OF ETHICAL AI AND AUTONOMOUS SYSTEMS Edited by AF Winfield, K. Michael, J. Pitt, and V. Evers 518 Designing a Value-Driven Future for Ethical Autonomous and Intelligent Systems By G. Adamson, JC Havens, and R. Chatila |INVITED PAPER| This paper provides an overview of IEEE\u2019s current activities related to ethics and argues that human values must drive our future autonomous systems in a way that both protects and benefits humanity. 526 A Value-Driven Eldercare Robot: Virtual and Physical Instantiations of a Case-Supported Principle-Based Behavior Paradigm By M. Anderson, SL Anderson, and V. Berenz |INVITED PAPER| This paper describes both simulated and real-robot implementations of an eldercare robot \u2026", "Philosophy and Theory of Artificial Intelligence 2017": null}, "Richard Turner": {"Variational continual learning": "This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.", "Q-prop: Sample-efficient policy gradient with an off-policy critic": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "Gaussian process behaviour in wide deep neural networks": "Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between random, wide, fully connected, feedforward networks with more than one hidden layer and Gaussian processes with a recursive kernel definition. We show that, under broad conditions, as we make the architecture increasingly wide, the implied random function converges in distribution to a Gaussian process, formalising and extending existing results by Neal (1996) to deep networks. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then compare finite Bayesian deep networks from the literature to Gaussian processes in terms of the key predictive quantities of interest, finding that in some cases the agreement can be very close. We discuss the desirability of Gaussian process behaviour and review non-Gaussian alternative models from the literature.", "The processing and perception of size information in speech sounds": "There is information in speech sounds about the length of the vocal tract; specifically, as a child grows, the resonators in the vocal tract grow and the formant frequencies of the vowels decrease. It has been hypothesized that the auditory system applies a scale transform to all sounds to segregate size information from resonator shape information, and thereby enhance both size perception and speech recognition [Irino and Patterson, Speech Commun. 36, 181\u2013203 (2002)]. This paper describes size discrimination experiments and vowel recognition experiments designed to provide evidence for an auditory scaling mechanism. Vowels were scaled to represent people with vocal tracts much longer and shorter than normal, and with pitches much higher and lower than normal. The results of the discrimination experiments show that listeners can make fine judgments about the relative size of speakers, and they can do \u2026", "R\\'enyi Divergence Variational Inference": "This paper introduces the variational R\\'enyi bound (VR) that extends traditional variational inference to R\\'enyi's alpha-divergences. This new family of variational methods unifies a number of existing approaches, and enables a smooth interpolation from the evidence lower-bound to the log (marginal) likelihood that is controlled by the value of alpha that parametrises the divergence. The reparameterization trick, Monte Carlo approximation and stochastic optimisation methods are deployed to obtain a tractable and unified framework for optimisation. We further consider negative alpha values and propose a novel variational inference method as a new special case in the proposed framework. Experiments on Bayesian neural networks and variational auto-encoders demonstrate the wide applicability of the VR bound.", "Deep Gaussian processes for regression using approximate expectation propagation": "Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic models and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. This paper develops a new approximate Bayesian learning scheme that enables DGPs to be applied to a range of medium to large scale regression problems for the first time. The new method uses an approximate Expectation Propagation procedure and a novel and efficient extension of the probabilistic backpropagation algorithm for learning. We evaluate the new method for non-linear regression on eleven real-world datasets, showing that it always outperforms GP regression and is almost always better than state-of-the-art deterministic and sampling-based approximate inference methods for Bayesian neural networks. As a by-product, this work provides a comprehensive analysis of six approximate Bayesian methods for training neural networks.", "Black-box alpha divergence minimization": "Black-box alpha (BB-\u03b1) is a new approximate inference method based on the minimization of \u03b1-divergences. BB-\u03b1scales to large datasets because it can be implemented using stochastic gradient descent. BB-\u03b1can be applied to complex probabilistic models with little effort since it only requires as input the likelihood function and its gradients. These gradients can be easily obtained using automatic differentiation. By changing the divergence parameter \u03b1, the method is able to interpolate between variational Bayes (VB)(\u03b1\u2192 0) and an algorithm similar to expectation propagation (EP)(\u03b1= 1). Experiments on probit regression and neural network regression and classification problems show that BB-\u03b1with non-standard settings of \u03b1, such as \u03b1= 0.5, usually produces better predictions than with \u03b1\u2192 0 (VB) or \u03b1= 1 (EP).", "Invariant models for causal transfer learning": "Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a subset of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the field of causality. We focus on the problem of Domain Generalization, in which no examples from the test task are observed. We prove that in an adversarial setting using this subset for prediction is optimal in Domain Generalization; we further provide examples, in which the tasks are sufficiently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we also provide a method to transfer knowledge from the training tasks and exploit all available features for prediction. However, we provide no guarantees for this method. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set.", "Meta-learning probabilistic inference for prediction": "This paper introduces a new framework for data efficient and versatile learning. Specifically: 1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction. ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. 2) We introduce VERSA, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. VERSA substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training. 3) We evaluate VERSA on benchmark datasets where the method sets new state-of-the-art results, handles arbitrary numbers of shots, and for classification, arbitrary numbers of classes at train and test time. The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task.", "Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning": "Off-policy model-free deep reinforcement learning methods using previously collected data can improve sample efficiency over on-policy policy gradient techniques. On the other hand, on-policy algorithms are often more stable and easier to use. This paper examines, both theoretically and empirically, approaches to merging on- and off-policy updates for deep reinforcement learning. Theoretical results show that off-policy updates with a value function estimator can be interpolated with on-policy policy gradient updates whilst still satisfying performance bounds. Our analysis uses control variate methods to produce a family of policy gradient algorithms, with several recently proposed algorithms being special cases of this family. We then provide an empirical comparison of these techniques with the remaining algorithmic details fixed, and show how different mixing of off-policy gradient estimates with on-policy samples contribute to improvements in empirical performance. The final algorithm provides a generalization and unification of existing deep policy gradient techniques, has theoretical guarantees on the bias introduced by off-policy updates, and improves on the state-of-the-art model-free deep RL methods on a number of OpenAI Gym continuous control benchmarks.", "Neural adaptive sequential monte carlo": "Sequential Monte Carlo (SMC), or particle filtering, is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions. Like other importance sampling-based methods, performance is critically dependent on the proposal distribution: a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution. This paper presents a new method for automatically adapting the proposal using an approximation of the Kullback-Leibler divergence between the true posterior and the proposal distribution. The method is very flexible, applicable to any parameterized proposal distribution and it supports online and batch variants. We use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to Neural Adaptive Sequential Monte Carlo (NASMC). Experiments indicate that NASMC significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the Extended Kalman and Unscented Particle Filters. Experiments also indicate that improved inference translates into improved parameter learning when NASMC is used as a subroutine of Particle Marginal Metropolis Hastings. Finally we show that NASMC is able to train a latent variable recurrent neural network (LV-RNN) achieving results that compete with the state-of-the-art for polymorphic music modelling. NASMC can be seen as bridging the gap between adaptive SMC methods and the recent work in scalable, black-box variational inference.", "A unifying framework for Gaussian process pseudo-point approximations using power expectation propagation": "Gaussian processes (GPs) are flexible distributions over functions that enable highlevel assumptions about unknown functions to be encoded in a parsimonious, flexible and general way. Although elegant, the application of GPs is limited by computational and analytical intractabilities that arise when data are sufficiently numerous or when employing non-Gaussian models. Consequently, a wealth of GP approximation schemes have been developed over the last 15 years to address these key limitations. Many of these schemes employ a small set of pseudo data points to summarise the actual data. In this paper we develop a new pseudo-point approximation framework using Power Expectation Propagation (Power EP) that unifies a large number of these pseudo-point approximations. Unlike much of the previous venerable work in this area, the new framework is built on standard methods for approximate inference (variational free-energy, EP and Power EP methods) rather than employing approximations to the probabilistic generative model itself. In this way all of the approximation is performed at \u2018inference time\u2019rather than at \u2018modelling time\u2019, resolving awkward philosophical and empirical questions that trouble previous approaches. Crucially, we demonstrate that the new framework includes new pseudo-point approximation methods that outperform current approaches on regression and classification tasks.", "Practical deep learning with Bayesian principles": "Bayesian methods promise to fix many shortcomings of deep learning, but they are impractical and rarely match the performance of standard methods, let alone improve them. In this paper, we demonstrate practical training of deep networks with natural-gradient variational inference. By applying techniques such as batch normalisation, data augmentation, and distributed training, we achieve similar performance in about the same number of epochs as the Adam optimiser, even on large datasets such as ImageNet. Importantly, the benefits of Bayesian principles are preserved: predictive probabilities are well-calibrated, uncertainties on out-of-distribution data are improved, and continual-learning performance is boosted. This work enables practical deep learning while preserving benefits of Bayesian principles. A PyTorch implementation is available as a plug-and-play optimiser.", "Stochastic expectation propagation": "Expectation propagation (EP) is a deterministic approximation algorithm that is often used to perform approximate Bayesian parameter learning. EP approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint. EP can offer analytic and computational advantages over other approximations, such as Variational Inference (VI), and is the method of choice for a number of models. The local nature of EP appears to make it an ideal candidate for performing Bayesian learning on large models in large-scale dataset settings. However, EP has a crucial limitation in this context: the number of approximating factors needs to increase with the number of data-points, N, which often entails a prohibitively large memory overhead. This paper presents an extension to EP, called stochastic expectation propagation (SEP), that maintains a global posterior approximation (like VI) but updates it in a local way (like EP). Experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that SEP performs almost as well as full EP, but reduces the memory consumption by a factor of . SEP is therefore ideally suited to performing approximate Bayesian learning in the large model, large dataset setting.", "Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control": "This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.", "Deterministic variational inference for robust bayesian neural networks": "Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data. Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient. With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications? We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates. We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances. Combining these two innovations, the resulting method is highly efficient and robust. On the application of heteroscedastic regression we demonstrate good predictive performance over alternative approaches.", "Nonlinear ICA using auxiliary variables and generalized contrastive learning": "Nonlinear ICA is a fundamental problem for unsupervised representation learning, emphasizing the capacity to recover the underlying latent variables generating the data (ie, identifiability). Recently, the very first identifiability proofs for nonlinear ICA have been proposed, leveraging the temporal structure of the independent components. Here, we propose a general framework for nonlinear ICA, which, as a special case, can make use of temporal structure. It is based on augmenting the data by an auxiliary variable, such as the time index, the history of the time series, or any other available information. We propose to learn nonlinear ICA by discriminating between true augmented data, or data in which the auxiliary variable has been randomized. This enables the framework to be implemented algorithmically through logistic regression, possibly in a neural network. We provide a comprehensive proof of the identifiability of the model as well as the consistency of our estimation method. The approach not only provides a general theoretical framework combining and generalizing previously proposed nonlinear ICA models and algorithms, but also brings practical advantages.", "The mirage of action-dependent baselines in reinforcement learning": "Policy gradient methods are a widely used class of model-free reinforcement learning algorithms where a state-dependent baseline is used to reduce gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action and suggest that this significantly reduces variance and improves sample efficiency without introducing bias into the gradient estimates. To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in commonly tested benchmark domains. We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the source of the previously observed empirical gains. Furthermore, the variance decomposition highlights areas for improvement, which we demonstrate by illustrating a simple change to the typical value function parameterization that can significantly improve performance.", "Structured evolution with compact architectures for scalable policy optimization": "We present a new method of blackbox optimization via gradient approximation with the use of structured random orthogonal matrices, providing more accurate estimators than baselines and with provable theoretical guarantees. We show that this algorithm can be successfully applied to learn better quality compact policies than those using standard gradient estimation techniques. The compact policies we learn have several advantages over unstructured ones, including faster training algorithms and faster inference. These benefits are important when the policy is deployed on real hardware with limited resources. Further, compact policies provide more scalable architectures for derivative-free optimization (DFO) in high-dimensional spaces. We show that most robotics tasks from the OpenAI Gym can be solved using neural networks with less than 300 parameters, with almost linear time complexity of the inference phase, with up to 13x fewer parameters relative to the Evolution Strategies (ES) algorithm introduced by Salimans et al.(2017). We do not need heuristics such as fitness shaping to learn good quality policies, resulting in a simple and theoretically motivated training mechanism.", "A maximum-likelihood interpretation for slow feature analysis": "The brain extracts useful features from a maelstrom of sensory information, and a fundamental goal of theoretical neuroscience is to work out how it does so. One proposed feature extraction strategy is motivated by the observation that the meaning of sensory data, such as the identity of a moving visual object, is often more persistent than the activation of any single sensory receptor. This notion is embodied in the slow feature analysis (SFA) algorithm, which uses \u201cslowness\u201d as a heuristic by which to extract semantic information from multidimensional time series. Here, we develop a probabilistic interpretation of this algorithm, showing that inference and learning in the limiting case of a suitable probabilistic model yield exactly the results of SFA. Similar equivalences have proved useful in interpreting and extending comparable algorithms such as independent component analysis. For SFA, we use the equivalent \u2026", "Fast and flexible multi-task classification using conditional neural adaptive processes": "The goal of this paper is to design image classification systems that, after an initial multi-task training phase, can automatically adapt to new tasks encountered at test time. We introduce a conditional neural process based approach to the multi-task classification setting for this purpose, and establish connections to the meta-and few-shot learning literature. The resulting approach, called CNAPs, comprises a classifier whose parameters are modulated by an adaptation network that takes the current task's dataset as input. We demonstrate that CNAPs achieves state-of-the-art results on the challenging Meta-Dataset benchmark indicating high-quality transfer-learning. We show that the approach is robust, avoiding both over-fitting in low-shot regimes and under-fitting in high-shot regimes. Timing experiments reveal that CNAPs is computationally efficient at test-time as it does not involve gradient based adaptation. Finally, we show that trained models are immediately deployable to continual learning and active learning where they can outperform existing approaches that do not leverage transfer learning.", "Tuning recurrent neural networks with reinforcement learning": "The approach of training sequence models using supervised learning and next-step prediction suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note-RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "Improving the Gaussian process sparse spectrum approximation by representing uncertainty in frequency inputs": "Standard sparse pseudo-input approximations to the Gaussian process (GP) cannot handle complex functions well. Sparse spectrum alternatives attempt to answer this but are known to over-fit. We suggest the use of variational inference for the sparse spectrum approximation to avoid both issues. We model the covariance function with a finite Fourier series approximation and treat it as a random variable. The random covariance function has a posterior, on which a variational distribution is placed. The variational distribution transforms the random covariance function to fit the data. We study the properties of our approximate inference, compare it to alternative ones, and extend it to the distributed and stochastic domains. Our approximation captures complex functions better than standard approaches and avoids over-fitting.", "Gradient estimators for implicit models": "Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the Stein gradient estimator, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference, and entropy regularised GANs that provide improved sample diversity.", "Streaming sparse Gaussian process approximations": "Sparse pseudo-point approximations for Gaussian process (GP) models provide a suite of methods that support deployment of GPs in the large data regime and enable analytic intractabilities to be sidestepped. However, the field lacks a principled method to handle streaming data in which both the posterior distribution over function values and the hyperparameter estimates are updated in an online fashion. The small number of existing approaches either use suboptimal hand-crafted heuristics for hyperparameter learning, or suffer from catastrophic forgetting or slow updating when new data arrive. This paper develops a new principled framework for deploying Gaussian process probabilistic models in the streaming setting, providing methods for learning hyperparameters and optimising pseudo-input locations. The proposed framework is assessed using synthetic and real-world datasets.", "Statistical models for natural sounds": "It is important to understand the rich structure of natural sounds in order to solve important tasks, like automatic speech recognition, and to understand auditory processing in the brain. This thesis takes a step in this direction by characterising the statistics of simple natural sounds. We focus on the statistics because perception often appears to depend on them, rather than on the raw waveform. For example the perception of auditory textures, like running water, wind, fire and rain, depends on summary-statistics, like the rate of falling rain droplets, rather than on the exact details of the physical source. In order to analyse the statistics of sounds accurately it is necessary to improve a number of traditional signal processing methods, including those for amplitude demodulation, time-frequency analysis, and sub-band demodulation. These estimation tasks are ill-posed and therefore it is natural to treat them as Bayesian inference problems. The new probabilistic versions of these methods have several advantages. For example, they perform more accurately on natural signals and are more robust to noise, they can also fill-in missing sections of data, and provide error-bars. Furthermore, free-parameters can be learned from the signal. Using these new algorithms we demonstrate that the energy, sparsity, modulation depth and modulation time-scale in each sub-band of a signal are critical statistics, together with the dependencies between the sub-band modulators. In order to validate this claim, a model containing co-modulated coloured noise carriers is shown to be capable of generating a range of realistic sounding auditory textures. Finally, we explored \u2026", "Learning stationary time series using Gaussian processes with nonparametric kernels": "We introduce the Gaussian Process Convolution Model (GPCM), a two-stage nonparametric generative procedure to model stationary signals as the convolution between a continuous-time white-noise process and a continuous-time linear filter drawn from Gaussian process. The GPCM is a continuous-time nonparametricwindow moving average process and, conditionally, is itself a Gaussian process with a nonparametric kernel defined in a probabilistic fashion. The generative model can be equivalently considered in the frequency domain, where the power spectral density of the signal is specified using a Gaussian process. One of the main contributions of the paper is to develop a novel variational freeenergy approach based on inter-domain inducing variables that efficiently learns the continuous-time linear filter and infers the driving white-noise process. In turn, this scheme provides closed-form probabilistic estimates of the covariance kernel and the noise-free signal both in denoising and prediction scenarios. Additionally, the variational inference procedure provides closed-form expressions for the approximate posterior of the spectral density given the observed data, leading to new Bayesian nonparametric approaches to spectrum estimation. The proposed GPCM is validated using synthetic and real-world signals.", "A statistical, formant-pattern model for segregating vowel type and vocal-tract length in developmental formant data": "This paper investigates the theoretical basis for estimating vocal-tract length (VTL) from the formant frequencies of vowel sounds. A statistical inference model was developed to characterize the relationship between vowel type and VTL, on the one hand, and formant frequency and vocal cavity size, on the other. The model was applied to two well known developmental studies of formant frequency. The results show that VTL is the major source of variability after vowel type and that the contribution due to other factors like developmental changes in oral-pharyngeal ratio is small relative to the residual measurement noise. The results suggest that speakers adjust the shape of the vocal tract as they grow to maintain a specific pattern of formant frequencies for individual vowels. This formant-pattern hypothesis motivates development of a statistical-inference model for estimating VTL from formant-frequency data. The \u2026", "Tree-structured Gaussian process approximations": "Gaussian process regression can be accelerated by constructing a small pseudo-dataset to summarise the observed data. This idea sits at the heart of many approximation schemes, but such an approach requires the number of pseudo-datapoints to be scaled with the range of the input space if the accuracy of the approximation is to be maintained. This presents problems in time-series settings or in spatial datasets where large numbers of pseudo-datapoints are required since computation typically scales quadratically with the pseudo-dataset size. In this paper we devise an approximation whose complexity grows linearly with the number of pseudo-datapoints. This is achieved by imposing a tree or chain structure on the pseudo-datapoints and calibrating the approximation using a Kullback-Leibler (KL) minimisation. Inference and learning can then be performed efficiently using the Gaussian belief propagation algorithm. We demonstrate the validity of our approach on a set of challenging regression tasks including missing data imputation for audio and spatial datasets. We trace out the speed-accuracy trade-off for the new method and show that the frontier dominates those obtained from a large number of existing approximation techniques.", "Demodulation as probabilistic inference": "Demodulation is an ill-posed problem whenever both carrier and envelope signals are broadband and unknown. Here, we approach this problem using the methods of probabilistic inference. The new approach, called Probabilistic Amplitude Demodulation (PAD), is computationally challenging but improves on existing methods in a number of ways. By contrast to previous approaches to demodulation, it satisfies five key desiderata: PAD has soft constraints because it is probabilistic; PAD is able to automatically adjust to the signal because it learns parameters; PAD is user-steerable because the solution can be shaped by user-specific prior information; PAD is robust to broad-band noise because this is modeled explicitly; and PAD's solution is self-consistent, empirically satisfying a Carrier Identity property. Furthermore, the probabilistic view naturally encompasses noise and uncertainty, allowing PAD to cope with \u2026", "A structured model of video reproduces primary visual cortical organisation": "The visual system must learn to infer the presence of objects and features in the world from the images it encounters, and as such it must, either implicitly or explicitly, model the way these elements interact to create the image. Do the response properties of cells in the mammalian visual system reflect this constraint? To address this question, we constructed a probabilistic model in which the identity and attributes of simple visual elements were represented explicitly and learnt the parameters of this model from unparsed, natural video sequences. After learning, the behaviour and grouping of variables in the probabilistic model corresponded closely to functional and anatomical properties of simple and complex cells in the primary visual cortex (V1). In particular, feature identity variables were activated in a way that resembled the activity of complex cells, while feature attribute variables responded much like simple cells. Furthermore, the grouping of the attributes within the model closely parallelled the reported anatomical grouping of simple cells in cat V1. Thus, this generative model makes explicit an interpretation of complex and simple cells as elements in the segmentation of a visual scene into basic independent features, along with a parametrisation of their moment-by-moment appearances. We speculate that such a segmentation may form the initial stage of a hierarchical system that progressively separates the identity and appearance of more articulated visual elements, culminating in view-invariant object recognition.", "'In-Between'Uncertainty in Bayesian Neural Networks": "We describe a limitation in the expressiveness of the predictive uncertainty estimate given by mean-field variational inference (MFVI), a popular approximate inference method for Bayesian neural networks. In particular, MFVI fails to give calibrated uncertainty estimates in between separated regions of observations. This can lead to catastrophically overconfident predictions when testing on out-of-distribution data. Avoiding such overconfidence is critical for active learning, Bayesian optimisation and out-of-distribution robustness. We instead find that a classical technique, the linearised Laplace approximation, can handle 'in-between' uncertainty much better for small network architectures.", "Discriminative k-shot learning using probabilistic models": "This paper introduces a probabilistic framework for k-shot image classification. The goal is to generalise from an initial large-scale classification task to a separate task comprising new classes and small numbers of examples. The new approach not only leverages the feature-based representation learned by a neural network from the initial task (representational transfer), but also information about the classes (concept transfer). The concept information is encapsulated in a probabilistic model for the final layer weights of the neural network which acts as a prior for probabilistic k-shot learning. We show that even a simple probabilistic model achieves state-of-the-art on a standard k-shot learning dataset by a large margin. Moreover, it is able to accurately model uncertainty, leading to well calibrated classifiers, and is easily extensible and flexible, unlike many recent approaches to k-shot learning.", "Approximate inference with amortised mcmc": "We propose a novel approximate inference algorithm that approximates a target distribution by amortising the dynamics of a user-selected MCMC sampler. The idea is to initialise MCMC using samples from an approximation network, apply the MCMC operator to improve these samples, and finally use the samples to update the approximation network thereby improving its quality. This provides a new generic framework for approximate inference, allowing us to deploy highly complex, or implicitly defined approximation families with intractable densities, including approximations produced by warping a source of randomness through a deep neural network. Experiments consider image modelling with deep generative models as a challenging test for the method. Deep models trained using amortised MCMC are shown to generate realistic looking samples as well as producing diverse imputations for images with regions of missing pixels.", "A role for amplitude modulation phase relationships in speech rhythm perception": "Prosodic rhythm in speech [the alternation of \u201cStrong\u201d (S) and \u201cweak\u201d (w) syllables] is cued, among others, by slow rates of amplitude modulation (AM) within the speech envelope. However, it is unclear exactly which envelope modulation rates and statistics are the most important for the rhythm percept. Here, the hypothesis that the phase relationship between \u201cStress\u201d rate (\u223c2\u2009Hz) and \u201cSyllable\u201d rate (\u223c4\u2009Hz) AMs provides a perceptual cue for speech rhythm is tested. In a rhythm judgment task, adult listeners identified AM tone-vocoded nursery rhyme sentences that carried either trochaic (S-w) or iambic patterning (w-S). Manipulation of listeners' rhythm perception was attempted by parametrically phase-shifting the Stress AM and Syllable AM in the vocoder. It was expected that a 1\u03c0 radian phase-shift (half a cycle) would reverse the perceived rhythm pattern (i.e., trochaic \u2192 iambic) whereas a 2\u03c0 radian shift (full \u2026", "Convolutional conditional neural processes": "We introduce the Convolutional Conditional Neural Process (ConvCNP), a new member of the Neural Process family that models translation equivariance in the data. Translation equivariance is an important inductive bias for many learning problems including time series modelling, spatial data, and images. The model embeds data sets into an infinite-dimensional function space as opposed to a finite-dimensional vector space. To formalize this notion, we extend the theory of neural representations of sets to include functional representations, and demonstrate that any translation-equivariant embedding can be represented using a convolutional deep set. We evaluate ConvCNPs in several settings, demonstrating that they achieve state-of-the-art performance compared to existing NPs. We demonstrate that building in translation equivariance enables zero-shot generalization to challenging, out-of-domain tasks.", "Tasknorm: Rethinking batch normalization for meta-learning": "Modern meta-learning approaches for image classification rely on increasingly deep networks to achieve state-of-the-art performance, making batch normalization an essential component of meta-learning pipelines. However, the hierarchical nature of the meta-learning setting presents several challenges that can render conventional batch normalization ineffective, giving rise to the need to rethink normalization in this setting. We evaluate a range of approaches to batch normalization for meta-learning scenarios, and develop a novel approach that we call TaskNorm. Experiments on fourteen datasets demonstrate that the choice of batch normalization has a dramatic effect on both classification accuracy and training time for both gradient based-and gradient-free meta-learning approaches. Importantly, TaskNorm is found to consistently improve performance. Finally, we provide a set of best practices for normalization that will allow fair comparison of meta-learning algorithms.", "Time-frequency analysis as probabilistic inference": "This paper proposes a new view of time-frequency analysis framed in terms of probabilistic inference. Natural signals are assumed to be formed by the superposition of distinct time-frequency components, with the analytic goal being to infer these components by application of Bayes' rule. The framework serves to unify various existing models for natural time-series; it relates to both the Wiener and Kalman filters, and with suitable assumptions yields inferential interpretations of the short-time Fourier transform, spectrogram, filter bank, and wavelet representations. Value is gained by placing time-frequency analysis on the same probabilistic basis as is often employed in applications such as denoising, source separation, or recognition. Uncertainty in the time-frequency representation can be propagated correctly to application-specific stages, improving the handing of noise and missing data. Probabilistic learning \u2026", "Continual deep learning by functional regularisation of memorable past": "Continually learning new skills is important for intelligent systems, yet standard deep learning methods suffer from catastrophic forgetting of the past. Recent works address this with weight regularisation. Functional regularisation, although computationally expensive, is expected to perform better, but rarely does so in practice. In this paper, we fix this issue by using a new functional-regularisation approach that utilises a few memorable past examples crucial to avoid forgetting. By using a Gaussian Process formulation of deep networks, our approach enables training in weight-space while identifying both the memorable past and a functional prior. Our method achieves state-of-the-art performance on standard benchmarks and opens a new direction for life-long learning where regularisation and memory-based methods are naturally combined.", "Continual learning with adaptive weights (claw)": "Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting.", "Conditional density estimation with bayesian normalising flows": "Modeling complex conditional distributions is critical in a variety of settings. Despite a long tradition of research into conditional density estimation, current methods employ either simple parametric forms or are difficult to learn in practice. This paper employs normalising flows as a flexible likelihood model and presents an efficient method for fitting them to complex densities. These estimators must trade-off between modeling distributional complexity, functional complexity and heteroscedasticity without overfitting. We recognize these trade-offs as modeling decisions and develop a Bayesian framework for placing priors over these conditional density estimators using variational Bayesian neural networks. We evaluate this method on several small benchmark regression datasets, on some of which it obtains state of the art performance. Finally, we apply the method to two spatial density modeling tasks with over 1 million datapoints using the New York City yellow taxi dataset and the Chicago crime dataset.", "A multi-label approach to target prediction taking ligand promiscuity into account": "According to Cobanoglu et al., it is now widely acknowledged that the single target paradigm (one protein/target, one disease, one drug) that has been the dominant premise in drug development in the recent past is untenable. More often than not, a drug-like compound (ligand) can be promiscuous \u2013 it can interact with more than one target protein. In recent years, in in silico target prediction methods the promiscuity issue has generally been approached computationally in three main ways: ligand-based methods; target-protein-based methods; and integrative schemes. In this study we confine attention to ligand-based target prediction machine learning approaches, commonly referred to as target-fishing. The target-fishing approaches that are currently ubiquitous in cheminformatics literature can be essentially viewed as single-label multi-classification schemes; these approaches inherently bank on the single target paradigm assumption that a ligand can zero in on one single target. In order to address the ligand promiscuity issue, one might be able to cast target-fishing as a multi-label multi-class classification problem. For illustrative and comparison purposes, single-label and multi-label Na\u00efve Bayes classification models (denoted here by SMM and MMM, respectively) for target-fishing were implemented. The models were constructed and tested on 65,587 compounds/ligands and 308 targets retrieved from the ChEMBL17 database. On classifying 3,332 test multi-label (promiscuous) compounds, SMM and MMM performed differently. At the 0.05 significance level, a Wilcoxon signed rank test performed on the paired target predictions yielded by \u2026", "Probabilistic amplitude and frequency demodulation": "A number of recent scientific and engineering problems require signals to be decomposed into a product of a slowly varying positive envelope and a quickly varying carrier whose instantaneous frequency also varies slowly over time. Although signal processing provides algorithms for so-called amplitude-and frequencydemodulation (AFD), there are well known problems with all of the existing methods. Motivated by the fact that AFD is ill-posed, we approach the problem using probabilistic inference. The new approach, called probabilistic amplitude and frequency demodulation (PAFD), models instantaneous frequency using an auto-regressive generalization of the von Mises distribution, and the envelopes using Gaussian auto-regressive dynamics with a positivity constraint. A novel form of expectation propagation is used for inference. We demonstrate that although PAFD is computationally demanding, it outperforms previous approaches on synthetic and real signals in clean, noisy and missing data settings.", "On sparsity and overcompleteness in image models.": "Computational models of visual cortex, and in particular those based on sparse coding, have enjoyed much recent attention. Despite this currency, the question of how sparse or how over-complete a sparse representation should be, has gone without principled answer. Here, we use Bayesian model-selection methods to address these questions for a sparse-coding model based on a Student-t prior. Having validated our methods on toy data, we find that natural images are indeed best modelled by extremely sparse distributions; although for the Student-t prior, the associated optimal basis size is only modestly over-complete.", "Overpruning in variational bayesian neural networks": "The motivations for using variational inference (VI) in neural networks differ significantly from those in latent variable models. This has a counter-intuitive consequence; more expressive variational approximations can provide significantly worse predictions as compared to those with less expressive families. In this work we make two contributions. First, we identify a cause of this performance gap, variational over-pruning. Second, we introduce a theoretically grounded explanation for this phenomenon. Our perspective sheds light on several related published results and provides intuition into the design of effective variational approximations of neural networks.", "Improving and understanding variational continual learning": "In the continual learning setting, tasks are encountered sequentially. The goal is to learn whilst i) avoiding catastrophic forgetting, ii) efficiently using model capacity, and iii) employing forward and backward transfer learning. In this paper, we explore how the Variational Continual Learning (VCL) framework achieves these desiderata on two benchmarks in continual learning: split MNIST and permuted MNIST. We first report significantly improved results on what was already a competitive approach. The improvements are achieved by establishing a new best practice approach to mean-field variational Bayesian neural networks. We then look at the solutions in detail. This allows us to obtain an understanding of why VCL performs as it does, and we compare the solution to what an `ideal' continual learning solution might be.", "Magnetic hamiltonian monte carlo": "Hamiltonian Monte Carlo (HMC) exploits Hamiltonian dynamics to construct efficient proposals for Markov chain Monte Carlo (MCMC). In this paper, we present a generalization of HMC which exploits non-canonical Hamiltonian dynamics. We refer to this algorithm as magnetic HMC, since in 3 dimensions a subset of the dynamics map onto the mechanics of a charged particle coupled to a magnetic field. We establish a theoretical basis for the use of non-canonical Hamiltonian dynamics in MCMC, and construct a symplectic, leapfrog-like integrator allowing for the implementation of magnetic HMC. Finally, we exhibit several examples where these non-canonical dynamics can lead to improved mixing of magnetic HMC relative to ordinary HMC.", "Bayesian neural network priors revisited": "Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference. However, such simplistic priors are unlikely to either accurately reflect our true beliefs about the weight distributions, or to give optimal performance. We study summary statistics of neural network weights in different networks trained using SGD. We find that fully connected networks (FCNNs) display heavy-tailed weight distributions, while convolutional neural network (CNN) weights display strong spatial correlations. Building these observations into the respective priors leads to improved performance on a variety of image classification datasets. Moreover, we find that these priors also mitigate the cold posterior effect in FCNNs, while in CNNs we see strong improvements at all temperatures, and hence no reduction in the cold posterior effect.", "Partitioned variational inference: A unified framework encompassing federated and continual learning": "Variational inference (VI) has become the method of choice for fitting many modern probabilistic models. However, practitioners are faced with a fragmented literature that offers a bewildering array of algorithmic options. First, the variational family. Second, the granularity of the updates e.g. whether the updates are local to each data point and employ message passing or global. Third, the method of optimization (bespoke or blackbox, closed-form or stochastic updates, etc.). This paper presents a new framework, termed Partitioned Variational Inference (PVI), that explicitly acknowledges these algorithmic dimensions of VI, unifies disparate literature, and provides guidance on usage. Crucially, the proposed PVI framework allows us to identify new ways of performing VI that are ideally suited to challenging learning scenarios including federated learning (where distributed computing is leveraged to process non-centralized data) and continual learning (where new data and tasks arrive over time and must be accommodated quickly). We showcase these new capabilities by developing communication-efficient federated training of Bayesian neural networks and continual learning for Gaussian process models with private pseudo-points. The new methods significantly outperform the state-of-the-art, whilst being almost as straightforward to implement as standard VI.", "On the expressiveness of approximate inference in bayesian neural networks": "While Bayesian neural networks (BNNs) hold the promise of being flexible, well-calibrated statistical models, inference often requires approximations whose consequences are poorly understood. We study the quality of common variational methods in approximating the Bayesian predictive distribution. For single-hidden layer ReLU BNNs, we prove a fundamental limitation in function-space of two of the most commonly used distributions defined in weight-space: mean-field Gaussian and Monte Carlo dropout. We find there are simple cases where neither method can have substantially increased uncertainty in between well-separated regions of low uncertainty. We provide strong empirical evidence that exact inference does not have this pathology, hence it is due to the approximation and not the model. In contrast, for deep networks, we prove a universality result showing that there exist approximate posteriors in the above classes which provide flexible uncertainty estimates. However, we find empirically that pathologies of a similar form as in the single-hidden layer case can persist when performing variational inference in deeper networks. Our results motivate careful consideration of the implications of approximate inference methods in BNNs.", "Infinite-horizon Gaussian processes": "Gaussian processes provide a flexible framework for forecasting, removing noise, and interpreting long temporal datasets. State space modelling (Kalman filtering) enables these non-parametric models to be deployed on long datasets by reducing the complexity to linear in the number of data points. The complexity is still cubic in the state dimension which is an impediment to practical application. In certain special cases (Gaussian likelihood, regular spacing) the GP posterior will reach a steady posterior state when the data are very long. We leverage this and formulate an inference scheme for GPs with general likelihoods, where inference is based on single-sweep EP (assumed density filtering). The infinite-horizon model tackles the cubic cost in the state dimensionality and reduces the cost in the state dimension to per data point. The model is extended to online-learning of hyperparameters. We show examples for large finite-length modelling problems, and present how the method runs in real-time on a smartphone on a continuous data stream updated at 100~Hz.", "Generating music by fine-tuning recurrent neural networks with reinforcement learning": "Supervised learning with next-step prediction is a common way to train a sequence prediction model; however, it suffers from known failure modes and is notoriously difficult to train models to learn certain properties, such as having a coherent global structure. Reinforcement learning can be used to impose arbitrary properties on generated data by choosing appropriate reward functions. In this paper we propose a novel approach for sequence training, where we refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from stochastic optimal control (SOC). We explore the usefulness of our approach in the context of music gener-ation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note-RNN is then refined using RL, where the reward function is a combination of rewards based on rules of music theory, as well as the output of another trained Note-RNN. We show that this combination of ML and RL can not only produce more pleasing melodies, but that it can significantly reduce unwanted behaviors and failure modes of the RNN.", "Modeling natural sounds with modulation cascade processes": "Natural sounds are structured on many time-scales. A typical segment of speech, for example, contains features that span four orders of magnitude: Sentences (\u223c 1 s); phonemes (\u223c 10\u2212 1 s); glottal pulses (\u223c 10\u2212 2 s); and formants (% 10\u2212 3 s). The auditory system uses information from each of these time-scales to solve complicated tasks such as auditory scene analysis [1]. One route toward understanding how auditory processing accomplishes this analysis is to build neuroscienceinspired algorithms which solve similar tasks and to compare the properties of these algorithms with properties of auditory processing. There is however a discord: Current machine-audition algorithms largely concentrate on the shorter time-scale structures in sounds, and the longer structures are ignored. The reason for this is two-fold. Firstly, it is a difficult technical problem to construct an algorithm that utilises both sorts of information. Secondly, it is computationally demanding to simultaneously process data both at high resolution (to extract short temporal information) and for long duration (to extract long temporal information). The contribution of this work is to develop a new statistical model for natural sounds that captures structure across a wide range of time-scales, and to provide efficient learning and inference algorithms. We demonstrate the success of this approach on a missing data task.", "Geometrically Coupled Monte Carlo Sampling.": "Monte Carlo sampling in high-dimensional, low-sample settings is important in many machine learning tasks. We improve current methods for sampling in Euclidean spaces by avoiding independence, and instead consider ways to couple samples. We show fundamental connections to optimal transport theory, leading to novel sampling algorithms, and providing new theoretical grounding for existing strategies. We compare our new strategies against prior methods for improving sample efficiency, including quasi-Monte Carlo, by studying discrepancy. We explore our findings empirically, and observe benefits of our sampling schemes for reinforcement learning and generative modelling.", "Occlusive components analysis": "We study unsupervised learning in a probabilistic generative model for occlusion. The model uses two types of latent variables: one indicates which objects are present in the image, and the other how they are ordered in depth. This depth order then determines how the positions and appearances of the objects present, specified in the model parameters, combine to form the image. We show that the object parameters can be learnt from an unlabelled set of images in which objects occlude one another. Exact maximum-likelihood learning is intractable. However, we show that tractable approximations to Expectation Maximization (EM) can be found if the training images each contain only a small number of objects on average. In numerical experiments it is shown that these approximations recover the correct set of object parameters. Experiments on a novel version of the bars test using colored bars, and experiments on more realistic data, show that the algorithm performs well in extracting the generating causes. Experiments based on the standard bars benchmark test for object learning show that the algorithm performs well in comparison to other recent component extraction approaches. The model and the learning algorithm thus connect research on occlusion with the research field of multiple-cause component extraction methods.", "Vowel normalisation: Time-domain processing of the internal dynamics of speech": "Human listeners can identify vowels regardless of speaker size, although the sound waves for an adult and a child speaking the'same'vowel would differ enormously. The differences are mainly due to differences in vocal tract length (VTL) and glottal pulse rate (GPR) which are both related to body size. ASR machines are notoriously bad at understanding children if they have been trained on the speech of an adult. In this paper, we propose that the auditory system adapts its analysis of speech sounds, dynamically and automatically to the GPR and VTL of the speaker on a syllable-to-syllable basis. In this paper, we illustrate how this rapid adaptation might be performed with the aid of a computational version of the auditory image model, and we propose that an auditory preprocessor of this form would improve the robustness of speech recognizers.", "Conservative uncertainty estimation by fitting prior networks": "Obtaining high-quality uncertainty estimates is essential for many applications of deep neural networks. In this paper, we theoretically justify a scheme for estimating uncertainties, based on sampling from a prior distribution. Crucially, the uncertainty estimates are shown to be conservative in the sense that they never underestimate a posterior uncertainty obtained by a hypothetical Bayesian algorithm. We also show concentration, implying that the uncertainty estimates converge to zero as we get more data. Uncertainty estimates obtained from random priors can be adapted to any deep network architecture and trained using standard supervised learning pipelines. We provide experimental evaluation of random priors on calibration and out-of-distribution detection on typical computer vision tasks, demonstrating that they outperform deep ensembles in practice.", "The gaussian process autoregressive regression model (gpar)": "Multi-output regression models must exploit dependencies between outputs to maximise predictive performance. The application of Gaussian processes (GPs) to this setting typically yields models that are computationally demanding and have limited representational power. We present the Gaussian Process Autoregressive Regression (GPAR) model, a scalable multi-output GP model that is able to capture nonlinear, possibly input-varying, dependencies between outputs in a simple and tractable way: the product rule is used to decompose the joint distribution over the outputs into a set of conditionals, each of which is modelled by a standard GP. GPAR\u2019s efficacy is demonstrated on a variety of synthetic and real-world problems, outperforming existing GP models and achieving state-of-the-art performance on established benchmarks.", "Development and validation of a spectro-temporal processing test for cochlear-implant listeners": "Psychophysical tests of spectro-temporal resolution may aid the evaluation of methods for improving hearing by cochlear implant (CI) listeners. Here the STRIPES (Spectro-Temporal Ripple for Investigating Processor EffectivenesS) test is described and validated. Like speech, the test requires both spectral and temporal processing to perform well. Listeners discriminate between complexes of sine sweeps which increase or decrease in frequency; difficulty is controlled by changing the stimulus spectro-temporal density. Care was taken to minimize extraneous cues, forcing listeners to perform the task only on the direction of the sweeps. Vocoder simulations with normal hearing listeners showed that the STRIPES test was sensitive to the number of channels and temporal information fidelity. An evaluation with CI listeners compared a standard processing strategy with one having very wide filters, thereby spectrally \u2026", "The geometry of random features": "We present an in-depth examination of the effectiveness of radial basis function kernel (beyond Gaussian) estimators based on orthogonal random feature maps. We show that orthogonal estimators outperform state-of-the-art mechanisms that use iid sampling under weak conditions for tails of the associated Fourier distributions. We prove that for the case of many dimensions, the superiority of the orthogonal transform can be accurately measured by a property we define called the charm of the kernel, and that orthogonal random features provide optimal (in terms of mean squared error) kernel estimators. We provide the first theoretical results which explain why orthogonal random features outperform unstructured on downstream tasks such as kernel ridge regression by showing that orthogonal random features provide kernel algorithms with better spectral properties than the previous state-of-the-art. Our results enable practitioners more generally to estimate the benefits from applying orthogonal transforms.", "The multivariate generalised von Mises distribution: inference and applications": "Circular variables arise in a multitude of data-modelling contexts ranging from robotics to the social sciences, but they have been largely overlooked by the machine learning community. This paper partially redresses this imbalance by extending some standard probabilistic modelling tools to the circular domain. First we introduce a new multivariate distribution over circular variables, called the multivariate Generalised von Mises (mGvM) distribution. This distribution can be constructed by restricting and renormalising a general multivariate Gaussian distribution to the unit hyper-torus. Previously proposed multivariate circular distributions are shown to be special cases of this construction. Second, we introduce a new probabilistic model for circular regression inspired by Gaussian Processes, and a method for probabilistic Principal Component Analysis with circular hidden variables. These models can leverage standard modelling tools (eg kernel functions and automatic relevance determination). Third, we show that the posterior distribution in these models is a mGvM distribution which enables development of an efficient variational free-energy scheme for performing approximate inference and approximate maximum-likelihood learning.", "An analysis of the size information in classical formant data: Peterson and Barney (1952) revisited": "Irino and Patterson (2002) have suggested the Mellin Transform as a model for vocal tract normalisation in the auditory system. In this report, we reanalyse the classical formant data reported by Peterson and Barney (1952) to see if it supports the normalisation hypothesis. The vowel formant data are clustered, quantitatively, using very general assumptions about speaker-variability. These clusters allow us to assess the degree to which vowel formant variability is attributable to changes in vocal tract length (VTL). The width of clusters associated with men, women and children within a given vowel cluster motivated consideration of a natural space in which to analyse scaled frequency components of sounds. By recasting Peterson and Barney\u2019s data into this new representation we are able to quantify the utility of scale normalisation.", "Training deep Gaussian processes using stochastic expectation propagation and probabilistic backpropagation": "Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are probabilistic and non-parametric and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. The focus of this paper is scalable approximate Bayesian learning of these networks. The paper develops a novel and efficient extension of probabilistic backpropagation, a state-of-the-art method for training Bayesian neural networks, that can be used to train DGPs. The new method leverages a recently proposed method for scaling Expectation Propagation, called stochastic Expectation Propagation. The method is able to automatically discover useful input warping, expansion or compression, and it is therefore is a flexible form of Bayesian kernel design. We demonstrate the success of the new method for supervised learning on several real-world datasets, showing that it typically outperforms GP regression and is never much worse.", "Probabilistic amplitude demodulation": "Auditory scene analysis is extremely challenging. One approach, perhaps that adopted by the brain, is to shape useful representations of sounds on prior knowledge about their statistical structure. For example, sounds with harmonic sections are common and so time-frequency representations are efficient. Most current representations concentrate on the shorter components. Here, we propose representations for structures on longer time-scales, like the phonemes and sentences of speech. We decompose a sound into a product of processes, each with its own characteristic time-scale. This demodulation cascade relates to classical amplitude demodulation, but traditional algorithms fail to realise the representation fully. A new approach, probabilistic amplitude demodulation, is shown to out-perform the established methods, and to easily extend to representation of a full demodulation cascade.", "Icebreaker: Element-wise active information acquisition with bayesian deep latent gaussian model": "In this paper we introduce the ice-start problem, i.e., the challenge of deploying machine learning models when only little or no training data is initially available, and acquiring each feature element of data is associated with costs. This setting is representative for the real-world machine learning applications. For instance, in the health-care domain, when training an AI system for predicting patient metrics from lab tests, obtaining every single measurement comes with a high cost. Active learning, where only the label is associated with a cost does not apply to such problem, because performing all possible lab tests to acquire a new training datum would be costly, as well as unnecessary due to redundancy. We propose Icebreaker, a principled framework to approach the ice-start problem. Icebreaker uses a full Bayesian Deep Latent Gaussian Model (BELGAM) with a novel inference method. Our proposed method combines recent advances in amortized inference and stochastic gradient MCMC to enable fast and accurate posterior inference. By utilizing BELGAM's ability to fully quantify model uncertainty, we also propose two information acquisition functions for imputation and active prediction problems. We demonstrate that BELGAM performs significantly better than the previous VAE (Variational autoencoder) based models, when the data set size is small, using both machine learning benchmarks and real-world recommender systems and health-care applications. Moreover, based on BELGAM, Icebreaker further improves the performance and demonstrate the ability to use minimum amount of the training data to obtain the highest test time \u2026", "Audiogram estimation using Bayesian active learning": "Two methods for estimating audiograms quickly and accurately using Bayesian active learning were developed and evaluated. Both methods provided an estimate of threshold as a continuous function of frequency. For one method, six successive tones with decreasing levels were presented on each trial and the task was to count the number of tones heard. A Gaussian Process was used for classification and maximum-information sampling to determine the frequency and levels of the stimuli for the next trial. The other method was based on a published method using a Yes/No task but extended to account for lapses. The obtained audiograms were compared to conventional audiograms for 40 ears, 19 of which were hearing impaired. The threshold estimates for the active-learning methods were systematically from 2 to 4\u2009dB below (better than) those for the conventional audiograms, which may indicate a less \u2026", "Instructions and Guide for Diagnostic Questions: The NeurIPS 2020 Education Challenge": "Digital technologies are becoming increasingly prevalent in education, enabling personalized, high quality education resources to be accessible by students across the world. Importantly, among these resources are diagnostic questions: the answers that the students give to these questions reveal key information about the specific nature of misconceptions that the students may hold. Analyzing the massive quantities of data stemming from students' interactions with these diagnostic questions can help us more accurately understand the students' learning status and thus allow us to automate learning curriculum recommendations. In this competition, participants will focus on the students' answer records to these multiple-choice diagnostic questions, with the aim of 1) accurately predicting which answers the students provide; 2) accurately predicting which questions have high quality; and 3) determining a personalized sequence of questions for each student that best predicts the student's answers. These tasks closely mimic the goals of a real-world educational platform and are highly representative of the educational challenges faced today. We provide over 20 million examples of students' answers to mathematics questions from Eedi, a leading educational platform which thousands of students interact with daily around the globe. Participants to this competition have a chance to make a lasting, real-world impact on the quality of personalized education for millions of students across the world.", "Denoising without access to clean data using a partitioned autoencoder": "Training a denoising autoencoder neural network requires access to truly clean data, a requirement which is often impractical. To remedy this, we introduce a method to train an autoencoder using only noisy data, having examples with and without the signal class of interest. The autoencoder learns a partitioned representation of signal and noise, learning to reconstruct each separately. We illustrate the method by denoising birdsong audio (available abundantly in uncontrolled noisy datasets) using a convolutional autoencoder.", "Efficient occlusive components analysis.": "We study unsupervised learning in a probabilistic generative model for occlusion. The model uses two types of latent variables: one indicates which objects are present in the image, and the other how they are ordered in depth. This depth order then determines how the positions and appearances of the objects present, specified in the model parameters, combine to form the image. We show that the object parameters can be learned from an unlabeled set of images in which objects occlude one another. Exact maximum-likelihood learning is intractable. Tractable approximations can be derived, however, by applying a truncated variational approach to Expectation Maximization (EM). In numerical experiments it is shown that these approximations recover the underlying set of object parameters including data noise and sparsity. Experiments on a novel version of the bars test using colored bars, and experiments on more realistic data, show that the algorithm performs well in extracting the generating components. The studied approach demonstrates that the multiple-causes generative approach can be generalized to extract occluding components, which links research on occlusion to the field of sparse coding approaches.", "VAEM: a deep generative model for heterogeneous mixed type data": "Deep generative models often perform poorly in real-world applications due to the heterogeneity of natural data sets. Heterogeneity arises from data containing different types of features (categorical, ordinal, continuous, etc.) and features of the same type having different marginal distributions. We propose an extension of variational autoencoders (VAEs) called VAEM to handle such heterogeneous data. VAEM is a deep generative model that is trained in a two stage manner such that the first stage provides a more uniform representation of the data to the second stage, thereby sidestepping the problems caused by heterogeneous data. We provide extensions of VAEM to handle partially observed data, and demonstrate its performance in data generation, missing data prediction and sequential feature selection tasks. Our results show that VAEM broadens the range of real-world applications where deep generative models can be successfully deployed.", "Pathologies of factorised gaussian and mc dropout posteriors in bayesian neural networks": "Neural networks provide state-of-the-art performance on a variety of tasks. However, they are often overconfident when making predictions. This inability to properly account for uncertainty limits their application to high-risk decision making, active learning and Bayesian optimisation. To address this, Bayesian inference has been proposed as a framework for improving uncertainty estimates. In practice, Bayesian neural networks rely on poorly understood approximations for computational tractability. We prove that two commonly used approximation methods, the factorised Gaussian assumption and Monte Carlo dropout, lead to pathological estimates of the predictive uncertainty in single hidden layer ReLU networks. This indicates that more flexible approximations are needed to obtain reliable uncertainty estimates.", "Meta-learning stationary stochastic process prediction with convolutional neural processes": "Stationary stochastic processes (SPs) are a key component of many probabilistic models, such as those for off-the-grid spatio-temporal data. They enable the statistical symmetry of underlying physical phenomena to be leveraged, thereby aiding generalization. Prediction in such models can be viewed as a translation equivariant map from observed data sets to predictive SPs, emphasizing the intimate relationship between stationarity and equivariance. Building on this, we propose the Convolutional Neural Process (ConvNP), which endows Neural Processes (NPs) with translation equivariance and extends convolutional conditional NPs to allow for dependencies in the predictive distribution. The latter enables ConvNPs to be deployed in settings which require coherent samples, such as Thompson sampling or conditional image completion. Moreover, we propose a new maximum-likelihood objective to replace the standard ELBO objective in NPs, which conceptually simplifies the framework and empirically improves performance. We demonstrate the strong performance and generalization capabilities of ConvNPs on 1D regression, image completion, and various tasks with real-world spatio-temporal data.", "Use of a deep recurrent neural network to reduce wind noise: effects on judged speech intelligibility and sound quality": "Despite great advances in hearing-aid technology, users still experience problems with noise in windy environments. The potential benefits of using a deep recurrent neural network (RNN) for reducing wind noise were assessed. The RNN was trained using recordings of the output of the two microphones of a behind-the-ear hearing aid in response to male and female speech at various azimuths in the presence of noise produced by wind from various azimuths with a velocity of 3\u2009m/s, using the \u201cclean\u201d speech as a reference. A paired-comparison procedure was used to compare all possible combinations of three conditions for subjective intelligibility and for sound quality or comfort. The conditions were unprocessed noisy speech, noisy speech processed using the RNN, and noisy speech that was high-pass filtered (which also reduced wind noise). Eighteen native English-speaking participants were tested, nine \u2026", "Design of covariance functions using inter-domain inducing variables": "We introduced the Gaussian Process Convolution Model (GPCM) in [1], a timeseries model for stationary signals based on the convolution between a continuoustime white-noise process and a continuous-time linear filter drawn from Gaussian process. The GPCM is, conditionally, itself a Gaussian process with a nonparametric kernel defined in a probabilistic fashion. Learning is achieved using a variational free-energy approach based on inter-domain inducing variables that summarise the (posterior) continuous-time linear filter and the driving white-noise process. However, the inter-domain transformation in [1] considers local averages of the noise process and therefore requires a large number inducing variables to represent underlying functions of complex spectra. In this paper, we develop an alternative transformation operating directly in the frequency domain, that retains the same modelling and predictive power as the original but requires fewer inducing variables and, consequently, has a reduced training time. The proposed approach is validated in a spectrum estimation task on a real-world time series. Note: this is a short version of [1] with additional results. For more technical details and experiments please refer to the original paper.", "Modelling of complex signals using Gaussian processes": "In complex-valued signal processing, estimation algorithms require complete knowledge (or accurate estimation) of the second order statistics, this makes Gaussian processes (GP) well suited for modelling complex signals, as they are designed in terms of covariance functions. Dealing with bivariate signals using GPs require four covariance matrices, or equivalently, two complex matrices. We propose a GP-based approach for modelling complex signals, whereby the second-order statistics are learnt through maximum likelihood; in particular, the complex GP approach allows for circularity coefficient estimation in a robust manner when the observed signal is corrupted by (circular) white noise. The proposed model is validated using climate signals, for both circular and noncircular cases. The results obtained open new possibilities for collaboration between the complex signal processing and Gaussian processes \u2026", "Stochastic variational inference for Gaussian process latent variable models using back constraints": "Gaussian process latent variable models (GPLVMs) are a probabilistic approach to modelling data that employs Gaussian process mapping from latent variables to observations. This paper revisits a recently proposed variational inference technique for GPLVMs and methodologically analyses the optimality and different parameterisations of the variational approximation. We investigate a structured variational distribution, that maintains information about the dependencies between hidden dimensions, and propose a mini-batch based stochastic training procedure, enabling more scalable training algorithm. This is achieved by using variational recognition models (also known as back constraints) to parameterise the variational approximation. We demonstrate the validity of our approach on a set of unsupervised learning tasks for texture images and handwritten digits.", "A hearing-model-based active-learning test for the determination of dead regions": "This article describes a Bayesian active-learning procedure for estimating the edge frequency, fe, of a dead region, that is, a region in the cochlea with no or very few functioning inner hair cells or neurons. The method is based on the psychophysical tuning curve (PTC) but estimates the shape of the PTC from the parameters of a hearing model, namely fe, and degree of outer hair cell loss. It chooses the masker frequency and level for each trial to be highly informative about the model parameters in the context of previous data. The procedure was tested using 14 ears from eight subjects previously diagnosed with high-frequency dead regions. The estimates of fe agreed well with estimates obtained using \u201cFast PTCs\u201d or more extensive measurements from an earlier study. On average, 33 trials were needed for the estimate of fe to fall and stay within 0.3 Cams of the final \u201ctrue\u201d value on the equivalent rectangular \u2026", "Neural network ensembles and variational inference revisited": "Ensembling methods and variational inference provide two orthogonal methods for obtaining reliable predictive uncertainty estimates for neural networks. In this work we compare and combine these approaches finding that: i) variational inference outperforms ensembles of neural networks, and ii) ensembled versions of variational inference bring further improvements. The first finding appears at odds with previous work (Lakshminarayanan et al., 2017), but we show that the previous results were due to an ambiguous experimental protocol in which the model and inference method were simultaneously changed.", "Inference and estimation in probabilistic time series models": "The term \u2018time series\u2019 refers to data that can be represented as a sequence. This includes for example \ufb01nancial data in which the sequence index indicates time, and genetic data (eg ACATGC...) in which the sequence index has no temporal meaning. In this tutorial we give an overview of discrete-time probabilistic models, which are the subject of most chapters in this book, with continuous-time models being discussed separately in Chapters 4, 6, 11 and 17. Throughout our focus is on the basic algorithmic issues underlying time series, rather than on surveying the wide \ufb01eld of applications. De\ufb01ning a probabilistic model of a time series y1, T 2 yl,..., yT requires the speci\ufb01cation of a joint distribution p (y1; T). 1 In general, specifying all independent entries of p (y1; T) is infeasible without making some statistical independence assumptions. For example, in the case of binary data, y, E {0, 1}, the joint distribution contains maximally 2T\u20141 indepen-dent entries. Therefore, for time series of more than a few time steps, we need to introduce simpli\ufb01cations in order to ensure tractability. One way to introduce statistical independence is to use the probability of a conditioned on observed b", "Independent subspace analysis for unsupervised learning of disentangled representations": "Recently there has been an increased interest in unsupervised learning of disentangled representations using the Variational Autoencoder (VAE) framework. Most of the existing work has focused largely on modifying the variational cost function to achieve this goal. We first show that these modifications, eg beta-VAE, simplify the tendency of variational inference to underfit, causing pathological over-pruning and over-orthogonalization of learned components. Second, we propose a complementary approach: to modify the probabilistic model with a structured latent prior. This prior discovers latent variable representations that are structured into a hierarchy of independent vector spaces. The proposed prior has three major advantages: First, in contrast to the standard VAE normal prior, the proposed prior is not rotationally invariant. This feature of our approach resolves the problem of unidentifiability of the standard VAE normal prior. Second, we demonstrate that the proposed prior encourages a disentangled latent representation which facilitates learning of disentangled representations. Third, extensive quantitative experiments demonstrate that the prior significantly mitigates the trade-off between reconstruction loss and disentanglement over the state of the art.", "Icebreaker: Element-wise efficient information acquisition with a bayesian deep latent gaussian model": "In this paper we introduce the ice-start problem, ie, the challenge of training machine learning models when only little or no training data is initially available, and acquiring each feature element of data is associated with costs. This setting is representative for real-world machine learning applications. For instance, in the health-care domain, when training an AI system for predicting patient metrics from lab tests, obtaining every single measurement comes with high cost. Active learning, where only the label is associated with a cost does not apply to such problem, because performing all possible lab tests to acquire a new training datum would be costly, as well as unnecessary due to redundancy. We propose Icebreaker, a principled framework to approach the ice-start problem. Icebreaker uses a full Bayesian Deep Latent Gaussian Model (BELGAM) with a novel inference method. Our proposed method combines recent advances in amortized inference and stochastic gradient MCMC to enable fast and accurate posterior inference. Utilizing BELGAM's ability to fully quantify model uncertainty, we also propose two information acquisition functions for imputation and active prediction problems. We demonstrate the excellent performance of Icebreaker in tasks relevant for recommender systems and health-care applications.", "STRFs in primary auditory cortex emerge from masking-based statistics of natural sounds": "We investigate how the neural processing in auditory cortex is shaped by the statistics of natural sounds. Hypothesising that auditory cortex (A1) represents the structural primitives out of which sounds are composed, we employ a statistical model to extract such components. The input to the model are cochleagrams which approximate the non-linear transformations a sound undergoes from the outer ear, through the cochlea to the auditory nerve. Cochleagram components do not superimpose linearly, but rather according to a rule which can be approximated using the max function. This is a consequence of the compression inherent in the cochleagram and the sparsity of natural sounds. Furthermore, cochleagrams do not have negative values. Cochleagrams are therefore not matched well by the assumptions of standard linear approaches such as sparse coding or ICA. We therefore consider a new encoding approach for natural sounds, which combines a model of early auditory processing with maximal causes analysis (MCA), a sparse coding model which captures both the non-linear combination rule and non-negativity of the data. An efficient truncated EM algorithm is used to fit the MCA model to cochleagram data. We characterize the generative fields (GFs) inferred by MCA with respect to in vivo neural responses in A1 by applying reverse correlation to estimate spectro-temporal receptive fields (STRFs) implied by the learned GFs. Despite the GFs being non-negative, the STRF estimates are found to contain both positive and negative subfields, where the negative subfields can be attributed to explaining away effects as captured by the \u2026", "Efficient low rank gaussian variational inference for neural networks": "OpenReview is a long-term project to advance science through improved peer review, with legal nonprofit status through Code for Science & Society. We gratefully acknowledge the support of the OpenReview Sponsors.", "Sample-then-optimize posterior sampling for bayesian linear models": "In modern machine learning it is common to train models which have an extremely high intrinsic capacity. The results obtained are often initialization dependent, are different for disparate optimizers and in some cases have no explicit regularization. This raises difficult questions about generalization [1]. A natural approach to questions of generalization is a Bayesian one. There is therefore a growing literature attempting to understand how Bayesian posterior inference could emerge from the complexity of modern practice [2, 3], even without having such a procedure as the stated goal.\nIn this work we consider a simple special case where exact Bayesian posterior sampling emerges from sampling (cf initialization) and then gradient descent. Specifically, for a Bayesian linear model, if we parameterize it in terms of a deterministic function of an isotropic normal prior, then the action of sampling from the prior followed by first order optimization of the squared loss will give a posterior sample. Although the assumptions are stronger than many real problems, it still exhibits the challenging properties of redundant model capacity and a lack of explicit regularizers, along with initialization and optimizer dependence. It is therefore an interesting controlled test case. Given its simplicity, the method itself may turn out to be of independent interest from our original goal. Whilst the material of Section 2 is classical [4], the material in Section 3 is, as far as we are aware, novel.", "Scalable exact inference in multi-output gaussian processes": "Multi-output Gaussian processes (MOGPs) leverage the flexibility and interpretability of GPs while capturing structure across outputs, which is desirable, for example, in spatio-temporal modelling. The key problem with MOGPs is their computational scaling , which is cubic in the number of both inputs (eg, time points or locations) and outputs . For this reason, a popular class of MOGPs assumes that the data live around a low-dimensional linear subspace, reducing the complexity to . However, this cost is still cubic in the dimensionality of the subspace , which is still prohibitively expensive for many applications. We propose the use of a sufficient statistic of the data to accelerate inference and learning in MOGPs with orthogonal bases. The method achieves linear scaling in in practice, allowing these models to scale to large without sacrificing significant expressivity or requiring approximation. This advance opens up a wide range of real-world tasks and can be combined with existing GP approximations in a plug-and-play way. We demonstrate the efficacy of the method on various synthetic and real-world data sets.", "Comparison of effects on subjective intelligibility and quality of speech in babble for two algorithms: A deep recurrent neural network and spectral subtraction": "The effects on speech intelligibility and sound quality of two noise-reduction algorithms were compared: a deep recurrent neural network (RNN) and spectral subtraction (SS). The RNN was trained using sentences spoken by a large number of talkers with a variety of accents, presented in babble. Different talkers were used for testing. Participants with mild-to-moderate hearing loss were tested. Stimuli were given frequency-dependent linear amplification to compensate for the individual hearing losses. A paired-comparison procedure was used to compare all possible combinations of three conditions. The conditions were: speech in babble with no processing (NP) or processed using the RNN or SS. In each trial, the same sentence was played twice using two different conditions. The participants indicated which one was better and by how much in terms of speech intelligibility and (in separate blocks) sound quality \u2026", "Online variational Bayesian inference: Algorithms for sparse Gaussian processes and theoretical bounds": "Sparse approximations for Gaussian process models provide a suite of methods that enable these models to be deployed in large data regime and enable analytic intractabilities to be sidestepped. However, the field lacks a principled method to handle streaming data, which are important for time-series analysis. The small number of existing approaches either use suboptimal hand-crafted heuristics for hyperparameter learning, or suffer from catastrophic forgetting or slow updating when new data arrive. This paper develops a new principled framework for deploying Gaussian process probabilistic models in the streaming setting, providing principled methods for learning hyperparameters and optimising pseudo-input locations. New theoretical bounds for general online variational Bayesian inference are also given and discussed in the paper.", "Statistical inference for single-and multi-band probabilistic amplitude demodulation": "Amplitude demodulation is an ill-posed problem and so it is natural to treat it from a Bayesian viewpoint, inferring the most likely carrier and envelope under probabilistic constraints. One such treatment is Probabilistic Amplitude Demodulation (PAD), which, whilst computationally more intensive than traditional approaches, offers several advantages. Here we provide methods for estimating the uncertainty in the PAD-derived envelopes and carriers, and for learning free parameters like the time-scale of the envelope. We show how the probabilistic approach can naturally handle noisy and missing data. Finally, we indicate how to extend the model to signals which contain multiple modulators and carriers.", "Rebuilding the limit order book: sequential Bayesian inference on hidden states": "The limit order book of an exchange represents an information store of market participants\u2019 future aims and for many traders the information held in this store is of interest. However, information loss occurs between orders being entered into the exchange and limit order book data being sent out. We present an online algorithm which carries out Bayesian inference to replace information lost at the level of the exchange server and apply our proof of concept algorithm to real historical data from some of the world\u2019s most liquid futures contracts as traded on CME GLOBEX, EUREX and NYSE Liffe exchanges.", "Comparison of relative and absolute judgments of speaker size based on vowel sounds": "Judgments of speaker size are largely determined by two acoustic variables: glottal pulse rate (GPR) and resonance scale (i.e., vocal tract length (VTL)). Both variables change with age (or height), but the rate is governed by different factors. The interaction of the variables was previously measured using absolute judgments of speaker size [D.R.R Smith and R.D. Patterson, J. Acoust. Soc. Am. 118, 3177-3186 (2005)]. The resulting size surface (over the GPR-VTL plane) bends down outside the normal range. In this paper, a method is developed for deriving the surface using size-discrimination data. In a two-alternative forced-choice experiment, listeners compared sequences of vowels scaled in GPR and VTL to represent speakers with slightly different sizes; they were required to choose the interval with the smaller speaker. Comparisons about a point in the plane reveal the gradient vector, and the vectors across \u2026", "Estimating vocal tract length from formant frequency data using a physical model and a latent variable factor analysis. P61": "\u2022 Fitch and Giedd (1999) used MRI to record the VTL dimensions, height and weight of 53 females and 76 males of different ages.\u27a2 VT shape\u2013the ratios of VT sections to the total VTL\u2013vary: the pharynx grows faster than the oral tract for men and women (Fig. 3).\u27a2 A non-uniform model of VT variability is required\u27a2 Bayesian methods show a linear model is sufficient:\u2022 Men and women differ only in their size (a). ret26@ cam. ac. uk tcw24@ cam. ac. uk rdp1@ cam. ac. uk", "Generalized variational continual learning": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "Sparse gaussian process variational autoencoders": "Large, multi-dimensional spatio-temporal datasets are omnipresent in modern science and engineering. An effective framework for handling such data are Gaussian process deep generative models (GP-DGMs), which employ GP priors over the latent variables of DGMs. Existing approaches for performing inference in GP-DGMs do not support sparse GP approximations based on inducing points, which are essential for the computational efficiency of GPs, nor do they handle missing data -- a natural occurrence in many spatio-temporal datasets -- in a principled manner. We address these shortcomings with the development of the sparse Gaussian process variational autoencoder (SGP-VAE), characterised by the use of partial inference networks for parameterising sparse GP approximations. Leveraging the benefits of amortised variational inference, the SGP-VAE enables inference in multi-output sparse GPs on previously unobserved data with no additional training. The SGP-VAE is evaluated in a variety of experiments where it outperforms alternative approaches including multi-output GPs and structured VAEs.", "Semi-supervised Bootstrapping of Dialogue State Trackers for Task Oriented Modelling": "Dialogue systems benefit greatly from optimizing on detailed annotations, such as transcribed utterances, internal dialogue state representations and dialogue act labels. However, collecting these annotations is expensive and time-consuming, holding back development in the area of dialogue modelling. In this paper, we investigate semi-supervised learning methods that are able to reduce the amount of required intermediate labelling. We find that by leveraging un-annotated data instead, the amount of turn-level annotations of dialogue state can be significantly reduced when building a neural dialogue system. Our analysis on the MultiWOZ corpus, covering a range of domains and topics, finds that annotations can be reduced by up to 30\\% while maintaining equivalent system performance. We also describe and evaluate the first end-to-end dialogue model created for the MultiWOZ corpus.", "VERSA: Versatile and efficient few-shot learning": "Despite recent advances in few-shot learning, notably in meta-learning based approaches [Ravi and Larochelle, 2017, Vinyals et al., 2016, Edwards and Storkey, 2017, Finn et al., 2017, Lacoste et al., 2018], there remains a lack of general purpose methods for flexible, data-efficient learning. This paper introduces VERSA, a system for data efficient and versatile meta-learning. It employs a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. VERSA substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training. We evaluate VERSA on benchmark datasets where the method achieves state-of-the-art results, handles arbitrary numbers of shots, and for classification, arbitrary numbers of classes at train and test time. The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task.", "On sparse variational methods and the Kullback-Leibler divergence between stochastic processes": "The variational framework for learning inducing variables (Titsias, 2009a) has had a large impact on the Gaussian process literature. The framework may be interpreted as minimizing a rigorously defined Kullback-Leibler divergence between the approximating and posterior processes. To our knowledge this connection has thus far gone unremarked in the literature. In this paper we give a substantial generalization of the literature on this topic. We give a new proof of the result for infinite index sets which allows inducing points that are not data points and likelihoods that depend on all function values. We then discuss augmented index sets and show that, contrary to previous works, marginal consistency of augmentation is not enough to guarantee consistency of variational inference with the original model. We then characterize an extra condition where such a guarantee is obtainable. Finally we show how our framework sheds light on interdomain sparse approximations and sparse approximations for Cox processes.", "Counterexamples to variational free energy compactness folk theorems": "Suppose we want to approximate some complicated distribution p (X) by a simpler distribution q (X). The approximating distribution can, for example, have a simpler parameteric form (eg Gaussian), or capture simpler dependencies (eg factored). One way of choosing the best approximation within some class, is to minimise the variational free-energy. It is then often the case that the approximating distribution, q (X) is more compact than the true distribution. A natural question is whether there in fact a theorem which says the \u201coptimized q (X) is always more compact than p (X)\u201d. Or can we find a counterexample? In this note we provide several such couterexamples which indicate that the compactness folk theorem is a useful rule of thumb, rather than a law of the cosmos.", "Combining Pseudo-Point and State Space Approximations for Sum-Separable Gaussian Processes": "Gaussian processes (GPs) are important probabilistic tools for inference and learning in spatio-temporal modelling problems such as those in climate science and epidemiology. However, existing GP approximations do not simultaneously support large numbers of off-the-grid spatial data-points and long time-series which is a hallmark of many applications. Pseudo-point approximations, one of the gold-standard methods for scaling GPs to large data sets, are well suited for handling off-the-grid spatial data. However, they cannot handle long temporal observation horizons effectively reverting to cubic computational scaling in the time dimension. State space GP approximations are well suited to handling temporal data, if the temporal GP prior admits a Markov form, leading to linear complexity in the number of temporal observations, but have a cubic spatial cost and cannot handle off-the-grid spatial data. In this work we show that there is a simple and elegant way to combine pseudo-point methods with the state space GP approximation framework to get the best of both worlds. The approach hinges on a surprising conditional independence property which applies to space--time separable GPs. We demonstrate empirically that the combined approach is more scalable and applicable to a greater range of spatio-temporal problems than either method on its own.", "The Gaussian Neural Process": "Neural Processes (NPs; Garnelo et al., 2018a,b) are a rich class of models for meta-learning that map data sets directly to predictive stochastic processes. We provide a rigorous analysis of the standard maximum-likelihood objective used to train conditional NPs. Moreover, we propose a new member to the Neural Process family called the Gaussian Neural Process (GNP), which models predictive correlations, incorporates translation equivariance, provides universal approximation guarantees, and demonstrates encouraging performance.", "HM-VAEs: a deep generative model for real-valued data with heterogeneous marginals": "In this paper, we propose a very simple but e ective VAE model (HM-VAE) that can handle real-valued data with heterogeneous marginals, meaning that they have drastically distinct marginal distributions, statistical properties as well as semantics. Preliminary results show that the HM-VAE can learn distributions with heterogeneous marginal distributions, whereas the vanilla VAEs fails.", "Differentially Private Federated Variational Inference": "In many real-world applications of machine learning, data are distributed across many clients and cannot leave the devices they are stored on. Furthermore, each client's data, computational resources and communication constraints may be very different. This setting is known as federated learning, in which privacy is a key concern. Differential privacy is commonly used to provide mathematical privacy guarantees. This work, to the best of our knowledge, is the first to consider federated, differentially private, Bayesian learning. We build on Partitioned Variational Inference (PVI) which was recently developed to support approximate Bayesian inference in the federated setting. We modify the client-side optimisation of PVI to provide an (, )-DP guarantee. We show that it is possible to learn moderately private logistic regression models in the federated setting that achieve similar performance to models trained non-privately on centralised data.", "Decomposing signals into a sum of amplitude and frequency modulated sinusoids using probabilistic inference": "There are many methods for decomposing signals into a sum of amplitude and frequency modulated sinusoids. In this paper we take a new estimation based approach. Identifying the problem as ill-posed, we show how to regularize the solution by imposing soft constraints on the amplitude and phase variables of the sinusoids. Estimation proceeds using a version of Kalman smoothing. We evaluate the method on synthetic and natural, clean and noisy signals, showing that it outperforms previous decompositions, but at a higher computational cost.", "The army of one (sample): the characteristics of sampling-based probabilistic neural representations": "There is growing evidence that humans and animals represent the uncertainty associated with sensory stimuli and utilize this uncertainty during planning and decision making in a statistically optimal way. Recently, a nonparametric framework for representing probabilistic information has been proposed whereby neural activity encodes samples from the distribution over external variables. Although such sample-based probabilistic representations have strong empirical and theoretical support, two major issues need to be clarified before they can be considered as viable candidate theories of cortical computation. First, in a fluctuating natural environment, can neural dynamics provide sufficient samples to accurately estimate a stimulus? Second, can such a code support accurate learning over biologically plausible time-scales? Although it is well known that sampling is statistically optimal if the number of samples is \u2026", "The panoramic ECAP method: estimating patient-specific patterns of current spread and neural health in cochlear implant users": "The knowledge of patient-specific neural excitation patterns from cochlear implants (CIs) can provide important information for optimizing efficacy and improving speech perception outcomes. The Panoramic ECAP (\u2018PECAP\u2019) method (Cosentino et al. 2015) uses forward-masked electrically evoked compound action-potentials (ECAPs) to estimate neural activation patterns of CI stimulation. The algorithm requires ECAPs be measured for all combinations of probe and masker electrodes, exploiting the fact that ECAP amplitudes reflect the overlapping excitatory areas of both probes and maskers. Here we present an improved version of the PECAP algorithm that imposes biologically realistic constraints on the solution, that, unlike the previous version, produces detailed estimates of neural activation patterns by modelling current spread and neural health along the intracochlear electrode array and is capable of \u2026", "Results and Insights from Diagnostic Questions: The NeurIPS 2020 Education Challenge": "This competition concerns educational diagnostic questions, which are pedagogically effective, multiple-choice questions (MCQs) whose distractors embody misconceptions. With a large and ever-increasing number of such questions, it becomes overwhelming for teachers to know which questions are the best ones to use for their students. We thus seek to answer the following question: how can we use data on hundreds of millions of answers to MCQs to drive automatic personalized learning in large-scale learning scenarios where manual personalization is infeasible? Success in using MCQ data at scale helps build more intelligent, personalized learning platforms that ultimately improve the quality of education en masse. To this end, we introduce a new, large-scale, real-world dataset and formulate 4 data mining tasks on MCQs that mimic real learning scenarios and target various aspects of the above question in a competition setting at NeurIPS 2020. We report on our NeurIPS competition in which nearly 400 teams submitted approximately 4000 submissions, with encouragingly diverse and effective approaches to each of our tasks.", "Convolutional conditional neural processes for local climate downscaling": "A new model is presented for multisite statistical downscaling of temperature and precipitation using convolutional conditional neural processes (convCNPs). ConvCNPs are a recently developed class of models that allow deep learning techniques to be applied to off-the-grid spatio-temporal data. This model has a substantial advantage over existing downscaling methods in that the trained model can be used to generate multisite predictions at an arbitrary set of locations, regardless of the availability of training data. The convCNP model is shown to outperform an ensemble of existing downscaling techniques over Europe for both temperature and precipitation taken from the VALUE intercomparison project. The model also outperforms an approach that uses Gaussian processes to interpolate single-site downscaling models at unseen locations. Importantly, substantial improvement is seen in the representation of extreme precipitation events. These results indicate that the convCNP is a robust downscaling model suitable for generating localised projections for use in climate impact studies, and motivates further research into applications of deep learning techniques in statistical downscaling.", "Hidden Markov models applied to intraday momentum trading with side information": "A Hidden Markov Model for intraday momentum trading is presented which specifies a latent momentum state responsible for generating the observed securities' noisy returns. Existing momentum trading models suffer from time-lagging caused by the delayed frequency response of digital filters. Time-lagging results in a momentum signal of the wrong sign, when the market changes trend direction. A key feature of this state space formulation, is no such lagging occurs, allowing for accurate shifts in signal sign at market change points. The number of latent states in the model is estimated using three techniques, cross validation, penalized likelihood criteria and simulation-based model selection for the marginal likelihood. All three techniques suggest either 2 or 3 hidden states. Model parameters are then found using Baum-Welch and Markov Chain Monte Carlo, whilst assuming a single (discretized) univariate Gaussian distribution for the emission matrix. Often a momentum trader will want to condition their trading signals on additional information. To reflect this, learning is also carried out in the presence of side information. Two sets of side information are considered, namely a ratio of realized volatilities and intraday seasonality. It is shown that splines can be used to capture statistically significant relationships from this information, allowing returns to be predicted. An Input Output Hidden Markov Model is used to incorporate these univariate predictive signals into the transition matrix, presenting a possible solution for dealing with the signal combination problem. Bayesian inference is then carried out to predict the securities return using the \u2026", "Consolidating the Meta-Learning Zoo: A Unifying Perspective as Posterior Predictive Inference": "A plethora of methods and approaches combining meta-learning [Schmidhuber, 1987, Thrun and Pratt, 2012] with deep neural networks have recently been proposed, achieving great success in applications such as few-shot learning. Much of the existing work may be characterized as either gradient-based [Finn et al., 2017, Ravi and Larochelle, 2017, Grant et al., 2018], metric-based [Vinyals et al., 2016, Snell et al., 2017], or amortized MAP based meta-learning [Gordon et al., 2018, Qiao et al., 2017]. Due to the ubiquity of recent work, a unifying view is useful for understanding and improving these methods. Existing frameworks [Finn et al., 2018, Grant et al., 2018] are limited to specific families of approaches, namely gradient-based methods [Ravi and Larochelle, 2017, Finn et al., 2017]. In this paper we develop a framework for meta-learning approximate probabilistic inference for prediction, or ML-PIP for short. ML-PIP provides this unifying perspective in terms of amortizing posterior predictive distributions. We show that ML-PIP re-frames and extends existing probabilistic interpretations of meta-learning [Finn et al., 2018, Grant et al., 2018] to cover both point-estimates and variational posteriors, as well as a broader class of methods, including gradient based meta-learning [Finn et al., 2017, Ravi and Larochelle, 2017], metric based meta-learning [Snell et al., 2017], amortized MAP inference [Qiao et al., 2017], and conditional probability modelling [Garnelo et al., 2018a, b].", "Perception of stochastic envelopes by normal-hearing and cochlear-implant listeners": "We assessed auditory sensitivity to three classes of temporal-envelope statistics (modulation depth, modulation rate, and comodulation) that are important for the perception of \u2018sound textures\u2019. The textures were generated by a probabilistic model that prescribes the temporal statistics of a selected number of modulation envelopes, superimposed onto noise carriers. Discrimination thresholds were measured for normal-hearing (NH) listeners and users of a MED-EL pulsar cochlear implant (CI), for separate manipulations of the average rate and modulation depth of the envelope in each frequency band of the stimulus, and of the co-modulation between bands. Normal-hearing (NH) listeners' discrimination of envelope rate was similar for baseline modulation rates of 5 and 34 Hz, and much poorer than previously reported for sinusoidally amplitude-modulated sounds. In contrast, discrimination of model parameters that \u2026", "Target Fishing: A Single-Label or Multi-Label Problem?": "According to Cobanoglu et al and Murphy, it is now widely acknowledged that the single target paradigm (one protein or target, one disease, one drug) that has been the dominant premise in drug development in the recent past is untenable. More often than not, a drug-like compound (ligand) can be promiscuous - that is, it can interact with more than one target protein. In recent years, in in silico target prediction methods the promiscuity issue has been approached computationally in different ways. In this study we confine attention to the so-called ligand-based target prediction machine learning approaches, commonly referred to as target-fishing. With a few exceptions, the target-fishing approaches that are currently ubiquitous in cheminformatics literature can be essentially viewed as single-label multi-classification schemes; these approaches inherently bank on the single target paradigm assumption that a ligand can home in on one specific target. In order to address the ligand promiscuity issue, one might be able to cast target-fishing as a multi-label multi-class classification problem. For illustrative and comparison purposes, single-label and multi-label Naive Bayes classification models (denoted here by SMM and MMM, respectively) for target-fishing were implemented. The models were constructed and tested on 65,587 compounds and 308 targets retrieved from the ChEMBL17 database. SMM and MMM performed differently: for 16,344 test compounds, the MMM model returned recall and precision values of 0.8058 and 0.6622, respectively; the corresponding recall and precision values yielded by the SMM model were 0.7805 and 0.7596 \u2026", "Learning complex tasks with probabilistic population codes": "Recent psychophysical experiments imply that the brain employs a neural representation of the uncertainty in sensory stimuli and that probabilistic computations are supported by the cortex. Several candidate neural codes for uncertainty have been posited including Probabilistic Population Codes (PPCs). PPCs support various versions of probabilistic inference and marginalisation in a neurally plausible manner. However, in order to establish whether PPCs can be of general use, three important limitations must be addressed. First, it is critical that PPCs support learning. For example, during cue combination, subjects are able to learn the uncertainties associated with the sensory cues as well as the prior distribution over the stimulus. However, previous modelling work with PPCs requires these parameters to be carefully set by hand. Second, PPCs must be able to support inference in non-linear models. Previous \u2026", "An amplitude modulation (AM) cascade model for nursery rhyme rhythm": "Nursery rhymes have a perfect rhythmic structure. Children\u2019s knowledge of rhymes predicts later phonology and reading (Maclean et al, 1987). What statistical relations in the speech signal capture rhythm patterns and are important for phonological development?", "The relative contribution of glottal pulse rate and vocal tract length in size discrimination judgements. P59": "FIGURE 5: The inferred size-surface (colour), with contours of constant size (solid lines). The gradients inferred previously from the experiment are shown in red again with their error-bars (red ellipses). The analysis routine fits an nth order polynomial to the partial derivatives at the centre of each ellipse. In this case a 5th order polynomial was used. The gradients of this surface are shown at each sample point (blue arrows) for comparison.", "Memory Efficient Meta-Learning with Large Images": "Meta learning approaches to few-shot classification are computationally efficient at test time, requiring just a few optimization steps or single forward pass to learn a new task, but they remain highly memory-intensive to train. This limitation arises because a task's entire support set, which can contain up to 1000 images, must be processed before an optimization step can be taken. Harnessing the performance gains offered by large images thus requires either parallelizing the meta-learner across multiple GPUs, which may not be available, or trade-offs between task and image size when memory constraints apply. We improve on both options by proposing LITE, a general and memory efficient episodic training scheme that enables meta-training on large tasks composed of large images on a single GPU. We achieve this by observing that the gradients for a task can be decomposed into a sum of gradients over the task's training images. This enables us to perform a forward pass on a task's entire training set but realize significant memory savings by back-propagating only a random subset of these images which we show is an unbiased approximation of the full gradient. We use LITE to train meta-learners and demonstrate new state-of-the-art accuracy on the real-world ORBIT benchmark and 3 of the 4 parts of the challenging VTAB+ MD benchmark relative to leading meta-learners. LITE also enables meta-learners to be competitive with transfer learning approaches but at a fraction of the test-time computational cost, thus serving as a counterpoint to the recent narrative that transfer learning is all you need for few-shot classification.", "Marginal Likelihood Gradient for Bayesian Neural Networks": "Bayesian learning of neural networks is attractive as it can protecting against over-fitting and provide automatic methods for inferring important hyperparameters by maximizing the marginal probability of the data. However, existing approaches in this vein, such as those based on variational inference, do not perform well. In this paper, we take a different approach and directly derive a practical estimator of the gradient of the marginal log-likelihood for BNNs by combining local reparametrization of the network wrt~ the prior distribution with the self-normalized importance sampling estimator. We show promising preliminary results on a toy example and on vectorized MNIST classification where the new method results in significantly improved performance of variational inference compared to existing approaches to tune hyperparameters.", "Application of Bayesian Active Learning to the Estimation of Auditory Filter Shapes Using the Notched-Noise Method": "Time-efficient hearing tests are important in both clinical practice and research studies. This particularly applies to notched-noise tests, which are rarely done in clinical practice because of the time required. Auditory-filter shapes derived from notched-noise data may be useful for diagnosis of the cause of hearing loss and for fitting of hearing aids, especially if measured over a wide range of center frequencies. To reduce the testing time, we applied Bayesian active learning (BAL) to the notched-noise test, picking the most informative stimulus parameters for each trial based on nine Gaussian Processes. A total of 11 hearing-impaired subjects were tested. In 20 to 30\u2009min, the test provided estimates of signal threshold as a continuous function of frequency from 500 to 4000\u2009Hz for nine notch widths and for notches placed both symmetrically and asymmetrically around the signal frequency. The thresholds were found \u2026", "Fast computation of loudness using a deep neural network": "The present paper introduces a deep neural network (DNN) for predicting the instantaneous loudness of a sound from its time waveform. The DNN was trained using the output of a more complex model, called the Cambridge loudness model. While a modern PC can perform a few hundred loudness computations per second using the Cambridge loudness model, it can perform more than 100,000 per second using the DNN, allowing real-time calculation of loudness. The root-mean-square deviation between the predictions of instantaneous loudness level using the two models was less than 0.5 phon for unseen types of sound. We think that the general approach of simulating a complex perceptual model by a much faster DNN can be applied to other perceptual models to make them run in real time.", "ISA-VAE: Independent subspace analysis with variational autoencoders": "Recent work has shown increased interest in using the Variational Autoencoder (VAE) framework to discover interpretable representations of data in an unsupervised way. These methods have focussed largely on modifying the variational cost function to achieve this goal. However, we show that methods like beta-VAE simplify the tendency of variational inference to underfit causing pathological over-pruning and over-orthogonalization of learned components. In this paper we take a complementary approach: to modify the probabilistic model to encourage structured latent variable representations to be discovered. Specifically, the standard VAE probabilistic model is unidentifiable: the likelihood of the parameters is invariant under rotations of the latent space. This means there is no pressure to identify each true factor of variation with a latent variable. We therefore employ a rich prior distribution, akin to the ICA model, that breaks the rotational symmetry. Extensive quantitative and qualitative experiments demonstrate that the proposed prior mitigates the trade-off introduced by modified cost functions like beta-VAE and TCVAE between reconstruction loss and disentanglement. The proposed prior allows to improve these approaches with respect to both disentanglement and reconstruction quality significantly over the state of the art.", "Learning Causally-Generated Stationary Time Series": "We present the Causal Gaussian Process Convolution Model (CGPCM), a doubly nonparametric model for causal, spectrally complex dynamical phenomena. The CGPCM is a generative model in which white noise is passed through a causal, nonparametric-window moving-average filter, a construction that we show to be equivalent to a Gaussian process with a nonparametric kernel that is biased towards causally-generated signals. We develop enhanced variational inference and learning schemes for the CGPCM and its previous acausal variant, the GPCM (Tobar et al., 2015b), that significantly improve statistical accuracy. These modelling and inferential contributions are demonstrated on a range of synthetic and real-world signals.", "The multivariate Generalised von Mises Distribution": "Objective\nDevelop models and inference algorithms for circular data, ie data points that lives on the surface of (hyper)-spheres and (hyper)-toruses", "Understanding Expectation Propagation": "Understanding and characterising the properties of approximate inference schemes is extremely important, but arguably under studied. This report continues work on characterising Expectation Propagation (EP), an approximate Bayesian inference scheme, looking at four toy cases of interest. We initially focus on the empirically motivated conjecture stating that EP\u2019s approximation for the model evidence is an underestimate of the true model evidence. The first two toy cases apply EP to a simple classification example. They indicate why EP tends to underestimate the model evidence on realistic datasets, even though there are counter-examples to the conjecture, which we show analytically for the first time. The third toy case uses the link between the Fully Independent Training Condition algorithm (FITC, a sparse approximation method for Gaussian Process regression) and EP to find another analytic counter-example. This toy case also raises interesting questions as to how and why FITC works, which we consider mathematically. The final toy example compares mean field EP to mean field and structured Variational Inference (VI) on a small time-series model. We find that EP\u2019s uncertainty estimates do not collapse pathologically as they do for mean field VI.", "Modelling time series via automatic learning of basis functions": "We present a model for time series consisting of an infinite mixture of basis functions, whereby the bases and the mixing process are modelled as posterior means of latent Gaussian processes (GPs). Conditional to observed data, the bases and the mixing process are learnt using a parametric approximation based on pseudo-observations, where the complexity and accuracy of the method are controlled by the number of pseudo-observations (N x and N h ). The resulting model is linear the pseudo-observations, and its likelihood function has a complexity O(NN h N x ), N x <; N, N h \u226a N, which is lower than that of the standard GP O(N 3 ) - where N is the number of observations. We validate the proposed approach using synthetic data, where we recovered latent GPs with five different kernels from noisy observations; and using a real-world heart-rate signal to assess the proposed model's computational complexity \u2026", "A Unifying Approximate Inference Framework from Variational Free Energy Relaxation": "This short paper extends the free-energy derivations of variational inference, loopy belief propagation and expectation propagation (EP) to a wider range of approximate inference methods including power EP, distributed EP, and black-box alphadivergence minimisation. The framework provides a very flexible framework for the design of variational algorithms that can mix versions of any of the aforementioned algorithms inside a single coherent algorithm. The framework is general, extending to latent variable models, for example.", "Importance weighted autoencoders with random neural network parameters": "Deep generative models for unsupervised learning have recently received considerable attention [1, 2, 3, 4, 5, 6]. These models can find a set of low-dimensional representative latent features that can accurately describe observed data. Furthermore, they can also infer the underlying mechanism that generates, from these features, new data instances similar to the observed ones. In general, these models need to perform posterior inference during learning, a task that is carried out by training, in addition to the top-down generative network, a bottom-up recognition network. This recognition network is used to predict the posterior distribution of the latent variables given the observed ones.\nVariational autoencoders (VAEs) are a family of generative models in which the parameters of the generative network and the recognition network are optimized during training to maximize a lower bound on the log-likelihood [3, 5, 6]. The VAE in [3] defines a generative process p (xi| zi, \u03b8) for the observed variables of the i-th data instance, xi, given the corresponding latent variables zi. p (xi| zi, \u03b8) is set to be a factorized Gaussian distribution (or a product of Bernoulli distributions in the case of binary data) whose mean and variance is computed by a deterministic feed-forward neural network with parameters \u03b8. The recognition network computes q (zi| xi, \u03c6), an approximation to p (zi| xi). q (zi| xi, \u03c6) is also a factorized Gaussian whose mean and variance is also computed by a feed-forward network with parameters \u03c6. The prior for each zi, p (zi), is set to be a product of standard Gaussians. During training, \u03b8 and \u03c6 are found by maximizing the lower bound", "Stochastic Expectation Propagation for Large Scale Gaussian Process Classification": "A method for large scale Gaussian process classification has been recently proposed based on expectation propagation (EP). Such a method allows Gaussian process classifiers to be trained on very large datasets that were out of the reach of previous deployments of EP and has been shown to be competitive with related techniques based on stochastic variational inference. Nevertheless, the memory resources required scale linearly with the dataset size, unlike in variational methods. This is a severe limitation when the number of instances is very large. Here we show that this problem is avoided when stochastic EP is used to train the model.", "A causal perspective on domain adaptation": "From training data from several related domains (or tasks), methods of domain adaptation try to combine knowledge to improve performance. This paper discusses an approach to domain adaptation which is inspired by a causal interpretation of the multi-task problem. We assume that a covariate shift assumption holds true for a subset of predictor variables: the conditional of the target variable given this subset of predictors is invariant with respect to shifts in those predictors (covariates). We propose to learn the corresponding conditional expectation in the training domains and use it for estimation in the target domain. We further introduce a method which allows for automatic inference of the above subset in regression and classification. We study the performance of this approach in an adversarial setting, in the case where no additional examples are available in the test domain. If a labeled sample is available, we provide a method for using both the transferred invariant conditional and task specific information. We present results on synthetic data sets and a sentiment analysis problem.", "The 1d Kalman Filter": "This is a Jekyll and Hyde of a document and should really be split up. We start with Jekyll which contains a very short derivation for the 1d Kalman filter, the purpose of which is to give intuitions about its more complex cousin. I find the Kalman filter/linear Gaussian state space model thing tough to inutit. This is an attempt to remedy this. Hyde can be found afterwards-here I show the connection between linear Gaussian state space models to Gaussian processes and the Wiener filter. This document is missing some pictures and references, apologies for that.", "Collapsed Variational Bounds for Bayesian Neural Networks": "Recent interest in learning large variational Bayesian Neural Networks (BNNs) has been partly hampered by poor predictive performance caused by underfitting, and their performance is known to be very sensitive to the prior over weights. Current practice often fixes the prior parameters to standard values or tunes them using heuristics or cross-validation. In this paper, we treat prior parameters in a distributional way by extending the model and collapsing the variational bound with respect to their posteriors. This leads to novel and tighter Evidence Lower Bounds (ELBOs) for performing variational inference (VI) in BNNs. Our experiments show that the new bounds significantly improve the performance of Gaussian mean-field VI applied to BNNs on a variety of data sets, demonstrating that mean-field VI works well even in deep models. We also find that the tighter ELBOs can be good optimization targets for learning the hyperparameters of hierarchical priors.", "Variational auto encoder for mixed data types": "In a first stage, training each of a plurality of first variational auto encoders, VAEs, each comprising: a respective first encoder arranged to encode a respective subset of one or more features of a feature space into a respective first latent representation, and a respective first decoder arranged to decode from the respective latent representation back to a decoded version of the respective subset of the feature space, wherein different subsets comprise features of different types of data. In a second stage following the first stage, training a second VAE comprising: a second encoder arranged to encode a plurality of inputs into a second latent representation, and a second decoder arranged to decode the second latent representation into decoded versions of the first latent representations, wherein each of the plurality of inputs comprises a combination of a different respective one of feature subsets with the respective first \u2026", "Efficient Gaussian Neural Processes for Regression": "Conditional Neural Processes (CNP; Garnelo et al., 2018) are an attractive family of meta-learning models which produce well-calibrated predictions, enable fast inference at test time, and are trainable via a simple maximum likelihood procedure. A limitation of CNPs is their inability to model dependencies in the outputs. This significantly hurts predictive performance and renders it impossible to draw coherent function samples, which limits the applicability of CNPs in down-stream applications and decision making. Neural Processes (NPs; Garnelo et al., 2018) attempt to alleviate this issue by using latent variables, relying on these to model output dependencies, but introduces difficulties stemming from approximate inference. One recent alternative (Bruinsma et al., 2021), which we refer to as the FullConvGNP, models dependencies in the predictions while still being trainable via exact maximum-likelihood. Unfortunately, the FullConvGNP relies on expensive 2D-dimensional convolutions, which limit its applicability to only one-dimensional data. In this work, we present an alternative way to model output dependencies which also lends itself maximum likelihood training but, unlike the FullConvGNP, can be scaled to two- and three-dimensional data. The proposed models exhibit good performance in synthetic experiments.", "Continual Novelty Detection": "Novelty Detection methods identify samples that are not representative of a model's training set thereby flagging misleading predictions and bringing a greater flexibility and transparency at deployment time. However, research in this area has only considered Novelty Detection in the offline setting. Recently, there has been a growing realization in the computer vision community that applications demand a more flexible framework - Continual Learning - where new batches of data representing new domains, new classes or new tasks become available at different points in time. In this setting, Novelty Detection becomes more important, interesting and challenging. This work identifies the crucial link between the two problems and investigates the Novelty Detection problem under the Continual Learning setting. We formulate the Continual Novelty Detection problem and present a benchmark, where we compare several Novelty Detection methods under different Continual Learning settings. We show that Continual Learning affects the behaviour of novelty detection algorithms, while novelty detection can pinpoint insights in the behaviour of a continual learner. We further propose baselines and discuss possible research directions. We believe that the coupling of the two problems is a promising direction to bring vision models into practice.", "Attacking Few-Shot Classifiers with Adversarial Support Poisoning": "This paper examines the robustness of deployed few-shot meta-learning systems when they are fed an imperceptibly perturbed few-shot dataset, showing that the resulting predictions on test inputs can become worse than chance. This is achieved by developing a novel attack, Adversarial Support Poisoning or ASP, which crafts a poisoned set of examples. When even a small subset of malicious data points is inserted into the support set of a meta-learner, accuracy is significantly reduced. We evaluate the new attack on a variety of few-shot classification algorithms and scenarios, and propose a form of adversarial training that significantly improves robustness against both poisoning and evasion attacks.", "How Tight Can PAC-Bayes be in the Small Data Regime?": "In this paper, we investigate the question: Given a small number of datapoints, for example N = 30, how tight can PAC-Bayes and test set bounds be made? For such small datasets, test set bounds adversely affect generalisation performance by discarding data. In this setting, PAC-Bayes bounds are especially attractive, due to their ability to use all the data to simultaneously learn a posterior and bound its generalisation risk. We focus on the case of i.i.d. data with a bounded loss and consider the generic PAC-Bayes theorem of Germain et al. (2009) and Begin et al. (2016). While their theorem is known to recover many existing PAC-Bayes bounds, it is unclear what the tightest bound derivable from their framework is. Surprisingly, we show that for a fixed learning algorithm and dataset, the tightest bound of this form coincides with the tightest bound of the more restrictive family of bounds considered in Catoni (2007). In contrast, in the more natural case of distributions over datasets, we give examples (both analytic and numerical) showing that the family of bounds in Catoni (2007) can be suboptimal. Within the proof framework of Germain et al. (2009) and Begin et al. (2016), we establish a lower bound on the best bound achievable in expectation, which recovers the Chernoff test set bound in the case when the posterior is equal to the prior. Finally, to illustrate how tight these bounds can potentially be, we study a synthetic one-dimensional classification task in which it is feasible to meta-learn both the prior and the form of the bound to obtain the tightest PAC-Bayes and test set bounds possible. We find that in this simple, controlled scenario, PAC \u2026", "Contextual HyperNetworks for Novel Feature Adaptation": "While deep learning has obtained state-of-the-art results in many applications, the adaptation of neural network architectures to incorporate new output features remains a challenge, as neural networks are commonly trained to produce a fixed output dimension. This issue is particularly severe in online learning settings, where new output features, such as items in a recommender system, are added continually with few or no associated observations. As such, methods for adapting neural networks to novel features which are both time and data-efficient are desired. To address this, we propose the Contextual HyperNetwork (CHN), an auxiliary model which generates parameters for extending the base model to a new feature, by utilizing both existing data as well as any observations and/or metadata associated with the new feature. At prediction time, the CHN requires only a single forward pass through a neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. To assess the performance of CHNs, we use a CHN to augment a partial variational autoencoder (P-VAE), a deep generative model which can impute the values of missing features in sparsely-observed data. We show that this system obtains improved few-shot learning performance for novel features over existing imputation and meta-learning baselines across recommender systems, e-learning, and healthcare tasks.", "Collecting observations for machine learning": "A method of training a model comprising a generative network mapping a latent vector to a feature vector, wherein weights in the generative network are modelled as probabilistic distributions. The method comprises: a) obtaining one or more observed data points, each comprising an incomplete observation of the features in the feature vector; b) training the model based on the observed data points to learn values of the weights of the generative network which map the latent vector to the feature vector; c) from amongst a plurality of potential next features to observe, searching for a target feature of the feature vector which maximizes a measure of expected reduction in uncertainty in a distribution of said weights of the generative network given the observed data points so far; and d) outputting a request to collect a target data point comprising at least the target feature.", "The Gaussian Process Latent Autoregressive Model": "Many real-world prediction problems involve modelling the dependencies between multiple different outputs across the input space. Multi-output Gaussian Processes (MOGP) are a particularly important approach to such problems. In this paper, we build on the Gaussian Process Autoregressive Regression (GPAR) model which is one of the best performing MOGP models, but which fails when observation noise is large, when there are missing data, and when non-Gaussian observation models are required. We extend the original GPAR model to handle these settings and provide a variational inference procedure similar to that used in deep Gaussian Processes which replaces the ad hoc denoising approximation used in the original work. We show that the new approach naturally handles noisy outputs, missing data and that it also enables the model to handle heterogeneous non-Gaussian observation models.", "Attacking Few-Shot Classifiers with Adversarial Support Sets": "Few-shot learning systems, especially those based on meta-learning, have recently made significant advances, and are now being considered for real world problems in healthcare, personalization, and science. In this paper, we examine the robustness of such deployed few-shot learning systems when they are fed an imperceptibly perturbed few-shot dataset, showing that the resulting predictions on test inputs can become worse than chance. This is achieved by developing a novel Adversarial Support Set Attack which crafts a poisoned set of examples. When even a small subset of malicious data points is inserted into the support set of a meta-learner, accuracy is significantly reduced. For example, the average classification accuracy of CNAPs on the Aircraft dataset in the META-DATASET benchmark drops from 69.2% to 9.1% when only 20% of the support set is poisoned by imperceptible perturbations. We evaluate the new attack on a variety of few-shot classification algorithms including MAML, prototypical networks, and CNAPs, on both small scale (miniImageNet) and large scale (META-DATASET) few-shot classification problems. Interestingly, adversarial support sets produced by attacking a meta-learning based few-shot classifier can also reduce the accuracy of a fine-tuning based few-shot classifier when both models use similar feature extractors.", "Development of a Deep Neural Network for Speeding Up a Model of Loudness for Time-Varying Sounds": "The \u201ctime-varying loudness\u201d (TVL) model of Glasberg and Moore calculates \u201cinstantaneous loudness\u201d every 1\u2009ms, and this is used to generate predictions of short-term loudness, the loudness of a short segment of sound, such as a word in a sentence, and of long-term loudness, the loudness of a longer segment of sound, such as a whole sentence. The calculation of instantaneous loudness is computationally intensive and real-time implementation of the TVL model is difficult. To speed up the computation, a deep neural network (DNN) was trained to predict instantaneous loudness using a large database of speech sounds and artificial sounds (tones alone and tones in white or pink noise), with the predictions of the TVL model as a reference (providing the \u201ccorrect\u201d answer, specifically the loudness level in phons). A multilayer perceptron with three hidden layers was found to be sufficient, with more complex DNN \u2026", "Interpreting Spatially Infinite Generative Models": "Traditional deep generative models of images and other spatial modalities can only generate fixed sized outputs. The generated images have exactly the same resolution as the training images, which is dictated by the number of layers in the underlying neural network. Recent work has shown, however, that feeding spatial noise vectors into a fully convolutional neural network enables both generation of arbitrary resolution output images as well as training on arbitrary resolution training images. While this work has provided impressive empirical results, little theoretical interpretation was provided to explain the underlying generative process. In this paper we provide a firm theoretical interpretation for infinite spatial generation, by drawing connections to spatial stochastic processes. We use the resulting intuition to improve upon existing spatially infinite generative models to enable more efficient training through a model that we call an infinite generative adversarial network, or -GAN. Experiments on world map generation, panoramic images and texture synthesis verify the ability of -GAN to efficiently generate images of arbitrary size.", "Combining Variational Continual Learning with FiLM Layers": "The standard architecture for continual learning is a multi-headed neural network, which has shared body parameters and task-specific heads. Features for each task are generated in the same way. This could be too restrictive, particularly when tasks are very distinct. We propose combining FiLM layers, a flexible way to enable task-specific feature modulation in CNNs, with an existing algorithm, Variational Continual Learning (VCL). We show that this addition consistently improves performance, particularly when tasks are more varied. Furthermore, we demonstrate how FiLM Layers can mitigate VCL's tendency to over-prune and help it use more model capacity. Finally, we find that FiLM Layers perform feature modulation as opposed to gating, making them more flexible than binary mask based approaches.", "The implementation of efficient hearing tests using machine learning": "Time-efficient hearing tests are important in both clinical practice and research studies. Bayesian active learning (BAL) methods were first proposed in the 1990s. We developed BAL methods for measuring the audiogram, conducting notched-noise tests, determination of the edge frequency of a dead region (fe), and estimating equal-loudness contours. The methods all use a probabilistic model of the outcome, which can be classification (audible/inaudible), regression (loudness) or model parameters (fe, outer hair cell loss at fe). The stimulus parameters for the next trial (eg frequency, level) are chosen to yield maximum reduction in the uncertainty of the parameters of the probabilistic model. The approach reduced testing time by a factor of about 5 and, for some tests, yielded results on a continuous frequency scale. For example, auditory filter shapes can be estimated for centre frequencies from 500 to 4000 Hz in 20-30 minutes. The probabilistic modelling allows quantitative comparison of different methods. For audiogram determination, asking subjects to count the number of audible tones in a sequence with decreasing level was slightly more efficient than requiring Yes/No responses. Counting tones yielded higher variance for a single response, but this was offset by the higher information per trial.", "Using a deep neural network to speed up a model of loudness for time-varying sounds": "The \u201ctime-varying loudness (TVL)\u201d model calculates \u201cinstantaneous loudness\u201d every 1 ms, and this is used to generate predictions of short-term loudness, the loudness of a short segment of sound such as a word in a sentence, and of long-term loudness, the loudness of a longer segment of sound, such as a whole sentence. The calculation of instantaneous loudness is computationally intensive and real-time implementation of the TVL model is difficult. To speed up the computation, a deep neural network (DNN) has been trained to predict instantaneous loudness using a large database of speech sounds and artificial sounds (tones alone and tones in white or pink noise), with the predictions of the TVL model as a reference (providing the\" correct\" answer, specifically the loudness level in phons). A multilayer perceptron with three hidden layers was found to be sufficient, with more complex DNN architecture not yielding higher accuracy. After training, the deviations between the predictions of the TVL model and the predictions of the DNN were typically less than 0.5 phons, even for types of sounds that were not used for training (music, rain, animal sounds, washing machine). The DNN calculates instantaneous loudness over 100 times more quickly than the TVL model.", "Interpretable Continual Learning": "We present a framework for interpretable continual learning (ICL). We show that explanations of previously performed tasks can be used to improve performance on future tasks. ICL generates a good explanation of a finished task, then uses this to focus attention on what is important when facing a new task. The ICL idea is general and may be applied to many continual learning approaches. Here we focus on the variational continual learning framework to take advantage of its flexibility and efficacy in overcoming catastrophic forgetting. We use saliency maps to provide explanations of performed tasks and propose a new metric to assess their quality. Experiments show that ICL achieves state-of-the-art results in terms of overall continual learning performance as measured by average classification accuracy, and also in terms of its explanations, which are assessed qualitatively and quantitatively using the proposed metric.", "Estimation of auditory filter shapes across frequencies using machine learning": "When fitting a hearing aid, the level-dependent gain prescribed at each frequency is usually based on the hearing loss at that frequency. This often results in reasonable fittings for a typical cochlear hearing loss, but may fail when the individual frequency selectivity and/or loudness growth are different from what would be typical for that hearing loss. Individualised fitting based on measures of frequency selectivity might be useful in improving a fitting, for example by reducing across-channel masking. A popular measure of frequency selectivity is the notched-noise method, but this test is time-consuming. To reduce testing time, Shen and Richards (2013) proposed an efficient machine-learning test that determines the slope of the skirts of the auditory filter (p), its minimum response for wide notches (r), and detection efficiency (K). However, their test did not determine asymmetries in the auditory filter, which are important to consider during fitting to reduce across-channel masking. The test proposed here provides a time-efficient way of estimating the auditory filter shape and asymmetry as a function of center frequency. The noise level required for threshold is estimated for a tone with frequency fs presented at 15 dB SL in nine symmetric or asymmetric notched noises with notch edge frequencies between 0.6 and 1.4 fs. Using only narrow to medium notch widths provides good information about the tip of the auditory filter, which is of most importance in determining across-channel masking for speech-like signals (but the tail is not well defined). The nine thresholds for a given fs can be used to fit an auditory filter model with three parameters: the slopes \u2026", "A fast test for determining the edge frequency of a dead region": "The presence and edge frequency, fe, of a dead region in the cochlea can be diagnosed using psychophysical tuning curves (PTCs). When the signal frequency, fs, falls in a dead region, the tip of the PTC lies close to fe, rather than close to fs. However, measurement of PTCs is time consuming, limiting their application in clinical practice. We have developed a fast test based on Bayesian active learning. Instead of estimating an entire PTC, we estimate parameters of an individual hearing model, including fe. The task is to detect a fixed signal in the presence of a masker whose level and frequency vary across trials. After each trial, the next masker level and frequency are chosen to produce maximum reduction of the uncertainty about the parameters. The results for four participants tested so far were close to those obtained using \u201cfast\u201d PTCs. The Bayesian procedure has two advantages compared to PTCs: it allows \u2026", "Sparse Approximations for Non-Conjugate Gaussian Process Regression": "Notes: This report only shows some preliminary work on scaling Gaussian process models that use non-Gaussian likelihoods. As there are recently arxived papers on the similar idea [1, 2], this report will stay as is, please consult the two papers above for a proper discussion and experiments.", "Audio time-frequency analysis as probabilistic inference: supplementary material": "This report fills in the technical details of the paper entitled,\u201cAudio time-frequency analysis as probabilistic inference\u201d and references Matlab implementations of the algorithms available from the git repository at http://learning. eng. cam. ac. uk/Public/Turner/Resouces/GTF-tNMF/. For a high level discussion of the ideas, see the main article. The sections have been written to be self-contained so that readers can dip into single sections, but this has resulted in some repetition for committed readers.", "On the paper: Variational Learning of Inducing Variables in Sparse Gaussian Processes (Titsias, 2009)": "This summary was prepared for our internal reading club and serves as notes on the sparse GP regression using the variational method (Titsias, 2009). We also discuss why this approximation can be viewed as the corrected version of the Projected Process or Deterministic Training Conditional (DTC) approximation (Seeger, 2003).", "Spoken Nursery Rhymes Have a Fractal Rhythmic Structure-Evidence from Patterns of Slow Amplitude Modulation (AM)": "UC Merced Page 1 UC Merced Proceedings of the Annual Meeting of the Cognitive Science Society Title Spoken Nursery Rhymes Have a Fractal Rhythmic Structure - Evidence from Patterns of Slow Amplitude Modulation (AM) Permalink https://escholarship.org/uc/item/3cv587w6 Journal Proceedings of the Annual Meeting of the Cognitive Science Society, 33(33) ISSN 1069-7977 Authors Leong, Victoria Turner, Richard Stone, Michael et al. Publication Date 2011 Peer reviewed eScholarship.org Powered by the California Digital Library University of California Page 2 Spoken Nursery Rhymes Have a Fractal Rhythmic Structure Evidence from Patterns of Slow Amplitude Modulation (AM) Victoria Leong University of Cambridge, Department of Experimental Psychology, Centre for Neuroscience in Education Richard Turner University of Cambridge, Department of Engineering, Computational & Biological Learning Lab \u2026", "Probabilistic auditory scene analysis from natural statistics": "\u2022 Gestalt Psychology proposes a list of rules\u2022 Computational Auditory Scene Analysis uses these rules as heuristics for source segregation\u2022 GOAL: Develop a generative model of auditory scene statistics, in which inference reproduces the Gestalt rules", "Modeling Sounds with Gaussian Modulation Cascade Processes": "Method: Devise a statistical model for natural sounds that captures their structure in latent variables.", "Simple and complex cells as style and content variables in a bilinear model based on temporal stability": "A promising approach to the study of representation in the visual system is based on the idea that neuronal response properties reflect an internal model of the statistical structure of the environment, and can thus be derived or learnt from the structure and parameters of a statistical model of sensory observations, following the Helmholtzian notions of perception as an inferential process. The generative modelling framework is particularly appealing in this respect, because it defines the internal model by making explicit how the visual image is generated from external sources. Successful learning of the models parameters thus implies the identification of such causes, which are ultimately what is needed to control behaviour.\nHere, we present a model of visual input where observations are generated from a set of binary\u201d content\u201d variables, each representing the presence of a particular visual feature. The appearance of \u2026", "Combining Pseudo-Point and State Space Approximations for Sum-Separable Gaussian Processes (Supplementary material)": "Gaussian processes (GPs) are important probabilistic tools for inference and learning in spatiotemporal modelling problems such as those in climate science and epidemiology. However, existing GP approximations do not simultaneously support large numbers of off-the-grid spatial data-points and long time-series which is a hallmark of many applications. Pseudo-point approximations, one of the gold-standard methods for scaling GPs to large data sets, are well suited for handling off-the-grid spatial data. However, they cannot handle long temporal observation horizons effectively reverting to cubic computational scaling in the time dimension. State space GP approximations are well suited to handling temporal data, if the temporal GP prior admits a Markov form, leading to linear complexity in the number of temporal observations, but have a cubic spatial cost and cannot handle off-thegrid spatial data. In this work we show that there is a simple and elegant way to combine pseudopoint methods with the state space GP approximation framework to get the best of both worlds. The approach hinges on a surprising conditional independence property which applies to space\u2013time separable GPs. We demonstrate empirically that the combined approach is more scalable and applicable to a greater range of spatio-temporal problems than either method on its own.", "Composing Neural Processes with Flux": "NeuralProcesses.jl: Composing Neural Processes with Flux Page 1 NeuralProcesses.jl: Composing Neural Processes with Flux Wessel P. Bruinsma1,2,\u2217, Jonathan Gordon1,\u2217, Richard E. Turner1 1University of Cambridge, 2Invenia Labs \u2217Equal contribution JuliaCon 2020 Page 2 Setting: Learning to Predict 1/7 ... Page 3 Setting: Learning to Predict 1/7 \u21a6\u2192 ... ... \u21a6\u2192 Page 4 Setting: Learning to Predict 1/7 \u03c6 \u21a6\u2192 data sets D predictions P \u2192 \u03c6: ... ... \u03c6 \u21a6\u2192 Page 5 Setting: Learning to Predict 1/7 \u03c6 \u21a6\u2192 data sets D predictions P \u2192 \u03c6: ... ... \u03c6 \u21a6\u2192 neural process Page 6 Setting: Learning to Predict 1/7 \u03c6 \u21a6\u2192 data sets D predictions P \u2192 \u03c6: ... ... \u03c6 \u21a6\u2192 neural process training Page 7 Setting: Learning to Predict 1/7 \u03c6 \u21a6\u2192 data sets D predictions P \u2192 \u03c6: ... ... \u03c6 \u21a6\u2192 neural process training test Page 8 Setting: Learning to Predict 1/7 \u03c6 \u21a6\u2192 data sets D predictions P \u2192 \u03c6: ... ... \u03c6 \u21a6\u2192 neural process training test \u03c6 \u21a6\u2192 Page 9 Neural \u2026", "Gaussian Process Probabilistic Programming with Stheno. jl": "\u2022 Gaussian processes (GPs)[3] are nonparametric models for collections of realvalued functions. Admit exact Bayesian inference (highly unusual).\u2022 Closely related to models in Deep Learning [2], and useful components in larger non-Gaussian models eg [1].\n\u2022 Stheno. jl is a probabilistic programming framework for modelling using GPs.\u2022 Work directly with transformations of processes, not kernels.\u2022 Plain Julia code,\u201creads like the math\u201d.\u2022 Easy to write intuitive and readable code: ideal for domain experts.\u2022 Extensible, modular design ideal for GP researchers.\u2022 Trivially compatible with Turing. jl.", "A Bayesian active-learning approach for obtaining notched-noise data": "Active-learning methods provide a promising way of running perceptual tests quickly while achieving high accuracy, at the cost of being computationally more expensive. Kontsevich and Tyler [1] used an active-learning approach to determine a psychometric function. For each trial, they chose the stimulus magnitude that was expected to be most informative about the mean and slope of the psychometric function. By using Gaussian Processes [2] and exploiting the commutativity of mutual information [3], active learning can be extended for use with more than two parameters, despite tough time constraints. In perceptual tests, the parameters for the next trial usually need to be chosen in an inter-trial interval of not more than 2 s.\nA shorter test duration is particularly beneficial when determining auditory filter shapes using notched noise [4] in large scale research studies or clinical practice. Typically, several thresholds \u2026", "Supplement: On the Expressiveness of Approximate Inference in Bayesian Neural Networks": "In this appendix, we show plots generated by placing two Gaussian clusters of data with centers randomly chosen on the sphere of radius\u221a D, where D= 5 denotes input dimension. We generate synthetic data by sampling from a wide-limit Gaussian process. For each plot, we show the predictive mean and 2 standard deviations along the line segment in input space joining the centres of these two clusters. For all plots, we choose \u03c3w=\u221a 2, \u03c3b= 1, networks of width 50 and dropout probability of p= 0.05 for MCDO. We set the observation noise standard deviation to 0.01, which is the ground truth value used to generate the synthetic data. The initialisation of MFVI and MCDO is the same as discussed in appendix F.\nIn the 1HL case, Theorem 2 implies that MCDO\u2019s predictive variance will be convex along any line, including the line plotted. In contrast, theorem 1 only applies to certain lines in input space, and does not \u2026", "Supplementary Material: Meta-Learning Stationary Stochastic Process Prediction with Convolutional Neural Processes": "Notation. We first review the notation introduced in the main body for convenience. Let X= Rdin and Y= R denote the input and output spaces respectively, and let (x, y) denote a generic inputoutput pair (higher-dimensional outputs can be treated easily). Define SN=(X\u00d7 Y) N to be the collection of all data sets of size N, and let S:=\u22c3\u221e", "TaskNorm: Rethinking Batch Normalization for Meta-Learning Open Website": "Modern meta-learning approaches for image classification rely on increasingly deep networks to achieve state-of-the-art performance, making batch normalization an essential component of meta-learning pipelines. However, the hierarchical nature of the meta-learning setting presents several challenges that can render conventional batch normalization ineffective, giving rise to the need to rethink normalization in this setting. We evaluate a range of approaches to batch normalization for meta-learning scenarios, and develop a novel approach that we call TaskNorm. Experiments on fourteen datasets demonstrate that the choice of batch normalization has a dramatic effect on both classification accuracy and training time for both gradient based and gradient-free meta-learning approaches. Importantly, TaskNorm is found to consistently improve performance. Finally, we provide a set of best practices for normalization \u2026", "Sample-then-optimize posterior sampling for Bayesian linear models Download PDF": "In modern machine learning it is common to train models which have an extremely high intrinsic capacity. The results obtained are often initialization dependent, are different for disparate optimizers and in some cases have no explicit regularization. This raises difficult questions about generalization [1]. A natural approach to questions of generalization is a Bayesian one. There is therefore a growing literature attempting to understand how Bayesian posterior inference could emerge from the complexity of modern practice [2, 3], even without having such a procedure as the stated goal. In this work we consider a simple special case where exact Bayesian posterior sampling emerges from sampling (cf initialization) and then gradient descent. Specifically, for a Bayesian linear model, if we parameterize it in terms of a deterministic function of an isotropic normal prior, then the action of sampling from the prior followed by \u2026", "Conditional Neural Processes for Semi-Supervised Learning": "Aim: Implement state of the art deep SSL methods in an easy to use framework. As well as, investigate the use of Conditional Neural Processes (CNP)[1] for SSL, which are latent-free, applicable to non uniformly discretized time series, and robust to missing features.", "Extending and Applying the Gaussian Process Autoregressive Regression Model": "The goal of this thesis is to investigate different properties and applications of the Gaussian Process Autoregressive Regression (GPAR) model [9]. Through the use of multiple Gaussian processes, GPAR can efficiently model multi-output systems by using information about the relationship between the various outputs. The goal of this thesis is twofold. First, we examine and describe GPAR behaviour when dealing with random hyperparameter initialisation, compositional kernel search, and optimal conditional ordering. Second, we discuss several approaches for using GPAR to enhance performance in solving multiple integrals for related integrands using Bayesian quadrature [12] and also for hyperparameter tuning with the freeze-thaw approach [11].", "Supplementary Material for Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes": "An algorithm for constructing the stochastic training objective \u02c6L (\u03c6; \u03c4) for a single task \u03c4 is given in Algorithm A. 1. CAT (\u00b7; \u03c0) denotes a the likelihood of a categorical distribution with parameter vector \u03c0. This algorithm can be used on a batch of tasks to construct an unbiased estimator for the auto-regressive likelihood of the task outputs.", "A Unifying Framework for Sparse Gaussian Process Approximation using Power Expectation Propagation": "A Unifying Framework for Sparse Gaussian Process Approximation using Power Expectation Propagation Page 1 A Unifying Framework for Sparse Gaussian Process Approximation using Power Expectation Propagation Dr. Richard E. Turner (ret26@cam.ac.uk) Computational and Biological Learning Lab, Department of Engineering, University of Cambridge ...joint work with Thang Bui, Cuong Nguyen and Josiah Yan 1 / 22 Page 2 Motivation: Gaussian Process Regression inputs outputs 2 / 22 Page 3 Motivation: Gaussian Process Regression inputs outputs ? 2 / 22 Page 4 Motivation: Gaussian Process Regression inputs outputs ? 2 / 22 Page 5 Motivation: Gaussian Process Regression inputs outputs ? inference & learning 2 / 22 Page 6 Motivation: Gaussian Process Regression inputs outputs ? inference & learning intractabilities computational analytic 2 / 22 Page 7 A Brief History of Gaussian Process \u2026", "Interpolated Policy Gradient": "First, the expected return J (\u03c0) of a policy \u03c0 can be written as the sum of the expected return J (\u03c0) of another policy \u03c0 and the expected advantage term between the two policies in the equation, where A\u03c0 (st, at) is the advantage of policy \u03c0,", "Tensors for matrix differentiation": "Here are some notes on how to use tensors to find matrix derivatives, and the relation to the.\u2217(Hadamard), vec,\u2297(Kronecker), vec-transpose and reshape operators. I wrote these notes for myself, and I apologise for any mistakes and confusions. Two sections are currently unfinished: I hope to complete them soon.", "Notes on the Wiener Filter (\u201copti-mal linear filter\u201d)": "To make things less abstract, this can be interpreted as a model for pixels in an image (d), which has been taken by a camera which blurs the light sources (f) via some point spread function (R) and is noisy (n). Note that the light sources are expected to have correlational structure in the real world and the prior over light sources reflects this (\u03a8). Particular methods for specifying \u03a8 are Gaussian processes and Kalman filters (generally in temporal settings). The model reduces to regular factor analysis when \u03a8= I, but this choice of prior covariance is just a convention.", "Relation to the energy model of complex cells": "As the main paper compares identity and attribute variables to complex and simple cells, respectively, one might ask how the interaction between the two sets of variables compares to the classical energy model of complex cells. In general, the relation between identity and attributes variables given a stimulus in our model is highly non-linear, and the state of a single identity\u2013attribute pair also depends on the state of the rest of the variables of the model. For example, the presence of the feature represented by an identity variable could be \u201cexplained away\u201d by the inference of the presence of similar features, encoded by other variables.\nHowever, it is possible to derive an expression for the interaction by making additional assumptions. In the following, we will consider a model without temporal dynamics, and with only 1 identity variable, b, and corresponding attribute variables, a. Also, we will identify the generative weights with the mean of their distribution, W:=\u2329 W\u232a P (W). We derive an expression for the identity variable given visual input by integrating over the state of the attribute variables:", "Variational Continual Learning in Deep Models": "This short paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that variational continual learning outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way. 1", "Supplementary material: Probabilistic amplitude and frequency demodulation": "In this supplementary material we flesh out the application of expectation propagation (EP) to probabilistic amplitude and frequency demodulation (PAFD).", "Free Energy in Statistical Physics and Inference": "Random notes on the useage of the free energy in statistical physics and inference. Taken from David\u2019s book Chapter 31 on Ising models, Radford Neal\u2019s review of MCMC methods, and Yedidia, Freeman and Weiss\u2019s introduction to understanding belief propogation and its generalisations.", "QProp: SAMPLE-EFFICIENT POLICY GRADIENT WITH AN OFF-POLICY CRITIC": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the stateof-the-art on-policy and off-policy methods, on OpenAI Gym\u2019s MuJoCo continuous control environments.", "Bubbles: A unifying Framework for Low-Level Statistical Properties of Natural Image Sequences. Hyvarinen et al, J. Opt. Soc. Am. 2003": "Bubbles: A unifying Framework for Low-Level Statistical Properties of Natural Image Sequences. Hyvarinen et al, J. Opt. Soc. Am. Page 1 Bubbles: A unifying Framework for Low-Level Statistical Properties of Natural Image Sequences. Hyvarinen et al, J. Opt. Soc. Am. 2003 Richard Turner (turner@gatsby.ucl.ac.uk) Gatsby Computational Neuroscience Unit, 03/03/2006 Page 2 Motivation \u2022 We want good generative models of natural scenes: P(y) = \u222b dxP(y|x)P(x) \u2022 View neurons as encoding some aspect of the posterior distribution over latents: P(x|y, \u03b8), like the mean, mode or width (inference) \u2022 View learning and adaptation as learning the parameters of this generative model (by maximum-likelihood for example) \u2022 Why would neurons do this? \u2013 Extraction of higher order statistical structure in the inputs into latent variables - causes - forms a computationally useful representation \u2013 Good generative models lead to \u2026", "Learning with Nonparametric Kernels": "Learning with Nonparametric Kernels \u2217 = Page 1 Learning with Nonparametric Kernels Wessel Bruinsma, Felipe Tobar, Richard Turner \u2022 Gaussian process (GP) regression models widely used for their expressiveness, robustness and tractability \u2022 Choice of kernel determines nature of GP \u2014expressive kernels are needed \u2022 Recent work [1] models kernel nonparametrically in a single-output time series setting \u2022 Aim is to extend this work to multidimensional input and output spaces\u2014 numerous important applications Gaussian Process Convolution Model (GPCM) [1] generative model 1 h \u223c GP(0,Kh) 2 Kf |h = 1 h(t) \u2217 h(\u2212t) 3 f |h \u223c GP(0,Kf |h ) 1 h(t) \u2217 h(\u2212t) = \u222b R h(\u03c4 + t)h(\u03c4)d\u03c4 \u2022 Parameterising Kf |h as the convolution between h(t) and h(\u2212t) ensures that Kf |h is positive definite \u2022 Equivalently, f = x \u2217 h for x \u223c GP(0,\u03b4(t \u2212 t )) \u2022 Learning and inference are performed using state-of-the-art variational freeenergy approximations \u2022 \u2026", "Black-box \u03b1-divergence for Deep Generative Models": "We propose using the black-box \u03b1-divergence [1] as a flexible alternative to variational inference in deep generative models. By simply switching the objective function from the variational free-energy to the black-box \u03b1-divergence objective we are able to learn better generative models, which is demonstrated by a considerable improvement of the test log-likelihood in several preliminary experiments.", "Circular Pseudo-Point Approximations for Scaling Gaussian Processes": "We introduce a new approach for performing accurate and computationally efficient posterior inference for Gaussian Process regression problems that exploits the combination of pseudo-point approximations and approximately circulant covariance structure. We argue mathematically that the new technique has substantially lower asymptotic complexity than traditional pseudo-point approximations and demonstrate empirically that it returns results that are very close to those obtained using exact inference.", "Inferring speaker size from speech formant data: A theoretical basis for vocal-tract length estimation": "People are surprisingly good at segregating and tracking the speech of a specific individual in a multi-speaker environment. One of the more stable speaker characteristics is vocal tract length (VTL), and recent research indicates that the auditory system can assess the acoustic scale information in vowels and syllables (Smith et al., 2005; Smith and Patterson, 2005; Ives et al., 2006). This suggests that speech recognition systems would benefit from the inclusion of a pre-processor that employs some form of VTL estimation on a syllable-bysyllable basis, which provides a stream of VTL values for use as a tracking variable in multispeaker environments (Turner et al, 2006). There is a problem, however, with the estimation of formant frequencies which makes it difficult to assess the contribution of vowel type, VTL, and other sources of variability such as oral-pharyngeal length ratio. This paper describes the development of a statistical model of formant ratio theory (FRT) to describe the relationships between vowel type, VTL, formant frequency, and vocal cavity size. The paper begins with a deterministic, quantitative clustering analysis of Peterson and Barney\u2019s (1952) classical formant-frequency data to determine the proportion of formant variability attributable to vowel type and VTL. The analysis suggests that there might be one more consistent and significant source of variability such as oral-pharyngeal ratio. The recent Magnetic Resonance Imaging data of Fitch and Giedd (1999) are then analyzed to determine the growth functions for the oral and pharyngeal cavities and the potential effect of the oralpharyngeal ratio on formant frequencies. Finally \u2026", "Statistical signal processing methods for speech": "\u2013Filling in missing data (artifact removal)\u2013Denoising (first model), Deconvolution/De-reverberation\u2013Source separation/Auditory scene analysis\u2013Super-resolution, Compression...", "Variational Inference for Sparse Spectrum Approximation in Gaussian Process Regression": "Standard sparse pseudo-input approximations to the Gaussian process (GP) cannot handle complex functions well. Sparse spectrum alternatives attempt to answer this but are known to over-fit. We suggest the use of variational inference for the sparse spectrum approximation to avoid both issues. We model the covariance function with a finite Fourier series approximation and treat it as a random variable. The random covariance function has a posterior, on which a variational distribution is placed. The variational distribution transforms the random covariance function to fit the data. We study the properties of our approximate inference, compare it to alternative ones, and extend it to the distributed and stochastic domains. Our approximation captures complex functions better than standard approaches and avoids over-fitting.", "Deep Gaussian Processes for Regression using Approximate Expectation Propagation: Supplementary material": "Given the approximate posterior and a new test input x\u2217, we wish to make a prediction about the test output y\u2217. That is to find p (y\u2217| x\u2217, X, Y)\u2248\u222b du p (y\u2217| x\u2217, u) q (u| X, Y). This predictive distribution is not analytically tractable, but fortunately, we can approximate it by a Gaussian in a similar fashion to the method described in the main text. That is, a single forward pass is performed, in which each layer takes in a Gaussian distribution over the input, incorporates the approximate posterior of the inducing outputs and approximates the output distribution by a Gaussian. An alternative to obtain the prediction is to forward sample from the model, but we do not use this approach in the experiments.", "Black-Box \u03b1-Divergence Minimization: Supplementary": "This section revisits the original EP algorithm as a min-max optimization problem. Recall in the main text that we approximate the true posterior distribution p (\u03b8| P) with a distribution in exponential family form given by q (\u03b8) o exp1s (\u03b8) T \u03bbql. Now we define a set of unnormalized cavity distributions q\\n (\u03b8)= exp1s (\u03b8) T \u03bb\\nl for every data point xn. Then according to [Minka, 2001], the EP energy function is", "Stochastic Expectation Propagation: Supplementary Material": "The supplementary material is divided into these sections. Section A details the design of stochastic power EP methods and presents relationships between SEP and SVI. Section B extends the discussion of distributed algorithms and SEP\u2019s applicability to latent variable models. Section C provides experimental details of the Bayesian neural network experiments and presents further emprical evalucations of the method.", "Variational Inference for Sparse Spectrum Approximation in Gaussian Process Regression\u2013Appendix": "Variational Inference for Sparse Spectrum Approximation in Gaussian Process Regression \u2013 Appendix Page 1 Variational Inference for Sparse Spectrum Approximation in Gaussian Process Regression \u2013 Appendix Yarin Gal YG279@CAM.AC.UK Richard Turner RET26@CAM.AC.UK University of Cambridge A. Appendix Identity 1. cos(x \u2212 y) = \u222b 2\u03c0 0 1 2\u03c0 \u221a 2 cos(x + b) \u221a 2 cos(y + b)db Proof. We first evaluate the term inside the integral. We have cos(x + b) cos(y + b) = (cos(x) cos(b) \u2212 sin(x) sin(b)) \u00b7 (cos(y) cos(b) \u2212 sin(y) sin(b)) = (cos(x) cos(y)) cos2(b) + (sin(x) sin(y)) sin2(b) \u2212 (sin(x) cos(y) + cos(x) sin(y)) sin(b) cos(b). Now, since \u222b cos2(b)db = b 2 + 1 4 sin(2b), as well as \u222b sin2(b)db = b 2 \u2212 1 4 sin(2b), and \u222b sin(b) cos(b)db = \u22121 4 cos(2b), we have \u222b 2\u03c0 0 1 2\u03c0 \u221a 2 cos(x + b) \u221a 2 cos(y + b)db = 1 \u03c0 (cos(x) cos(y)(\u03c0 \u2212 0) + sin(x) sin(y)(\u03c0 \u2212 0) \u2212 (sin(x) cos(y) + cos(x) sin(y)) \u00b7 0 = cos(x \u2212 y) Identity 2. EN(w;\u00b5,\u03a3)( cos(\u2026", "Tree-structured Gaussian process approximations Supplementary material": "The FITC approximation assumes that all latent functions f are independent given inducing variables u. The joint distribution of the inducing variables and the latent functions q (f, u) is then chosen by minimising the KL divergence between p (f, u) and q (f, u), q (fi| u)\u2190 arg min q (fi| u)", "Effect of dimensionality reduction": "In the main paper we argued that the dimensionality reduction step that we performed in the preprocessing of the image data is unlikely to affect the results of the paper, because of the self-similar structure of natural images, and because it consists of a linear operation for which the learning algorithm could easily compensate (Materials and Methods). To test these assumptions, we performed an additional run of our model which was identical to the one described in the main paper, except for the fact that the input consisted of 9x9 image patches, obtained from 18x18 patches in the original data and downsampled by a factor of 2 by block averaging. Downsampling allows to preserve the same scale of temporal changes in the smaller patch (eg, movements larger than 9 pixels per frame would have been lost without downsampling). The representation learned in this case has a qualitative similarity with the representation in the main simulation in the manuscript (Fig. 4). All attribute units were classified as simple cells (minimum F1/F0 ratio 1.18), and all identity units as complex cells (maximum F1/F0 ratio 0.01). The representation of the input patches is slightly overcomplete, with 99 basis vectors representing the 81-dimensional input space; the size of the attribute manifold is between 2 and 4 for most identity units. The shape of the RFs is still Gabor-like, but it is in many cases dominated by pixellation artifacts, in particular for the units representing high frequencies. As a consequence, the RF statistics for the population do not match electrophysiological results as closely as those in the manuscript. A similar effect has been reported for ICA ([1], Fig. 3 \u2026", "Probabilistic Population Codes": "Currently there are two main working hypotheses that purport to answer the first of these questions: what do neural populations represent? The first (standard model) claims that populations encode the value of a stimulus. Whilst the second, more recent perspective, claims they encode a probability distribution over the possible values of a stimulus.\nThe standard model can be caricatured in the following manner: Firstly we specify an encoding rule from stimulus x to neural rate ri. This will be a probabilistic mapping P (ri| x) due to neuronal noise. To decode 1 we form P (x| r) and typically take a point estimate of the stimulus, one popular choice for which is the MAP estimate: x= arg maxx P (x| r). In summary then, the standard model typically only considers a single source of uncertainty (arising from noisy neural activities) and only decodes a point estimate from the posterior.", "Structured representations in the visual cortex": "In these plots, we identify the mean of the attribute variables with the firing rate of simple cells. Simulation data is reported for attribute variables corresponding to the same cause, while the physiological data from DeAngelis et al.(1999) is for simple cells on the same electrode.", "Finding the optimal sparse, overcomplete model for natural im-ages by model selection": "The principles that underlie the structure of receptive fields in the primary visual cortex are not well understood. One theory is that they emerge from information processing constraints, and that two basic principles in particular play a key role. The first principle is that of sparsity. Both neural firing rates and visual statistics are sparsely distributed, and sparse models for images have been successful in reproducing some of the characteristics of simple cell receptive fields (RFs) in V1 [1, 2]. The second principle is overcompleteness. The number of neurons in V1 is 100-300 times larger than the number of neurons in the LGN. It has often been assumed that sparse, overcomplete codes might lend some computational advantage in the representation and processing of visual information [3, 4]. The goal of this work is to investigate this claim, within the context of the most common form of encoding model.\nMany different \u2026", "Finding the optimal sparse, overcomplete model for natural images by model selection": "Computational models of visual cortex, and in particular those based on sparse coding, have enjoyed much recent attention. Despite this currency, the question of how sparse or how over-complete a sparse representation should be, has gone without principled answer. Here, we use Bayesian model-selection methods to address these questions for a sparse-coding models based on a Student-t prior and on a Gaussian scale mixture model with uniform prior on precision. We find that natural images are indeed best modeled by extremely sparse distributions, although for these priors, the associated optimal basis size is only modestly over-complete (Berkes et al., 2008).", "Complex and simple cells form a structured representation of identities and attributes": "Many computational models have offered functional accounts of the organization of the sensory cortex. However, most have lacked the structure needed to extract the high-order causes of the sensory input. Here we present a generative model of visual input based on the duality between the identity of image features and their attributes. The presence of a feature is encoded by a binary identity variable, while its appearance is modeled by a multidimensional manifold, parametrized by a set of attribute variables. When applied to natural image sequences, the model finds attribute manifolds spanned by localized Gabor wavelets with similar positions, orientations, and frequencies, but different phases. Thus the inferred activity of attribute variables after learning resembles that of simple cells in the primary visual cortex. Identity variables indicate the presence of a feature irrespective of its position on the underlying manifold, making them phase-insensitive, like complex cells. The dimensionality of the learnt manifolds and the relationships between the wavelets correspond closely to anatomical and functional observations regarding simple and complex cells. Thus, this generative model makes explicit an interpretation of complex and simple cells as elements in the segmentation of a visual scene into independent features, with a parametrization of their episodic appearance. It also suggest a possible role for them in a hierarchical system that extracts progressively higher-level entities, starting from simpler, low-level features.", "Probilistic Amplitude and Frequency Demodulation: On the equivalence of the Dynamic Harmonic Model and Bayesian Spectral Estimation": "Imagine that we want to do simultaneous probabilistic amplitude-and frequencydemodulation. That is, derive a probabilisic represention of data in terms of a positive, slowly varying amplitude and a slowly varying phase (or instantaneuos frequency). In this note, two existing algorithms are re-interpreted in this context, and shown to be equivalent. This result is important: first as it shows that exact inference is possible in certain AM/FM models; second as the modelling choices can then be criticised from this new perspective. Third as extensions and alternatives can then be proposed (which invariably require approximate inference algorithms).", "A maximum likelihood algorithm for slow features analysis": "\u2022 TOP: take images of natural scenes; construct a time series by translating, rotating, and zooming a window\u2022 input to SFA at one time step is two consecutive frames so full spatial-temporal field is found\u2022 Recover rich repertoire of complex cell properties: direction selectivity,[nonorthogonal, end, side] inhibition [4]", "Two problems with variational expectation maximisation for time-series models": null, "Causal transfer in machine learning": null, "Decomposing signals into a sum of AM-FM sinusoids using probabilistic inference": null}, "David Krueger": {"Nice: Non-linear independent components estimation": "We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.", "Zoneout: Regularizing rnns by randomly preserving hidden activations": "We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "Neural autoregressive flows": "Normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via Masked Autoregressive Flows (MAF)(Papamakarios et al., 2017), and to accelerate state-of-the-art WaveNet-based speech synthesis to 20x faster than real-time (Oord et al., 2017), via Inverse Autoregressive Flows (IAF)(Kingma et al., 2016). We unify and generalize these approaches, replacing the (conditionally) affine univariate transformations of MAF/IAF with a more general class of invertible univariate transformations expressed as monotonic neural networks. We demonstrate that the proposed neural autoregressive flows (NAF) are universal approximators for continuous probability distributions, and their greater expressivity allows them to better capture multimodal target distributions. Experimentally, NAF yields state-of-the-art performance on a suite of density estimation tasks and outperforms IAF in variational autoencoders trained on binarized MNIST.", "Out-of-distribution generalization via risk extrapolation (rex)": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing robustness to changes in the input distribution (\u201ccovariate shift\u201d). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "Zero-bias autoencoders and the benefits of co-adapting features": "Regularized training of an autoencoder typically results in hidden unit biases that take on large negative values. We show that negative biases are a natural result of using a hidden layer whose responsibility is to both represent the input data and act as a selection mechanism that ensures sparsity of the representation. We then show that negative biases impede the learning of data distributions whose intrinsic dimensionality is high. We also propose a new activation function that decouples the two roles of the hidden layer and that allows us to learn representations on data with very high intrinsic dimensionality, where standard autoencoders typically fail. Since the decoupled activation function acts like an implicit regularizer, the model can be trained by minimizing the reconstruction error of training data, without requiring any additional regularization.", "Toward trustworthy AI development: mechanisms for supporting verifiable claims": "With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.", "Bayesian hypernetworks": "We study Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks. A Bayesian hypernetwork $\\h$ is a neural network which learns to transform a simple noise distribution, $p(\\vec\\epsilon) = \\N(\\vec 0,\\mat I)$, to a distribution $q(\\pp) := q(h(\\vec\\epsilon))$ over the parameters $\\pp$ of another neural network (the \"primary network\")\\@. We train with variational inference, using an invertible $\\h$ to enable efficient estimation of the variational lower bound on the posterior $p(\\pp | \\D)$ via sampling. In contrast to most methods for Bayesian deep learning, Bayesian hypernets can represent a complex multimodal approximate posterior with correlations between parameters, while enabling cheap iid sampling of~$q(\\pp)$. In practice, Bayesian hypernets can provide a better defense against adversarial examples than dropout, and also exhibit competitive performance on a suite of tasks which evaluate model uncertainty, including regularization, active learning, and anomaly detection.", "Scalable agent alignment via reward modeling: a research direction": "One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.", "Regularizing rnns by stabilizing activations": "We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing the squared distance between successive hidden states' norms. This penalty term is an effective regularizer for RNNs including LSTMs and IRNNs, improving performance on character-level language modeling and phoneme recognition, and outperforming weight noise and dropout. We achieve competitive performance (18.6\\% PER) on the TIMIT phoneme recognition task for RNNs evaluated without beam search or an RNN transducer. With this penalty term, IRNN can achieve similar performance to LSTM on language modeling, although adding the penalty term to the LSTM results in superior performance. Our penalty term also prevents the exponential growth of IRNN's activations outside of their training horizon, allowing them to generalize to much longer sequences.", "Nested lstms": "We propose\\emphNested LSTMs (NLSTM), a novel RNN architecture with multiple levels of memory. Nested LSTMs add depth to LSTMs via nesting as opposed to stacking. The value of a memory cell in an NLSTM is computed by an LSTM cell, which has its own\\it inner memory cell. Specifically, instead of computing the value of the (outer) memory cell as , NLSTM memory cells use the concatenation as input to an inner LSTM (or NLSTM) memory cell, and set = . Nested LSTMs outperform both stacked and single-layer LSTMs with similar numbers of parameters in our experiments on various character-level language modeling tasks, and the inner memories of an LSTM learn longer term dependencies compared with the higher-level units of a stacked LSTM.", "Deep prior": "The recent literature on deep learning offers new tools to learn a rich probability distribution over high dimensional data such as images or sounds. In this work we investigate the possibility of learning the prior distribution over neural network parameters using such tools. Our resulting variational Bayes algorithm generalizes well to new tasks, even when very few training examples are provided. Furthermore, this learned prior allows the model to extrapolate correctly far from a given task's training data on a meta-dataset of periodic signals.", "Active reinforcement learning: Observing rewards at a cost": "Active reinforcement learning (ARL) is a variant on reinforcement learning where the agent does not observe the reward unless it chooses to pay a query cost c > 0. The central question of ARL is how to quantify the long-term value of reward information. Even in multi-armed bandits, computing the value of this information is intractable and we have to rely on heuristics. We propose and evaluate several heuristic approaches for ARL in multi-armed bandits and (tabular) Markov decision processes, and discuss and illustrate some challenging aspects of the ARL problem.", "Hidden Incentives for Auto-Induced Distributional Shift": "Decisions made by machine learning systems have increasing influence on the world, yet it is common for machine learning algorithms to assume that no such influence exists. An example is the use of the i.i.d. assumption in content recommendation. In fact, the (choice of) content displayed can change users' perceptions and preferences, or even drive them away, causing a shift in the distribution of users. We introduce the term auto-induced distributional shift (ADS) to describe the phenomenon of an algorithm causing a change in the distribution of its own inputs. Our goal is to ensure that machine learning systems do not leverage ADS to increase performance when doing so could be undesirable. We demonstrate that changes to the learning algorithm, such as the introduction of meta-learning, can cause hidden incentives for auto-induced distributional shift (HI-ADS) to be revealed. To address this issue, we introduce `unit tests' and a mitigation strategy for HI-ADS, as well as a toy environment for modelling real-world issues with HI-ADS in content recommendation, where we demonstrate that strong meta-learners achieve gains in performance via ADS. We show meta-learning and Q-learning both sometimes fail unit tests, but pass when using our mitigation strategy.", "AI Research Considerations for Human Existential Safety (ARCHES)": "Framed in positive terms, this report examines how technical AI research might be steered in a manner that is more attentive to humanity's long-term prospects for survival as a species. In negative terms, we ask what existential risks humanity might face from AI development in the next century, and by what principles contemporary technical research might be directed to address those risks. A key property of hypothetical AI technologies is introduced, called \\emph{prepotence}, which is useful for delineating a variety of potential existential risks from artificial intelligence, even as AI paradigms might shift. A set of \\auxref{dirtot} contemporary research \\directions are then examined for their potential benefit to existential safety. Each research direction is explained with a scenario-driven motivation, and examples of existing work from which to build. The research directions present their own risks and benefits to society that could occur at various scales of impact, and in particular are not guaranteed to benefit existential safety if major developments in them are deployed without adequate forethought and oversight. As such, each direction is accompanied by a consideration of potentially negative side effects.", "Memorization in Recurrent Neural Networks": "We present work in progress on understanding generalization in deep networks by analyzing differences in learning behaviour of recurrent neural networks (RNNs) on noise data vs. real dataie, by investigating memorization. There has been a recent surge of interest in explaining the generalization performance of deep neural networks (DNNs), partially spurred by the observation that feedforward DNNs can fit random noise datasets with 0 training error. RNNs are typically extremely deep; analyzing learning behaviour in RNNs thus gives an interesting perspective for understanding memorization, generalization, and effective capacity in deep networks. We demonstrate that fitting noise with RNNs is more difficult than in feedforward networks; standard gradientbased optimization fails to reach 0 training error. We make several other observations comparing and contrasting our results with previous work, and suggest a suite of experiments for future work.", "Testing visual attention in dynamic environments": "We investigate attention as the active pursuit of useful information. This contrasts with attention as a mechanism for the attenuation of irrelevant information. We also consider the role of short-term memory, whose use is critical to any model incapable of simultaneously perceiving all information on which its output depends. We present several simple synthetic tasks, which become considerably more interesting when we impose strong constraints on how a model can interact with its input, and on how long it can take to produce its output. We develop a model with a different structure from those seen in previous work, and we train it using stochastic variational inference with a learned proposal distribution.", "Multi-Domain Balanced Sampling Improves Out-of-Generalization of Chest X-ray Pathology Prediction Models": "Learning models that generalize under different distribution shifts in medical imaging has been a long-standing research challenge. There have been several proposals for efficient and robust visual representation learning among vision research practitioners, especially in the sensitive and critical biomedical domain. In this paper, we propose an idea for out-of-distribution generalization of chest X-ray pathologies that uses a simple balanced batch sampling technique. We observed that balanced sampling between the multiple training datasets improves the performance over baseline models trained without balancing.", "Filling gaps in trustworthy development of AI": "Incident sharing, auditing, and other concrete mechanisms could help verify the trustworthiness of actors", "Reserve Output Units for Deep Open-Set Learning": "Open-set learning poses a classification problem where the set of class labels expands over time; a realistic but not widely-studied setting. We propose a deep learning technique for open-set learning based on Reserve Output Units (ROUs), which are designed to help a network anticipate the introduction of new categories during training. ROUs are additional output units whose representations are trained along with units for already-seen classes, which can be assigned to new classes once a labeled instance of a novel class is observed. We experiment with different initialization methods, compare this method with simply adding an new output vector for the novel class, and find that ROUs achieve better and more consistent performance than the simple add-new baseline. We further experiment with a loss which encourages the output space to match pretrained word embeddings, with the goal of encouraging good semantics of this space. In experiments on MNIST and CIFAR, this technique hurt or does not affect performance, but we're optimistic this technique could be helpful for larger output spaces.", "Designing Regularizers and Architectures for Recurrent Neural Networks": "This thesis represents incremental work towards artificial intelligence using connectionist methods. Recurrent neural networks are a set of increasingly popular sequential models capable in principle of learning arbitrary algorithms. These models perform deep learning, a type of machine learning. Their generality and empirical success makes them an attractive candidate for further work and a promising tool for creating more general artificial intelligence. The first chapter of this thesis gives a brief overview of the background topics: artificial intelligence, machine learning, deep learning, and recurrent neural nets. The next three chapters cover these topics in order of increasing specificity. Finally, we contribute some general methods for recurrent neural networks. Chapter \\ref{arxiv1} presents our work on the topic of recurrent neural network regularization. Regularization aims to improve a model's generalization ability, and is a key bottleneck in the performance for several applications of recurrent neural networks, most notably speech recognition. Our approach gives state of the art results on the standard TIMIT benchmark for this task. Chapter \\ref{cpgp} presents the second line of work, still in progress, exploring a new architecture for recurrent neural nets. Recurrent neural networks maintain a hidden state which represents their previous observations. The idea of this work is to encode some abstract dynamics in the hidden state, giving the network a natural way to encode consistent or slow-changing trends in the state of its environment. Our work builds on a previously developed model; we describe this previous work and our contributions \u2026", "A closer look at memorization in deep networks": null, "35th International Conference on Machine Learning, ICML 2018": null}, "Hong Ge": {"Inferring the effectiveness of government interventions against COVID-19": "INTRODUCTION\nGovernments across the world have implemented a wide range of nonpharmaceutical interventions (NPIs) to mitigate the spread of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Given the increasing death toll of the pandemic and the social cost of some interventions, it is critical to understand their relative effectiveness. By considering the effects that interventions had on transmission during the first wave of the outbreak, governments can make more-informed decisions about how to control the pandemic.\nRATIONALE\nRigorously studying the effectiveness of individual interventions poses considerable methodological challenges. Simulation studies can explore scenarios, but they make strong assumptions that may be difficult to validate. Data-driven, cross-country modeling comparing the timing of national interventions to the subsequent numbers of cases or deaths is a promising \u2026", "Transcriptional data: a new gateway to drug repositioning?": "Recent advances in computational biology suggest that any perturbation to the transcriptional programme of the cell can be summarised by a proper \u2018signature\u2019: a set of genes combined with a pattern of expression. Therefore, it should be possible to generate proxies of clinicopathological phenotypes and drug effects through signatures acquired via DNA microarray technology.\nGene expression signatures have recently been assembled and compared through genome-wide metrics, unveiling unexpected drug\u2013disease and drug\u2013drug \u2018connections\u2019 by matching corresponding signatures. Consequently, novel applications for existing drugs have been predicted and experimentally validated.\nHere, we describe related methods, case studies and resources while discussing challenges and benefits of exploiting existing repositories of microarray data that could serve as a search space for systematic drug repositioning.", "Turing: a language for flexible probabilistic inference": "Probabilistic programming promises to simplify and democratize probabilistic machine learning, but successful probabilistic programming systems require flexible, generic and efficient inference engines. In this work, we present a system called Turing for building MCMC algorithms for probabilistic programming inference. Turing has a very simple syntax and makes full use of the numerical capabilities in the Julia programming language, including all implemented probability distributions, and automatic differentiation. Turing supports a wide range of popular Monte Carlo algorithms, including Hamiltonian Monte Carlo (HMC), HMC with No-U-Turns (NUTS), Gibbs sampling, sequential Monte Carlo (SMC), and several particle MCMC (PMCMC) samplers. Most importantly, Turing inference is composable: it combines MCMC operations on subsets of variables, for example using a combination of an HMC engine and a particle Gibbs (PG) engine. We explore several combinations of inference methods with the aim of finding approaches that are both efficient and universal, ie applicable to arbitrary probabilistic models. NUTS\u2014a popular variant of HMC that adapts Hamiltonian simulation path length automatically, although quite powerful for exploring differentiable target distributions, is however not universal. We identify some failure modes for the NUTS engine, and demonstrate that composition of PG (for discrete variables) and NUTS (for continuous variables) can be useful when the NUTS engine is either not applicable, or simply does not work well. Our aim is to present Turing and its composable inference engines to the world and encourage other \u2026", "The effectiveness of eight nonpharmaceutical interventions against COVID-19 in 41 countries": "Background:\nGovernments are attempting to control the COVID-19 pandemic with nonpharmaceutical interventions (NPIs). However, it is still largely unknown how effective different NPIs are at reducing transmission. Data-driven studies can estimate the effectiveness of NPIs while minimizing assumptions, but existing analyses lack sufficient data and validation to robustly distinguish the effects of individual NPIs.\nMethods:\nWe collect chronological data on NPIs in 41 countries between January and May 2020, using independent double entry by researchers to ensure high data quality. We estimate NPI effectiveness with a Bayesian hierarchical model, by linking NPI implementation dates to national case and death counts. To our knowledge, this is the largest and most thoroughly validated data-driven study of NPI effectiveness to date.\nResults:\nWe model each NPI9s effect as a multiplicative (percentage) reduction in the reproduction number R. We estimate the mean reduction in R across the countries in our data for eight NPIs: mandating mask-wearing in (some) public spaces (2%; 95% CI:-14%-16%), limiting gatherings to 1000 people or less (2%;-20%-22%), to 100 people or less (21%; 1%-39%), to 10 people or less (36%; 16%-53%), closing some high-risk businesses (31%; 13%-46%), closing most nonessential businesses (40%; 22%-55%), closing schools and universities (39%; 21%-55%), and issuing stay-at-home orders (18%; 4%-31%). These results are supported by extensive empirical validation, including 15 sensitivity analyses.\nConclusions:\nOur results suggest that, by implementing effective NPIs, many countries can reduce R below 1 \u2026", "Distributed Inference for Dirichlet Process Mixture Models": "Bayesian nonparametric mixture models based on the Dirichlet process (DP) have been widely used for solving problems like clustering, density estimation and topic modelling. These models make weak assumptions about the underlying process that generated the observed data. Thus, when more data are collected, the complexity of these models can change accordingly. These theoretical properties often lead to superior predictive performance when compared to traditional finite mixture models. However, despite the increasing amount of data available, the application of Bayesian nonparametric mixture models is so far limited to relatively small data sets. In this paper, we propose an efficient distributed inference algorithm for the DP and the HDP mixture model. The proposed method is based on a variant of the slice sampler for DPs. Since this sampler does not involve a pre-determined truncation, the stationary distribution of the sampling algorithm is unbiased. We provide both local thread-level and distributed machine-level parallel implementations and study the performance of this sampler through an extensive set of experiments on image and text data. When compared to existing inference algorithms, the proposed method exhibits state-of-the-art accuracy and strong scalability with up to 512 cores.", "Bayesian learning of sum-product networks": "Sum-product networks (SPNs) are flexible density estimators and have received significant attention due to their attractive inference properties. While parameter learning in SPNs is well developed, structure learning leaves something to be desired: Even though there is a plethora of SPN structure learners, most of them are somewhat ad-hoc and based on intuition rather than a clear learning principle. In this paper, we introduce a well-principled Bayesian framework for SPN structure learning. First, we decompose the problem into i) laying out a computational graph, and ii) learning the so-called scope function over the graph. The first is rather unproblematic and akin to neural network architecture validation. The second represents the effective structure of the SPN and needs to respect the usual structural constraints in SPN, i.e. completeness and decomposability. While representing and learning the scope function is somewhat involved in general, in this paper, we propose a natural parametrisation for an important and widely used special case of SPNs. These structural parameters are incorporated into a Bayesian model, such that simultaneous structure and parameter learning is cast into monolithic Bayesian posterior inference. In various experiments, our Bayesian SPNs often improve test likelihoods over greedy SPN learners. Further, since the Bayesian framework protects against overfitting, we can evaluate hyper-parameters directly on the Bayesian model score, waiving the need for a separate validation set, which is especially beneficial in low data regimes. Bayesian SPNs can be applied to heterogeneous domains and can easily be \u2026", "Particle Gibbs for Infinite Hidden Markov Models": "Infinite Hidden Markov Models (iHMM\u2019s) are an attractive, nonparametric generalization of the classical Hidden Markov Model which can automatically infer the number of hidden states in the system. However, due to the infinite-dimensional nature of the transition dynamics, performing inference in the iHMM is difficult. In this paper, we present an infinite-state Particle Gibbs (PG) algorithm to resample state trajectories for the iHMM. The proposed algorithm uses an efficient proposal optimized for iHMMs and leverages ancestor sampling to improve the mixing of the standard PG algorithm. Our algorithm demonstrates significant convergence improvements on synthetic and real world data sets.", "Turing: Composable inference for probabilistic programming.": "Turing: Composable inference for probabilistic programming. - CUED Publications database Logo CUED Publications database Home About Browse by Year Browse by Division Login Turing: Composable inference for probabilistic programming. Ge, H and Xu, K and Ghahramani, Z (2018) Turing: Composable inference for probabilistic programming. In: UNSPECIFIED pp. 1682-1690.. Full text not available from this repository. Item Type: Conference or Workshop Item (UNSPECIFIED) Subjects: UNSPECIFIED Divisions: Div F > Computational and Biological Learning Depositing User: Cron Job Date Deposited: 23 May 2018 20:11 Last Modified: 18 Feb 2021 16:42 DOI: EPrints Logo CUED Publications database is powered by EPrints 3 which is developed by the School of Electronics and Computer Science at the University of Southampton. More information and software credits. \u2026", "AdvancedHMC.jl: A robust, modular and efficient implementation of advanced HMC algorithms": "Stan\u2019s Hamilton Monte Carlo (HMC) has demonstrated remarkable sampling robustness and efficiency in a wide range of Bayesian inference problems through carefully crafted adaption schemes to the celebrated No-U-Turn sampler (NUTS) algorithm. It is challenging to implement these adaption schemes robustly in practice, hindering wider adoption amongst practitioners who are not directly working with the Stan modelling language. AdvancedHMC. jl (AHMC) contributes a modular, well-tested, standalone implementation of NUTS that recovers and extends Stan\u2019s NUTS algorithm. AHMC is written in Julia, a modern high-level language for scientic computing, benefoting from optional hardware acceleration and interoperability with a wealth of existing software written in both Julia and other languages, such as Python. Efficacy is demonstrated empirically by comparison with Stan through a third-party Markov chain Monte Carlo benchmarking suite.", "Graph Tracking in Dynamic Probabilistic Programs via Source Transformations": "Many modern machine learning algorithms, such as automatic differentiation (AD) and versions of approximate Bayesian inference, can be understood as a particular case of message passing on some computation graph. To permit learning complex models, recent approaches to extract a computation graph focus on dynamic graphs using so-called operator overloading techniques. However, in contrast to source transformation, which is commonly used for static graphs, using operator overloading does not allow to analyse the graph and naturally leads to higher overhead costs. In this paper, we present a combination of source transformation and operator overloading, which allows extracting the underlaying computation graph of complex machine learning models, eg~ Bayesian nonparametric models or models with stochastic control flow. To track the execution of operations in a machine learning model during run-time, we inject additional statements into the existing program at compile-time using the intermediate representation of the program. We introduce an extension of the well-known Wengert lists, used in many AD implementations, to record all necessary control flow information and any additional metadata. Finally, we discuss future applications of our approach, such as conjugacy exploitation in universal probabilistic programming languages, BUGS like Gibbs sampling for dynamic models and variational message-passing.", "Sampling and inference for discrete random probability measures in probabilistic programs": "We consider the problem of sampling a sequence from a discrete random probability measure (RPM) with countable support, under (probabilistic) constraints of finite memory and computation. A canonical example is sampling from the Dirichlet Process, which can be accomplished using its stick-breaking representation and lazy initialization of its atoms. We show that efficiently lazy initialization is possible if and only if a size-biased representation of the discrete RPM is used. For models constructed from such discrete RPMs, we consider the implications for generic particle-based inference methods in probabilistic programming systems. To demonstrate, we implement SMC for Normalized Inverse Gaussian Process mixture models in Turing.\nBayesian non-parametric (BNP) models are a powerful and flexible class of methods for carrying out Bayesian analysis [25]. By allowing an unbounded number of parameters, BNP models can adapt to the data, providing an increasingly complex representation as more data becomes available. However, a major drawback to BNP modeling is that the resultant inference problems are often challenging, meaning that many models require custom-built inference schemes that are challenging and time consuming to design, thereby hampering the development and implementation of new models and applications. Probabilistic programming systems (PPSs)[eg, 11; 42; 14] have the potential to alleviate this problem by providing an expressive modeling framework, and automating the required inference, making powerful statistical methods accessible to non-experts. Universal probabilistic programming languages [11 \u2026", "Interoperability of statistical models in pandemic preparedness: principles and reality": "We present \"interoperability\" as a guiding framework for statistical modelling to assist policy makers asking multiple questions using diverse datasets in the face of an evolving pandemic response. Interoperability provides an important set of principles for future pandemic preparedness, through the joint design and deployment of adaptable systems of statistical models for disease surveillance using probabilistic reasoning. We illustrate this through case studies for inferring spatial-temporal coronavirus disease 2019 (COVID-19) prevalence and reproduction numbers in England.", "DynamicPPL: Stan-like Speed for Dynamic Probabilistic Models": "We present the preliminary high-level design and features of DynamicPPL.jl, a modular library providing a lightning-fast infrastructure for probabilistic programming. Besides a computational performance that is often close to or better than Stan, DynamicPPL provides an intuitive DSL that allows the rapid development of complex dynamic probabilistic programs. Being entirely written in Julia, a high-level dynamic programming language for numerical computing, DynamicPPL inherits a rich set of features available through the Julia ecosystem. Since DynamicPPL is a modular, stand-alone library, any probabilistic programming system written in Julia, such as Turing.jl, can use DynamicPPL to specify models and trace their model parameters. The main features of DynamicPPL are: 1) a meta-programming based DSL for specifying dynamic models using an intuitive tilde-based notation; 2) a tracing data-structure for tracking RVs in dynamic probabilistic models; 3) a rich contextual dispatch system allowing tailored behaviour during model execution; and 4) a user-friendly syntax for probabilistic queries. Finally, we show in a variety of experiments that DynamicPPL, in combination with Turing.jl, achieves computational performance that is often close to or better than Stan.", "Couplings for Multinomial Hamiltonian Monte Carlo": "Hamiltonian Monte Carlo (HMC) is a popular sampling method in Bayesian inference. Recently, Heng & Jacob (2019) studied Metropolis HMC with couplings for unbiased Monte Carlo estimation, establishing a generic parallelizable scheme for HMC. However, in practice a different HMC method, multinomial HMC, is considered as the go-to method, eg as part of the no-U-turn sampler. In multinomial HMC, proposed states are not limited to end-points as in Metropolis HMC; instead points along the entire trajectory can be proposed. In this paper, we establish couplings for multinomial HMC, based on optimal transport for multinomial sampling in its transition. We prove an upper bound for the meeting time\u2013the time it takes for the coupled chains to meet\u2013based on the notion of local contractivity. We evaluate our methods using three targets: 1,000 dimensional Gaussians, logistic regression and log-Gaussian Cox point processes. Compared to Heng & Jacob (2019), coupled multinomial HMC generally attains a smaller meeting time, and is more robust to choices of step sizes and trajectory lengths, which allows re-use of existing adaptation methods for HMC. These improvements together paves the way for a wider and more practical use of coupled HMC methods.", "Bijectors.jl: Flexible transformations for probability distributions": "Transforming one probability distribution to another is a powerful tool in Bayesian inference and machine learning. Some prominent examples are constrained-to-unconstrained transformations of distributions for use in Hamiltonian Monte Carlo and constructing exible and learnable densities such as normalizing ows. We present Bijectors. jl, a software package in Julia for transforming distributions, available at github. com/TuringLang/Bijectors. jl. The package provides a exible and composable way of implementing transformations of distributions without being tied to a computational framework. We demonstrate the use of Bijectors. jl on improving variational inference by encoding known statistical dependencies into the variational posterior using normalizing ows, providing a general approach to relaxing the mean-field assumption usually made in variational inference.", "Dirichlet Fragmentation Processes": "Tree structures are ubiquitous in data across many domains, and many datasets are naturally modelled by unobserved tree structures. In this paper, first we review the theory of random fragmentation processes [Bertoin, 2006], and a number of existing methods for modelling trees, including the popular nested Chinese restaurant process (nCRP). Then we define a general class of probability distributions over trees: the Dirichlet fragmentation process (DFP) through a novel combination of the theory of Dirichlet processes and random fragmentation processes. This DFP presents a stick-breaking construction, and relates to the nCRP in the same way the Dirichlet process relates to the Chinese restaurant process. Furthermore, we develop a novel hierarchical mixture model with the DFP, and empirically compare the new model to similar models in machine learning. Experiments show the DFP mixture model to be convincingly better than existing state-of-the-art approaches for hierarchical clustering and density modelling.", "A Bayesian model for calibrating conference review scores": null}}